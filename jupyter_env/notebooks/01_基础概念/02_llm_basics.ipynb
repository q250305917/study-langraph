{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": ["å­¦ä¹ ç›®æ ‡"]
   },
   "source": [
    "# ğŸ§  å¤§è¯­è¨€æ¨¡å‹(LLM)åŸºç¡€\n",
    "\n",
    "<div class=\"learning-objectives\">\n",
    "<h3>ğŸ¯ å­¦ä¹ ç›®æ ‡</h3>\n",
    "<ul>\n",
    "<li>ç†è§£å¤§è¯­è¨€æ¨¡å‹çš„åŸºæœ¬æ¦‚å¿µå’Œå·¥ä½œåŸç†</li>\n",
    "<li>æŒæ¡LangChainä¸­ä¸åŒLLMçš„ä½¿ç”¨æ–¹æ³•</li>\n",
    "<li>å­¦ä¼šé…ç½®å’Œä¼˜åŒ–LLMå‚æ•°</li>\n",
    "<li>äº†è§£LLMçš„èƒ½åŠ›é™åˆ¶å’Œæœ€ä½³å®è·µ</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "**â±ï¸ é¢„è®¡å­¦ä¹ æ—¶é—´**: 45åˆ†é’Ÿ  \n",
    "**ğŸ“Š éš¾åº¦çº§åˆ«**: <span class=\"badge medium\">â­â­â­â˜†â˜†</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": ["ç¯å¢ƒåˆå§‹åŒ–"]
   },
   "outputs": [],
   "source": [
    "# ğŸ”§ ç¯å¢ƒåˆå§‹åŒ–\n",
    "import sys\n",
    "import os\n",
    "import asyncio\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# æ·»åŠ å·¥å…·è·¯å¾„\n",
    "sys.path.append('../utils')\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ§  å¤§è¯­è¨€æ¨¡å‹åŸºç¡€è¯¾ç¨‹\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# æ£€æŸ¥å¿…è¦ä¾èµ–\n",
    "required_imports = {\n",
    "    'langchain_openai': 'OpenAI LLMé›†æˆ',\n",
    "    'langchain_anthropic': 'Anthropic Claudeé›†æˆ (å¯é€‰)',\n",
    "    'langchain_core': 'LangChainæ ¸å¿ƒç»„ä»¶',\n",
    "    'tiktoken': 'Tokenè®¡æ•°å·¥å…·'\n",
    "}\n",
    "\n",
    "available_llms = []\n",
    "for module, description in required_imports.items():\n",
    "    try:\n",
    "        __import__(module)\n",
    "        print(f\"âœ… {description}\")\n",
    "        if 'openai' in module:\n",
    "            available_llms.append('OpenAI')\n",
    "        elif 'anthropic' in module:\n",
    "            available_llms.append('Anthropic')\n",
    "    except ImportError:\n",
    "        print(f\"âš ï¸ {description}: æœªå®‰è£…\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ å¯ç”¨LLM: {', '.join(available_llms) if available_llms else 'éœ€è¦é…ç½®APIå¯†é’¥'}\")\n",
    "\n",
    "# åˆå§‹åŒ–è¿›åº¦è¿½è¸ª\n",
    "try:\n",
    "    from progress_tracker import start_lesson, complete_section, complete_exercise, get_tracker\n",
    "    \n",
    "    start_lesson(\"02_llm_basics\", \"å¤§è¯­è¨€æ¨¡å‹åŸºç¡€\")\n",
    "    tracker = get_tracker()\n",
    "    tracker.set_lesson_info(\"02_llm_basics\", total_sections=6, total_exercises=3)\n",
    "    \n",
    "    print(\"ğŸ“ˆ è¿›åº¦è¿½è¸ªå·²å¯åŠ¨\")\n",
    "except ImportError:\n",
    "    def complete_section(name): pass\n",
    "    def complete_exercise(name, score=None): pass\n",
    "    print(\"âš ï¸ è¿›åº¦è¿½è¸ªå™¨æœªåŠ è½½\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ğŸ¤– ä»€ä¹ˆæ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼Ÿ\n",
    "\n",
    "**å¤§è¯­è¨€æ¨¡å‹(Large Language Model, LLM)** æ˜¯ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„AIæ¨¡å‹ï¼Œé€šè¿‡å­¦ä¹ å¤§é‡æ–‡æœ¬æ•°æ®æ¥ç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚\n",
    "\n",
    "### ğŸ—ï¸ LLMçš„æ ¸å¿ƒç‰¹å¾\n",
    "\n",
    "| ç‰¹å¾ | æè¿° | ç¤ºä¾‹ |\n",
    "|------|------|------|\n",
    "| **å¤§è§„æ¨¡å‚æ•°** | åŒ…å«æ•°åäº¿åˆ°æ•°åƒäº¿ä¸ªå‚æ•° | GPT-3: 1750äº¿å‚æ•° |\n",
    "| **é¢„è®­ç»ƒ** | åœ¨å¤§é‡æ— æ ‡ç­¾æ–‡æœ¬ä¸Šé¢„è®­ç»ƒ | ç½‘é¡µã€ä¹¦ç±ã€æ–‡ç« ç­‰ |\n",
    "| **æ³›åŒ–èƒ½åŠ›** | èƒ½å¤„ç†å¤šç§è¯­è¨€ä»»åŠ¡ | ç¿»è¯‘ã€æ‘˜è¦ã€é—®ç­”ç­‰ |\n",
    "| **ä¸Šä¸‹æ–‡ç†è§£** | ç†è§£é•¿æ–‡æœ¬çš„ä¸Šä¸‹æ–‡å…³ç³» | å¤šè½®å¯¹è¯ã€æ–‡æ¡£ç†è§£ |\n",
    "\n",
    "### ğŸ§¬ LLMçš„å·¥ä½œåŸç†\n",
    "\n",
    "```\n",
    "è¾“å…¥æ–‡æœ¬ â†’ è¯æ±‡ç¼–ç  â†’ æ³¨æ„åŠ›æœºåˆ¶ â†’ ç¥ç»ç½‘ç»œå±‚ â†’ æ¦‚ç‡åˆ†å¸ƒ â†’ è¾“å‡ºæ–‡æœ¬\n",
    "    â†“           â†“           â†“            â†“           â†“\n",
    "  TokenåŒ–    åµŒå…¥å‘é‡    ä¸Šä¸‹æ–‡ç†è§£    ç‰¹å¾æå–    ä¸‹ä¸€è¯é¢„æµ‹\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": ["æ¦‚å¿µæ¼”ç¤º"]
   },
   "outputs": [],
   "source": [
    "# ğŸ¨ LLMå·¥ä½œæµç¨‹å¯è§†åŒ–\n",
    "workflow_html = \"\"\"\n",
    "<div style=\"margin: 20px 0;\">\n",
    "    <h4>ğŸ”„ LLMå¤„ç†æµç¨‹</h4>\n",
    "    <div style=\"display: flex; justify-content: space-between; align-items: center; \n",
    "                background: linear-gradient(90deg, #f0f9ff 0%, #e0f2fe 100%); \n",
    "                padding: 20px; border-radius: 10px; margin: 15px 0;\">\n",
    "        \n",
    "        <div style=\"text-align: center; min-width: 100px;\">\n",
    "            <div style=\"font-size: 2em; margin-bottom: 5px;\">ğŸ“</div>\n",
    "            <strong>è¾“å…¥æ–‡æœ¬</strong><br>\n",
    "            <small>ç”¨æˆ·æç¤ºè¯</small>\n",
    "        </div>\n",
    "        \n",
    "        <div style=\"font-size: 1.5em; color: #0277bd;\">â†’</div>\n",
    "        \n",
    "        <div style=\"text-align: center; min-width: 100px;\">\n",
    "            <div style=\"font-size: 2em; margin-bottom: 5px;\">ğŸ”¤</div>\n",
    "            <strong>TokenåŒ–</strong><br>\n",
    "            <small>æ–‡æœ¬åˆ†å‰²</small>\n",
    "        </div>\n",
    "        \n",
    "        <div style=\"font-size: 1.5em; color: #0277bd;\">â†’</div>\n",
    "        \n",
    "        <div style=\"text-align: center; min-width: 100px;\">\n",
    "            <div style=\"font-size: 2em; margin-bottom: 5px;\">ğŸ§ </div>\n",
    "            <strong>æ¨¡å‹å¤„ç†</strong><br>\n",
    "            <small>ç¥ç»ç½‘ç»œ</small>\n",
    "        </div>\n",
    "        \n",
    "        <div style=\"font-size: 1.5em; color: #0277bd;\">â†’</div>\n",
    "        \n",
    "        <div style=\"text-align: center; min-width: 100px;\">\n",
    "            <div style=\"font-size: 2em; margin-bottom: 5px;\">ğŸ“Š</div>\n",
    "            <strong>æ¦‚ç‡è®¡ç®—</strong><br>\n",
    "            <small>ä¸‹ä¸€è¯é¢„æµ‹</small>\n",
    "        </div>\n",
    "        \n",
    "        <div style=\"font-size: 1.5em; color: #0277bd;\">â†’</div>\n",
    "        \n",
    "        <div style=\"text-align: center; min-width: 100px;\">\n",
    "            <div style=\"font-size: 2em; margin-bottom: 5px;\">ğŸ’¬</div>\n",
    "            <strong>ç”Ÿæˆæ–‡æœ¬</strong><br>\n",
    "            <small>æœ€ç»ˆè¾“å‡º</small>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(workflow_html))\n",
    "\n",
    "# Tokenæ¦‚å¿µæ¼”ç¤º\n",
    "def demonstrate_tokenization():\n",
    "    \"\"\"æ¼”ç¤ºæ–‡æœ¬TokenåŒ–è¿‡ç¨‹\"\"\"\n",
    "    \n",
    "    # å¦‚æœæœ‰tiktokenï¼Œå±•ç¤ºçœŸå®çš„tokenization\n",
    "    try:\n",
    "        import tiktoken\n",
    "        \n",
    "        # ä½¿ç”¨GPT-3.5çš„ç¼–ç å™¨\n",
    "        encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "        \n",
    "        test_texts = [\n",
    "            \"Hello, world!\",\n",
    "            \"LangChainæ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ¡†æ¶\",\n",
    "            \"äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œ\"\n",
    "        ]\n",
    "        \n",
    "        print(\"ğŸ”¤ TokenåŒ–æ¼”ç¤ºï¼ˆä½¿ç”¨GPT-3.5ç¼–ç å™¨ï¼‰:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for text in test_texts:\n",
    "            tokens = encoding.encode(text)\n",
    "            decoded_tokens = [encoding.decode([token]) for token in tokens]\n",
    "            \n",
    "            print(f\"åŸæ–‡: {text}\")\n",
    "            print(f\"Tokenæ•°: {len(tokens)}\")\n",
    "            print(f\"Tokenåˆ—è¡¨: {decoded_tokens}\")\n",
    "            print(f\"Token ID: {tokens}\")\n",
    "            print(\"-\" * 30)\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"ğŸ”¤ TokenåŒ–æ¦‚å¿µæ¼”ç¤ºï¼ˆæ¨¡æ‹Ÿï¼‰:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # ç®€å•çš„æ¨¡æ‹Ÿtokenization\n",
    "        test_text = \"LangChainæ˜¯ä¸€ä¸ªå¼ºå¤§çš„AIæ¡†æ¶\"\n",
    "        # ç®€åŒ–çš„åˆ†è¯ï¼ˆå®é™…TokenåŒ–æ›´å¤æ‚ï¼‰\n",
    "        mock_tokens = [\"Lang\", \"Chain\", \"æ˜¯\", \"ä¸€ä¸ª\", \"å¼ºå¤§\", \"çš„\", \"AI\", \"æ¡†æ¶\"]\n",
    "        \n",
    "        print(f\"åŸæ–‡: {test_text}\")\n",
    "        print(f\"æ¨¡æ‹ŸToken: {mock_tokens}\")\n",
    "        print(f\"Tokenæ•°é‡: {len(mock_tokens)}\")\n",
    "        print(\"\\nğŸ’¡ æ³¨æ„: å®é™…çš„TokenåŒ–ä¼šè€ƒè™‘å­è¯ã€æ ‡ç‚¹ç­‰å¤æ‚è§„åˆ™\")\n",
    "\n",
    "demonstrate_tokenization()\n",
    "\n",
    "complete_section(\"llm_concepts\")\n",
    "print(\"\\nâœ… ç« èŠ‚å®Œæˆï¼šLLMæ¦‚å¿µ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ğŸ”§ LangChainä¸­çš„LLMé›†æˆ\n",
    "\n",
    "LangChainæ”¯æŒå¤šç§LLMæä¾›å•†ï¼Œè®©æˆ‘ä»¬å­¦ä¹ å¦‚ä½•ä½¿ç”¨å®ƒä»¬ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": ["LLMé›†æˆ"]
   },
   "outputs": [],
   "source": [
    "# ğŸŒ å¤šLLMæä¾›å•†æ¼”ç¤º\n",
    "def demonstrate_llm_providers():\n",
    "    \"\"\"æ¼”ç¤ºä¸åŒLLMæä¾›å•†çš„ä½¿ç”¨\"\"\"\n",
    "    \n",
    "    llm_configs = {}\n",
    "    \n",
    "    print(\"ğŸŒ LLMæä¾›å•†é…ç½®æ¼”ç¤º\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # 1. OpenAIé…ç½®\n",
    "    print(\"\\n1ï¸âƒ£ OpenAI GPTæ¨¡å‹\")\n",
    "    try:\n",
    "        from langchain_openai import ChatOpenAI, OpenAI\n",
    "        \n",
    "        if os.getenv('OPENAI_API_KEY'):\n",
    "            # Chatæ¨¡å‹ï¼ˆæ¨èï¼‰\n",
    "            chat_llm = ChatOpenAI(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                temperature=0.7,\n",
    "                max_tokens=150,\n",
    "                timeout=30\n",
    "            )\n",
    "            \n",
    "            # è¡¥å…¨æ¨¡å‹\n",
    "            completion_llm = OpenAI(\n",
    "                model=\"gpt-3.5-turbo-instruct\",\n",
    "                temperature=0.5,\n",
    "                max_tokens=100\n",
    "            )\n",
    "            \n",
    "            llm_configs['openai_chat'] = chat_llm\n",
    "            llm_configs['openai_completion'] = completion_llm\n",
    "            \n",
    "            print(\"   âœ… ChatOpenAIé…ç½®æˆåŠŸ\")\n",
    "            print(\"   âœ… OpenAIè¡¥å…¨æ¨¡å‹é…ç½®æˆåŠŸ\")\n",
    "            \n",
    "        else:\n",
    "            print(\"   âš ï¸ éœ€è¦OPENAI_API_KEYç¯å¢ƒå˜é‡\")\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"   âŒ langchain_openaiæœªå®‰è£…\")\n",
    "    \n",
    "    # 2. Anthropicé…ç½®\n",
    "    print(\"\\n2ï¸âƒ£ Anthropic Claudeæ¨¡å‹\")\n",
    "    try:\n",
    "        from langchain_anthropic import ChatAnthropic\n",
    "        \n",
    "        if os.getenv('ANTHROPIC_API_KEY'):\n",
    "            claude_llm = ChatAnthropic(\n",
    "                model=\"claude-3-sonnet-20240229\",\n",
    "                temperature=0.7,\n",
    "                max_tokens=150\n",
    "            )\n",
    "            \n",
    "            llm_configs['anthropic'] = claude_llm\n",
    "            print(\"   âœ… Claudeé…ç½®æˆåŠŸ\")\n",
    "        else:\n",
    "            print(\"   âš ï¸ éœ€è¦ANTHROPIC_API_KEYç¯å¢ƒå˜é‡\")\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"   âš ï¸ langchain_anthropicæœªå®‰è£…ï¼ˆå¯é€‰ï¼‰\")\n",
    "    \n",
    "    # 3. æœ¬åœ°æ¨¡å‹é…ç½®ï¼ˆç¤ºä¾‹ï¼‰\n",
    "    print(\"\\n3ï¸âƒ£ æœ¬åœ°æ¨¡å‹é…ç½®ï¼ˆç¤ºä¾‹ï¼‰\")\n",
    "    try:\n",
    "        from langchain_community.llms import Ollama\n",
    "        \n",
    "        # æ³¨æ„ï¼šéœ€è¦æœ¬åœ°è¿è¡ŒOllamaæœåŠ¡\n",
    "        local_llm = Ollama(\n",
    "            model=\"llama2\",\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        # ä¸å®é™…æµ‹è¯•è¿æ¥ï¼Œåªæ˜¯å±•ç¤ºé…ç½®\n",
    "        print(\"   ğŸ“ Ollamaé…ç½®ç¤ºä¾‹ï¼ˆéœ€è¦æœ¬åœ°å®‰è£…ï¼‰\")\n",
    "        print(\"      å®‰è£…: https://ollama.ai/\")\n",
    "        print(\"      æ¨¡å‹: ollama pull llama2\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"   ğŸ“ æœ¬åœ°æ¨¡å‹éœ€è¦å®‰è£… langchain_community\")\n",
    "    \n",
    "    return llm_configs\n",
    "\n",
    "# è¿è¡Œæ¼”ç¤º\n",
    "available_llms = demonstrate_llm_providers()\n",
    "\n",
    "# æ˜¾ç¤ºå¯ç”¨çš„LLMæ€»ç»“\n",
    "if available_llms:\n",
    "    print(f\"\\nğŸ“‹ æœ¬æ¬¡è¯¾ç¨‹å¯ç”¨LLM: {list(available_llms.keys())}\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ æ— å¯ç”¨LLMï¼Œéƒ¨åˆ†æ¼”ç¤ºå°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®\")\n",
    "\n",
    "complete_section(\"llm_integration\")\n",
    "print(\"\\nâœ… ç« èŠ‚å®Œæˆï¼šLLMé›†æˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. âš™ï¸ LLMå‚æ•°è°ƒä¼˜\n",
    "\n",
    "äº†è§£å’ŒæŒæ¡LLMçš„å…³é”®å‚æ•°ï¼Œèƒ½å¤Ÿæ˜¾è‘—å½±å“æ¨¡å‹çš„è¡¨ç°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": ["å‚æ•°è°ƒä¼˜"]
   },
   "outputs": [],
   "source": [
    "# âš™ï¸ LLMå‚æ•°è¯¦è§£å’Œæ¼”ç¤º\n",
    "def demonstrate_llm_parameters():\n",
    "    \"\"\"æ¼”ç¤ºLLMçš„é‡è¦å‚æ•°\"\"\"\n",
    "    \n",
    "    # å‚æ•°è¯´æ˜è¡¨\n",
    "    params_html = \"\"\"\n",
    "    <div style=\"margin: 20px 0;\">\n",
    "        <h4>âš™ï¸ æ ¸å¿ƒå‚æ•°è¯´æ˜</h4>\n",
    "        <table style=\"width: 100%; border-collapse: collapse; font-size: 14px;\">\n",
    "            <thead>\n",
    "                <tr style=\"background-color: #f5f5f5;\">\n",
    "                    <th style=\"border: 1px solid #ddd; padding: 10px; text-align: left;\">å‚æ•°</th>\n",
    "                    <th style=\"border: 1px solid #ddd; padding: 10px; text-align: left;\">èŒƒå›´</th>\n",
    "                    <th style=\"border: 1px solid #ddd; padding: 10px; text-align: left;\">ä½œç”¨</th>\n",
    "                    <th style=\"border: 1px solid #ddd; padding: 10px; text-align: left;\">å»ºè®®å€¼</th>\n",
    "                </tr>\n",
    "            </thead>\n",
    "            <tbody>\n",
    "                <tr>\n",
    "                    <td style=\"border: 1px solid #ddd; padding: 8px;\"><strong>temperature</strong></td>\n",
    "                    <td style=\"border: 1px solid #ddd; padding: 8px;\">0.0 - 2.0</td>\n",
    "                    <td style=\"border: 1px solid #ddd; padding: 8px;\">æ§åˆ¶è¾“å‡ºçš„éšæœºæ€§å’Œåˆ›é€ æ€§</td>\n",
    "                    <td style=\"border: 1px solid #ddd; padding: 8px;\">äº‹å®é—®ç­”: 0.1-0.3<br>åˆ›æ„å†™ä½œ: 0.7-1.0</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"border: 1px solid #ddd; padding: 8px;\"><strong>max_tokens</strong></td>\n",
    "                    <td style=\"border: 1px solid #ddd; padding: 8px;\">1 - 4096+</td>\n",
    "                    <td style=\"border: 1px solid #ddd; padding: 8px;\">é™åˆ¶è¾“å‡ºé•¿åº¦</td>\n",
    "                    <td style=\"border: 1px solid #ddd; padding: 8px;\">æ‘˜è¦: 100-200<br>æ–‡ç« : 500-1000</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"border: 1px solid #ddd; padding: 8px;\"><strong>top_p</strong></td>\n",
    "                    <td style=\"border: 1px solid #ddd; padding: 8px;\">0.0 - 1.0</td>\n",
    "                    <td style=\"border: 1px solid #ddd; padding: 8px;\">æ ¸å¿ƒé‡‡æ ·ï¼Œæ§åˆ¶è¯æ±‡å¤šæ ·æ€§</td>\n",
    "                    <td style=\"border: 1px solid #ddd; padding: 8px;\">é€šå¸¸: 0.9-1.0</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"border: 1px solid #ddd; padding: 8px;\"><strong>frequency_penalty</strong></td>\n",
    "                    <td style=\"border: 1px solid #ddd; padding: 8px;\">-2.0 - 2.0</td>\n",
    "                    <td style=\"border: 1px solid #ddd; padding: 8px;\">å‡å°‘é‡å¤è¯æ±‡</td>\n",
    "                    <td style=\"border: 1px solid #ddd; padding: 8px;\">é¿å…é‡å¤: 0.1-0.5</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"border: 1px solid #ddd; padding: 8px;\"><strong>presence_penalty</strong></td>\n",
    "                    <td style=\"border: 1px solid #ddd; padding: 8px;\">-2.0 - 2.0</td>\n",
    "                    <td style=\"border: 1px solid #ddd; padding: 8px;\">é¼“åŠ±æ–°è¯é¢˜</td>\n",
    "                    <td style=\"border: 1px solid #ddd; padding: 8px;\">è¯é¢˜å¤šæ ·åŒ–: 0.1-0.3</td>\n",
    "                </tr>\n",
    "            </tbody>\n",
    "        </table>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    display(HTML(params_html))\n",
    "    \n",
    "    # å®é™…å‚æ•°æ¼”ç¤º\n",
    "    if available_llms:\n",
    "        print(\"\\nğŸ§ª å‚æ•°æ•ˆæœå¯¹æ¯”æ¼”ç¤º\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # é€‰æ‹©ä¸€ä¸ªå¯ç”¨çš„LLMè¿›è¡Œæ¼”ç¤º\n",
    "        llm_key = list(available_llms.keys())[0]\n",
    "        base_llm = available_llms[llm_key]\n",
    "        \n",
    "        # æµ‹è¯•æç¤ºè¯\n",
    "        test_prompt = \"æè¿°ä¸€ä¸ªæœªæ¥åŸå¸‚çš„ä¸€å¤©\"\n",
    "        \n",
    "        # ä¸åŒtemperatureè®¾ç½®\n",
    "        temp_configs = [\n",
    "            {\"name\": \"ä¿å®ˆå‹(0.1)\", \"temperature\": 0.1, \"description\": \"è¾“å‡ºç¡®å®šæ€§å¼ºï¼Œé‡å¤æ€§é«˜\"},\n",
    "            {\"name\": \"å¹³è¡¡å‹(0.7)\", \"temperature\": 0.7, \"description\": \"åˆ›é€ æ€§ä¸ä¸€è‡´æ€§å¹³è¡¡\"},\n",
    "            {\"name\": \"åˆ›æ„å‹(1.2)\", \"temperature\": 1.2, \"description\": \"é«˜åˆ›é€ æ€§ï¼Œæ›´å¤šå˜åŒ–\"}\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            from langchain_openai import ChatOpenAI\n",
    "            from langchain_core.messages import HumanMessage\n",
    "            \n",
    "            print(f\"ä½¿ç”¨ {llm_key} è¿›è¡Œtemperatureå¯¹æ¯”:\")\n",
    "            print(f\"æç¤ºè¯: '{test_prompt}'\\n\")\n",
    "            \n",
    "            for config in temp_configs:\n",
    "                print(f\"ğŸŒ¡ï¸ {config['name']} - {config['description']}\")\n",
    "                \n",
    "                # åˆ›å»ºé…ç½®åçš„LLM\n",
    "                configured_llm = ChatOpenAI(\n",
    "                    model=\"gpt-3.5-turbo\",\n",
    "                    temperature=config['temperature'],\n",
    "                    max_tokens=100\n",
    "                )\n",
    "                \n",
    "                try:\n",
    "                    response = configured_llm.invoke([HumanMessage(content=test_prompt)])\n",
    "                    print(f\"   å›å¤: {response.content[:150]}...\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   æ¼”ç¤ºè·³è¿‡ï¼ˆéœ€è¦APIå¯†é’¥ï¼‰: {str(e)[:50]}...\")\n",
    "                \n",
    "                print()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"å‚æ•°æ¼”ç¤ºéœ€è¦APIå¯†é’¥å’Œç½‘ç»œè¿æ¥\")\n",
    "    \n",
    "    else:\n",
    "        print(\"\\nğŸ’¡ å‚æ•°æ•ˆæœæ¨¡æ‹Ÿæ¼”ç¤º:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        demo_responses = {\n",
    "            \"ä½temperature(0.1)\": \"æœªæ¥åŸå¸‚å°†æœ‰é«˜æ•ˆçš„äº¤é€šç³»ç»Ÿå’Œæ¸…æ´èƒ½æºï¼Œäººä»¬çš„ç”Ÿæ´»æ›´åŠ ä¾¿åˆ©å’Œç¯ä¿ã€‚\",\n",
    "            \"ä¸­temperature(0.7)\": \"åœ¨2050å¹´çš„æ–°æµ·å¸‚ï¼Œæ™¨å…‰é€è¿‡é€æ˜çš„ç©ºæ°”å‡€åŒ–ç©¹é¡¶æ´’å‘å¤§åœ°ï¼Œæ™ºèƒ½é£è¡Œå™¨åœ¨æœ‰åºçš„èˆªé“ä¸­ç©¿æ¢­...\",\n",
    "            \"é«˜temperature(1.2)\": \"å¢¨ç»¿è‰²çš„è—¤è”“ç¼ ç»•ç€æ‘©å¤©å¤§æ¥¼ï¼Œå…¨æ¯é²¸é±¼åœ¨äº‘å±‚ä¸­æ¸¸å¼‹ï¼Œå±…æ°‘ä»¬é€šè¿‡æ„å¿µæ§åˆ¶ç€æ¼‚æµ®çš„å®¶å›­...\"\n",
    "        }\n",
    "        \n",
    "        for setting, response in demo_responses.items():\n",
    "            print(f\"ğŸŒ¡ï¸ {setting}:\")\n",
    "            print(f\"   {response}\")\n",
    "            print()\n",
    "\n",
    "demonstrate_llm_parameters()\n",
    "\n",
    "complete_section(\"parameter_tuning\")\n",
    "print(\"âœ… ç« èŠ‚å®Œæˆï¼šå‚æ•°è°ƒä¼˜\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ğŸ”„ å¼‚æ­¥å’Œæµå¼å¤„ç†\n",
    "\n",
    "å­¦ä¹ å¦‚ä½•ä½¿ç”¨å¼‚æ­¥è°ƒç”¨å’Œæµå¼å“åº”æ¥æé«˜ç”¨æˆ·ä½“éªŒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": ["å¼‚æ­¥å¤„ç†"]
   },
   "outputs": [],
   "source": [
    "# ğŸ”„ å¼‚æ­¥å’Œæµå¼å¤„ç†æ¼”ç¤º\n",
    "async def demonstrate_async_streaming():\n",
    "    \"\"\"æ¼”ç¤ºå¼‚æ­¥è°ƒç”¨å’Œæµå¼å¤„ç†\"\"\"\n",
    "    \n",
    "    print(\"ğŸ”„ å¼‚æ­¥å’Œæµå¼å¤„ç†æ¼”ç¤º\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    if not available_llms:\n",
    "        print(\"ğŸ“ æ¼”ç¤ºæ¦‚å¿µï¼ˆéœ€è¦APIå¯†é’¥æ‰èƒ½çœ‹åˆ°å®é™…æ•ˆæœï¼‰:\\n\")\n",
    "        \n",
    "        # æ¨¡æ‹Ÿå¼‚æ­¥è°ƒç”¨\n",
    "        print(\"1ï¸âƒ£ å¼‚æ­¥è°ƒç”¨æ¼”ç¤º:\")\n",
    "        print(\"   - å¯ä»¥å¹¶å‘å¤„ç†å¤šä¸ªè¯·æ±‚\")\n",
    "        print(\"   - ä¸é˜»å¡ä¸»çº¿ç¨‹\")\n",
    "        print(\"   - æé«˜åº”ç”¨å“åº”æ€§\")\n",
    "        \n",
    "        print(\"\\n2ï¸âƒ£ æµå¼å“åº”æ¼”ç¤º:\")\n",
    "        print(\"   - å®æ—¶è¿”å›ç”Ÿæˆçš„æ–‡æœ¬\")\n",
    "        print(\"   - å‡å°‘ç”¨æˆ·ç­‰å¾…æ—¶é—´\")\n",
    "        print(\"   - æ›´å¥½çš„äº¤äº’ä½“éªŒ\")\n",
    "        \n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        from langchain_openai import ChatOpenAI\n",
    "        from langchain_core.messages import HumanMessage\n",
    "        from langchain_core.callbacks import BaseCallbackHandler\n",
    "        import time\n",
    "        \n",
    "        # 1. å¼‚æ­¥è°ƒç”¨æ¼”ç¤º\n",
    "        print(\"1ï¸âƒ£ å¼‚æ­¥è°ƒç”¨æ¼”ç¤º\")\n",
    "        \n",
    "        llm = ChatOpenAI(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            temperature=0.7,\n",
    "            max_tokens=50\n",
    "        )\n",
    "        \n",
    "        # å‡†å¤‡å¤šä¸ªè¯·æ±‚\n",
    "        questions = [\n",
    "            \"ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ\",\n",
    "            \"ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ\",\n",
    "            \"ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ\"\n",
    "        ]\n",
    "        \n",
    "        # åŒæ­¥è°ƒç”¨æ—¶é—´æµ‹è¯•\n",
    "        print(\"\\nâ° åŒæ­¥è°ƒç”¨ï¼ˆé¡ºåºæ‰§è¡Œï¼‰:\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for i, question in enumerate(questions):\n",
    "            try:\n",
    "                response = llm.invoke([HumanMessage(content=question)])\n",
    "                print(f\"   Q{i+1}: {response.content[:50]}...\")\n",
    "            except:\n",
    "                print(f\"   Q{i+1}: [æ¨¡æ‹Ÿå›ç­”] è¿™æ˜¯å…³äº{question.split('æ˜¯')[1]}çš„è§£é‡Š...\")\n",
    "        \n",
    "        sync_time = time.time() - start_time\n",
    "        print(f\"   æ€»è€—æ—¶: {sync_time:.2f}ç§’\")\n",
    "        \n",
    "        # å¼‚æ­¥è°ƒç”¨æ—¶é—´æµ‹è¯•\n",
    "        print(\"\\nâš¡ å¼‚æ­¥è°ƒç”¨ï¼ˆå¹¶å‘æ‰§è¡Œï¼‰:\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡\n",
    "        async def ask_question(llm, question, index):\n",
    "            try:\n",
    "                response = await llm.ainvoke([HumanMessage(content=question)])\n",
    "                return f\"Q{index+1}: {response.content[:50]}...\"\n",
    "            except:\n",
    "                return f\"Q{index+1}: [æ¨¡æ‹Ÿå›ç­”] è¿™æ˜¯å…³äº{question.split('æ˜¯')[1]}çš„è§£é‡Š...\"\n",
    "        \n",
    "        # å¹¶å‘æ‰§è¡Œ\n",
    "        tasks = [ask_question(llm, q, i) for i, q in enumerate(questions)]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        for result in results:\n",
    "            print(f\"   {result}\")\n",
    "        \n",
    "        async_time = time.time() - start_time\n",
    "        print(f\"   æ€»è€—æ—¶: {async_time:.2f}ç§’\")\n",
    "        print(f\"   âš¡ é€Ÿåº¦æå‡: {sync_time/async_time:.1f}x\")\n",
    "        \n",
    "        # 2. æµå¼å“åº”æ¼”ç¤º\n",
    "        print(\"\\n2ï¸âƒ£ æµå¼å“åº”æ¼”ç¤º\")\n",
    "        \n",
    "        class StreamingCallback(BaseCallbackHandler):\n",
    "            \"\"\"æµå¼è¾“å‡ºå›è°ƒ\"\"\"\n",
    "            def on_llm_new_token(self, token: str, **kwargs):\n",
    "                print(token, end=\"\", flush=True)\n",
    "        \n",
    "        streaming_llm = ChatOpenAI(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            temperature=0.7,\n",
    "            max_tokens=100,\n",
    "            streaming=True,\n",
    "            callbacks=[StreamingCallback()]\n",
    "        )\n",
    "        \n",
    "        print(\"\\nğŸ“ æµå¼ç”Ÿæˆæ–‡æœ¬ï¼ˆè¯·è§‚å¯Ÿé€å­—è¾“å‡ºï¼‰:\")\n",
    "        print(\"é—®é¢˜: æè¿°LangChainçš„ä¸»è¦ä¼˜åŠ¿\")\n",
    "        print(\"å›ç­”: \", end=\"\")\n",
    "        \n",
    "        try:\n",
    "            response = await streaming_llm.ainvoke(\n",
    "                [HumanMessage(content=\"æè¿°LangChainçš„ä¸»è¦ä¼˜åŠ¿ï¼Œçº¦50å­—\")]\n",
    "            )\n",
    "            print(\"\\n\")\n",
    "        except:\n",
    "            # æ¨¡æ‹Ÿæµå¼è¾“å‡º\n",
    "            mock_response = \"LangChainæä¾›äº†æ¨¡å—åŒ–çš„AIåº”ç”¨å¼€å‘æ¡†æ¶ï¼Œæ”¯æŒå¤šç§LLMé›†æˆï¼Œç®€åŒ–äº†å¤æ‚AIåº”ç”¨çš„æ„å»ºè¿‡ç¨‹ã€‚\"\n",
    "            for char in mock_response:\n",
    "                print(char, end=\"\", flush=True)\n",
    "                await asyncio.sleep(0.03)  # æ¨¡æ‹Ÿæ‰“å­—æ•ˆæœ\n",
    "            print(\"\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"å¼‚æ­¥æ¼”ç¤ºéœ€è¦APIå¯†é’¥: {e}\")\n",
    "        \n",
    "        # æ˜¾ç¤ºæ¦‚å¿µè¯´æ˜\n",
    "        concepts_html = \"\"\"\n",
    "        <div style=\"background-color: #f8f9fa; padding: 15px; border-radius: 8px; margin: 15px 0;\">\n",
    "            <h4>ğŸ’¡ å¼‚æ­¥å’Œæµå¼å¤„ç†çš„ä¼˜åŠ¿</h4>\n",
    "            <div style=\"display: grid; grid-template-columns: 1fr 1fr; gap: 20px;\">\n",
    "                <div>\n",
    "                    <h5 style=\"color: #2196F3;\">ğŸ”„ å¼‚æ­¥å¤„ç†</h5>\n",
    "                    <ul style=\"font-size: 14px;\">\n",
    "                        <li>å¹¶å‘å¤„ç†å¤šä¸ªè¯·æ±‚</li>\n",
    "                        <li>ä¸é˜»å¡ç”¨æˆ·ç•Œé¢</li>\n",
    "                        <li>æé«˜ç³»ç»Ÿååé‡</li>\n",
    "                        <li>æ›´å¥½çš„èµ„æºåˆ©ç”¨</li>\n",
    "                    </ul>\n",
    "                </div>\n",
    "                <div>\n",
    "                    <h5 style=\"color: #4CAF50;\">ğŸ“¡ æµå¼å“åº”</h5>\n",
    "                    <ul style=\"font-size: 14px;\">\n",
    "                        <li>å®æ—¶æ˜¾ç¤ºç”Ÿæˆå†…å®¹</li>\n",
    "                        <li>å‡å°‘ç”¨æˆ·ç­‰å¾…æ—¶é—´</li>\n",
    "                        <li>æä¾›å³æ—¶åé¦ˆ</li>\n",
    "                        <li>æ”¹å–„ç”¨æˆ·ä½“éªŒ</li>\n",
    "                    </ul>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        display(HTML(concepts_html))\n",
    "\n",
    "# è¿è¡Œå¼‚æ­¥æ¼”ç¤º\n",
    "await demonstrate_async_streaming()\n",
    "\n",
    "complete_section(\"async_streaming\")\n",
    "print(\"\\nâœ… ç« èŠ‚å®Œæˆï¼šå¼‚æ­¥å’Œæµå¼å¤„ç†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ğŸ’° æˆæœ¬ä¼˜åŒ–ä¸æ€§èƒ½ç›‘æ§\n",
    "\n",
    "äº†è§£å¦‚ä½•ä¼˜åŒ–LLMä½¿ç”¨æˆæœ¬å’Œç›‘æ§æ€§èƒ½æŒ‡æ ‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": ["æˆæœ¬ä¼˜åŒ–"]
   },
   "outputs": [],
   "source": [
    "# ğŸ’° æˆæœ¬ä¼˜åŒ–å’Œæ€§èƒ½ç›‘æ§\n",
    "def demonstrate_cost_optimization():\n",
    "    \"\"\"æ¼”ç¤ºæˆæœ¬ä¼˜åŒ–ç­–ç•¥å’Œæ€§èƒ½ç›‘æ§\"\"\"\n",
    "    \n",
    "    print(\"ğŸ’° LLMæˆæœ¬ä¼˜åŒ–ä¸æ€§èƒ½ç›‘æ§\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # 1. æˆæœ¬ç»“æ„åˆ†æ\n",
    "    cost_html = \"\"\"\n",
    "    <div style=\"margin: 20px 0;\">\n",
    "        <h4>ğŸ’µ ä¸»è¦LLMæˆæœ¬å¯¹æ¯”ï¼ˆ2024å¹´ï¼‰</h4>\n",
    "        <table style=\"width: 100%; border-collapse: collapse; font-size: 14px;\">\n",
    "            <thead>\n",
    "                <tr style=\"background-color: #f5f5f5;\">\n",
    "                    <th style=\"border: 1px solid #ddd; padding: 10px;\">æ¨¡å‹</th>\n",
    "                    <th style=\"border: 1px solid #ddd; padding: 10px;\">è¾“å…¥ä»·æ ¼</th>\n",
    "                    <th style=\"border: 1px solid #ddd; padding: 10px;\">è¾“å‡ºä»·æ ¼</th>\n",
    "                    <th style=\"border: 1px solid #ddd; padding: 10px;\">é€‚ç”¨åœºæ™¯</th>\n",
    "                </tr>\n",
    "            </thead>\n",
    "            <tbody>\n",
    "                <tr>\n",
    "                    <td style=\"border: 1px solid #ddd; padding: 8px;\">GPT-3.5 Turbo</td>\n",
    "                    <td style=\"border: 1px solid #ddd; padding: 8px;\">$0.5/1M tokens</td>\n",
    "                    <td style=\"border: 1px solid #ddd; padding: 8px;\">$1.5/1M tokens</td>\n",
    "                    <td style=\"border: 1px solid #ddd; padding: 8px;\">é€šç”¨ä»»åŠ¡ï¼Œæˆæœ¬æ•æ„Ÿ</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"border: 1px solid #ddd; padding: 8px;\">GPT-4</td>\n",
    "                    <td style=\"border: 1px solid #ddd; padding: 8px;\">$10/1M tokens</td>\n",
    "                    <td style=\"border: 1px solid #ddd; padding: 8px;\">$30/1M tokens</td>\n",
    "                    <td style=\"border: 1px solid #ddd; padding: 8px;\">å¤æ‚æ¨ç†ï¼Œé«˜è´¨é‡</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"border: 1px solid #ddd; padding: 8px;\">Claude-3 Sonnet</td>\n",
    "                    <td style=\"border: 1px solid #ddd; padding: 8px;\">$3/1M tokens</td>\n",
    "                    <td style=\"border: 1px solid #ddd; padding: 8px;\">$15/1M tokens</td>\n",
    "                    <td style=\"border: 1px solid #ddd; padding: 8px;\">å¹³è¡¡æ€§èƒ½å’Œæˆæœ¬</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"border: 1px solid #ddd; padding: 8px;\">æœ¬åœ°æ¨¡å‹</td>\n",
    "                    <td style=\"border: 1px solid #ddd; padding: 8px;\">ç¡¬ä»¶æˆæœ¬</td>\n",
    "                    <td style=\"border: 1px solid #ddd; padding: 8px;\">ç”µåŠ›æˆæœ¬</td>\n",
    "                    <td style=\"border: 1px solid #ddd; padding: 8px;\">éšç§æ•æ„Ÿï¼Œå¤§é‡ä½¿ç”¨</td>\n",
    "                </tr>\n",
    "            </tbody>\n",
    "        </table>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    display(HTML(cost_html))\n",
    "    \n",
    "    # 2. Tokenè®¡ç®—å’Œæˆæœ¬ä¼°ç®—\n",
    "    print(\"\\nğŸ“Š Tokenè®¡ç®—å’Œæˆæœ¬ä¼°ç®—\")\n",
    "    \n",
    "    def calculate_cost(text: str, model: str = \"gpt-3.5-turbo\"):\n",
    "        \"\"\"è®¡ç®—æ–‡æœ¬çš„æˆæœ¬\"\"\"\n",
    "        \n",
    "        # ç®€åŒ–çš„tokenè®¡ç®—ï¼ˆå®é™…åº”è¯¥ä½¿ç”¨tiktokenï¼‰\n",
    "        estimated_tokens = len(text.split()) * 1.3  # ç²—ç•¥ä¼°ç®—\n",
    "        \n",
    "        # ä»·æ ¼è¡¨ï¼ˆç¾å…ƒ/1M tokensï¼‰\n",
    "        prices = {\n",
    "            \"gpt-3.5-turbo\": {\"input\": 0.5, \"output\": 1.5},\n",
    "            \"gpt-4\": {\"input\": 10, \"output\": 30},\n",
    "            \"claude-3-sonnet\": {\"input\": 3, \"output\": 15}\n",
    "        }\n",
    "        \n",
    "        if model in prices:\n",
    "            input_cost = (estimated_tokens / 1_000_000) * prices[model][\"input\"]\n",
    "            output_cost = (estimated_tokens / 1_000_000) * prices[model][\"output\"]  # å‡è®¾è¾“å‡ºé•¿åº¦ç›¸è¿‘\n",
    "            total_cost = input_cost + output_cost\n",
    "            \n",
    "            return {\n",
    "                \"tokens\": int(estimated_tokens),\n",
    "                \"input_cost\": input_cost,\n",
    "                \"output_cost\": output_cost,\n",
    "                \"total_cost\": total_cost\n",
    "            }\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    # ç¤ºä¾‹æ–‡æœ¬\n",
    "    sample_texts = [\n",
    "        \"ä»€ä¹ˆæ˜¯AIï¼Ÿ\",\n",
    "        \"è¯·è¯¦ç»†è§£é‡Šæœºå™¨å­¦ä¹ çš„å·¥ä½œåŸç†ï¼ŒåŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ çš„åŒºåˆ«ã€‚\",\n",
    "        \"ç¼–å†™ä¸€ä¸ªå®Œæ•´çš„Pythonç¨‹åºï¼Œå®ç°ä¸€ä¸ªç®€å•çš„èŠå¤©æœºå™¨äººï¼ŒåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ„å›¾è¯†åˆ«å’Œå“åº”ç”ŸæˆåŠŸèƒ½ã€‚è¦æ±‚ä»£ç ç»“æ„æ¸…æ™°ï¼ŒåŒ…å«è¯¦ç»†æ³¨é‡Šã€‚\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nğŸ’¸ ä¸åŒæ–‡æœ¬é•¿åº¦çš„æˆæœ¬å¯¹æ¯”:\")\n",
    "    \n",
    "    for i, text in enumerate(sample_texts, 1):\n",
    "        print(f\"\\nğŸ“ ç¤ºä¾‹{i}: {text[:50]}...\")\n",
    "        \n",
    "        for model in [\"gpt-3.5-turbo\", \"gpt-4\", \"claude-3-sonnet\"]:\n",
    "            cost_info = calculate_cost(text, model)\n",
    "            if cost_info:\n",
    "                print(f\"   {model}:\")\n",
    "                print(f\"     - Tokens: ~{cost_info['tokens']}\")\n",
    "                print(f\"     - æˆæœ¬: ${cost_info['total_cost']:.6f}\")\n",
    "    \n",
    "    # 3. ä¼˜åŒ–ç­–ç•¥\n",
    "    print(\"\\nğŸ¯ æˆæœ¬ä¼˜åŒ–ç­–ç•¥\")\n",
    "    \n",
    "    optimization_strategies = [\n",
    "        \"ğŸ’¡ æç¤ºè¯ä¼˜åŒ–: ç®€æ´æ˜ç¡®çš„æç¤ºè¯å‡å°‘tokenä½¿ç”¨\",\n",
    "        \"ğŸ”„ æ¨¡å‹é€‰æ‹©: æ ¹æ®ä»»åŠ¡å¤æ‚åº¦é€‰æ‹©åˆé€‚æ¨¡å‹\",\n",
    "        \"ğŸ“ é•¿åº¦æ§åˆ¶: è®¾ç½®åˆç†çš„max_tokensé™åˆ¶\",\n",
    "        \"ğŸ¯ æ‰¹é‡å¤„ç†: åˆå¹¶å¤šä¸ªç›¸å…³æŸ¥è¯¢\",\n",
    "        \"ğŸ’¾ ç»“æœç¼“å­˜: ç¼“å­˜å¸¸è§æŸ¥è¯¢çš„ç»“æœ\",\n",
    "        \"âš–ï¸ æ··åˆç­–ç•¥: ç»“åˆå¤šä¸ªæ¨¡å‹çš„ä¼˜åŠ¿\",\n",
    "        \"ğŸ“Š ç›‘æ§å‘Šè­¦: è®¾ç½®æˆæœ¬é˜ˆå€¼å’Œå‘Šè­¦\"\n",
    "    ]\n",
    "    \n",
    "    for strategy in optimization_strategies:\n",
    "        print(f\"   {strategy}\")\n",
    "    \n",
    "    # 4. æ€§èƒ½ç›‘æ§å®ç°\n",
    "    print(\"\\nğŸ“ˆ æ€§èƒ½ç›‘æ§å®ç°\")\n",
    "    \n",
    "    class LLMMonitor:\n",
    "        \"\"\"LLMæ€§èƒ½ç›‘æ§ç±»\"\"\"\n",
    "        \n",
    "        def __init__(self):\n",
    "            self.calls = []\n",
    "            self.total_tokens = 0\n",
    "            self.total_cost = 0.0\n",
    "        \n",
    "        def log_call(self, model: str, input_tokens: int, output_tokens: int, duration: float, cost: float):\n",
    "            \"\"\"è®°å½•APIè°ƒç”¨\"\"\"\n",
    "            call_info = {\n",
    "                \"timestamp\": time.time(),\n",
    "                \"model\": model,\n",
    "                \"input_tokens\": input_tokens,\n",
    "                \"output_tokens\": output_tokens,\n",
    "                \"total_tokens\": input_tokens + output_tokens,\n",
    "                \"duration\": duration,\n",
    "                \"cost\": cost\n",
    "            }\n",
    "            \n",
    "            self.calls.append(call_info)\n",
    "            self.total_tokens += call_info[\"total_tokens\"]\n",
    "            self.total_cost += cost\n",
    "        \n",
    "        def get_stats(self):\n",
    "            \"\"\"è·å–ç»Ÿè®¡ä¿¡æ¯\"\"\"\n",
    "            if not self.calls:\n",
    "                return {\"total_calls\": 0}\n",
    "            \n",
    "            avg_duration = sum(call[\"duration\"] for call in self.calls) / len(self.calls)\n",
    "            avg_tokens = sum(call[\"total_tokens\"] for call in self.calls) / len(self.calls)\n",
    "            \n",
    "            return {\n",
    "                \"total_calls\": len(self.calls),\n",
    "                \"total_tokens\": self.total_tokens,\n",
    "                \"total_cost\": self.total_cost,\n",
    "                \"avg_duration\": avg_duration,\n",
    "                \"avg_tokens\": avg_tokens,\n",
    "                \"cost_per_call\": self.total_cost / len(self.calls)\n",
    "            }\n",
    "    \n",
    "    # æ¼”ç¤ºç›‘æ§å™¨ä½¿ç”¨\n",
    "    monitor = LLMMonitor()\n",
    "    \n",
    "    # æ¨¡æ‹Ÿä¸€äº›APIè°ƒç”¨\n",
    "    import random\n",
    "    \n",
    "    for i in range(5):\n",
    "        monitor.log_call(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            input_tokens=random.randint(50, 200),\n",
    "            output_tokens=random.randint(30, 150),\n",
    "            duration=random.uniform(0.5, 3.0),\n",
    "            cost=random.uniform(0.001, 0.01)\n",
    "        )\n",
    "    \n",
    "    stats = monitor.get_stats()\n",
    "    \n",
    "    print(\"\\nğŸ“Š ç›‘æ§æ•°æ®ç¤ºä¾‹:\")\n",
    "    print(f\"   æ€»è°ƒç”¨æ¬¡æ•°: {stats['total_calls']}\")\n",
    "    print(f\"   æ€»Tokenæ•°: {stats['total_tokens']}\")\n",
    "    print(f\"   æ€»æˆæœ¬: ${stats['total_cost']:.4f}\")\n",
    "    print(f\"   å¹³å‡å“åº”æ—¶é—´: {stats['avg_duration']:.2f}ç§’\")\n",
    "    print(f\"   å¹³å‡æ¯æ¬¡æˆæœ¬: ${stats['cost_per_call']:.4f}\")\n",
    "\n",
    "demonstrate_cost_optimization()\n",
    "\n",
    "complete_section(\"cost_optimization\")\n",
    "print(\"\\nâœ… ç« èŠ‚å®Œæˆï¼šæˆæœ¬ä¼˜åŒ–\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ğŸ’ª å®è·µç»ƒä¹ \n",
    "\n",
    "é€šè¿‡å®é™…ç»ƒä¹ æ¥å·©å›ºLLMçš„ä½¿ç”¨æŠ€èƒ½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": ["ç»ƒä¹ 1"]
   },
   "outputs": [],
   "source": [
    "# ğŸ‹ï¸ ç»ƒä¹ 1: LLMå‚æ•°è°ƒä¼˜å®éªŒ\n",
    "\n",
    "exercise1_html = \"\"\"\n",
    "<div class=\"exercise-box\">\n",
    "    <div class=\"header\">\n",
    "        <h4>ğŸ‹ï¸ ç»ƒä¹ 1: LLMå‚æ•°è°ƒä¼˜å®éªŒ</h4>\n",
    "        <small>éš¾åº¦: â­â­â­â˜†â˜†</small>\n",
    "    </div>\n",
    "    <div class=\"content\">\n",
    "        <p><strong>ä»»åŠ¡</strong>: åˆ›å»ºä¸€ä¸ªå‚æ•°å¯¹æ¯”å®éªŒï¼Œæµ‹è¯•ä¸åŒtemperatureå€¼å¯¹åˆ›æ„å†™ä½œçš„å½±å“ã€‚</p>\n",
    "        <p><strong>è¦æ±‚</strong>:</p>\n",
    "        <ul>\n",
    "            <li>æµ‹è¯•è‡³å°‘3ä¸ªä¸åŒçš„temperatureå€¼ (0.1, 0.7, 1.2)</li>\n",
    "            <li>ä½¿ç”¨ç›¸åŒçš„åˆ›æ„å†™ä½œæç¤ºè¯</li>\n",
    "            <li>æ¯”è¾ƒè¾“å‡ºçš„åˆ›é€ æ€§å’Œä¸€è‡´æ€§</li>\n",
    "            <li>åˆ†æç»“æœå¹¶å¾—å‡ºç»“è®º</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(exercise1_html))\n",
    "\n",
    "def exercise_1_solution():\n",
    "    \"\"\"ç»ƒä¹ 1: LLMå‚æ•°è°ƒä¼˜å®éªŒ\"\"\"\n",
    "    \n",
    "    print(\"ğŸ§ª å¼€å§‹å‚æ•°è°ƒä¼˜å®éªŒ\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # åˆ›æ„å†™ä½œæç¤ºè¯\n",
    "    creative_prompt = \"å†™ä¸€ä¸ªå…³äºæ—¶é—´æ—…è¡Œè€…åœ¨å¤ä»£é‡åˆ°å›°éš¾çš„çŸ­æ•…äº‹å¼€å¤´ï¼ˆ50å­—å†…ï¼‰\"\n",
    "    \n",
    "    # æµ‹è¯•ä¸åŒtemperatureå€¼\n",
    "    temperature_configs = [\n",
    "        {\"temp\": 0.1, \"name\": \"ä¿å®ˆå‹\", \"expected\": \"è¾ƒä¸ºå¹³å®ã€é€»è¾‘æ€§å¼º\"},\n",
    "        {\"temp\": 0.7, \"name\": \"å¹³è¡¡å‹\", \"expected\": \"åˆ›æ„ä¸é€»è¾‘å¹³è¡¡\"},\n",
    "        {\"temp\": 1.2, \"name\": \"åˆ›æ„å‹\", \"expected\": \"é«˜åº¦åˆ›æ–°ã€æƒ³è±¡ä¸°å¯Œ\"}\n",
    "    ]\n",
    "    \n",
    "    print(f\"ğŸ“ æµ‹è¯•æç¤ºè¯: {creative_prompt}\\n\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for config in temperature_configs:\n",
    "        print(f\"ğŸŒ¡ï¸ Temperature {config['temp']} ({config['name']})\")\n",
    "        print(f\"   é¢„æœŸç‰¹ç‚¹: {config['expected']}\")\n",
    "        \n",
    "        if available_llms:\n",
    "            try:\n",
    "                from langchain_openai import ChatOpenAI\n",
    "                from langchain_core.messages import HumanMessage\n",
    "                \n",
    "                # åˆ›å»ºé…ç½®çš„LLM\n",
    "                llm = ChatOpenAI(\n",
    "                    model=\"gpt-3.5-turbo\",\n",
    "                    temperature=config['temp'],\n",
    "                    max_tokens=80\n",
    "                )\n",
    "                \n",
    "                response = llm.invoke([HumanMessage(content=creative_prompt)])\n",
    "                generated_text = response.content\n",
    "                \n",
    "                print(f\"   ç”Ÿæˆç»“æœ: {generated_text}\")\n",
    "                \n",
    "                # ç®€å•çš„åˆ›æ„è¯„åˆ†ï¼ˆåŸºäºè¯æ±‡å¤šæ ·æ€§å’Œæƒ…èŠ‚å¤æ‚åº¦ï¼‰\n",
    "                words = generated_text.split()\n",
    "                unique_words = len(set(words))\n",
    "                creativity_score = unique_words / len(words) if words else 0\n",
    "                \n",
    "                results.append({\n",
    "                    \"temperature\": config['temp'],\n",
    "                    \"text\": generated_text,\n",
    "                    \"word_count\": len(words),\n",
    "                    \"unique_words\": unique_words,\n",
    "                    \"creativity_score\": creativity_score\n",
    "                })\n",
    "                \n",
    "                print(f\"   è¯æ±‡å¤šæ ·æ€§: {creativity_score:.2f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   éœ€è¦APIå¯†é’¥è¿›è¡Œå®é™…æµ‹è¯•\")\n",
    "                \n",
    "                # æä¾›æ¨¡æ‹Ÿç»“æœ\n",
    "                mock_results = {\n",
    "                    0.1: \"ææ˜ç©¿è¶Šåˆ°æ˜æœï¼Œå‘ç°è‡ªå·±çš„ç°ä»£è£…æ‰®å¼•èµ·äº†æ‘æ°‘çš„å›´è§‚å’Œæ€€ç–‘ã€‚\",\n",
    "                    0.7: \"æ—¶ç©ºè£‚ç¼å°†è‰¾ç±³é€åˆ°äº†ä¸­ä¸–çºªçš„åŸå ¡ï¼Œå¥¹çš„æ™ºèƒ½æ‰‹è¡¨åœ¨çƒ›å…‰ä¸‹æ˜¾å¾—æ ¼å¤–é†’ç›®ã€‚\",\n",
    "                    1.2: \"å¢¨è‰²æ—‹æ¶¡åå™¬äº†æ°å…‹ï¼Œå½“ä»–ççœ¼æ—¶ï¼Œç¿¡ç¿ è‰²çš„é¾™æ­£å‡è§†ç€ä»–æ‰‹ä¸­é—ªçƒçš„é‡å­è£…ç½®ã€‚\"\n",
    "                }\n",
    "                \n",
    "                mock_text = mock_results.get(config['temp'], \"æ¨¡æ‹Ÿç”Ÿæˆçš„åˆ›æ„æ–‡æœ¬\")\n",
    "                print(f\"   æ¨¡æ‹Ÿç»“æœ: {mock_text}\")\n",
    "                \n",
    "                words = mock_text.split()\n",
    "                creativity_score = len(set(words)) / len(words)\n",
    "                \n",
    "                results.append({\n",
    "                    \"temperature\": config['temp'],\n",
    "                    \"text\": mock_text,\n",
    "                    \"creativity_score\": creativity_score\n",
    "                })\n",
    "                \n",
    "                print(f\"   æ¨¡æ‹Ÿåˆ›æ„åˆ†æ•°: {creativity_score:.2f}\")\n",
    "        \n",
    "        else:\n",
    "            # å®Œå…¨æ¨¡æ‹Ÿçš„ç»“æœ\n",
    "            mock_results = {\n",
    "                0.1: \"ææ˜ç©¿è¶Šåˆ°æ˜æœï¼Œå‘ç°è‡ªå·±çš„ç°ä»£è£…æ‰®å¼•èµ·äº†æ‘æ°‘çš„å›´è§‚ã€‚\",\n",
    "                0.7: \"æ—¶ç©ºè£‚ç¼å°†è‰¾ç±³é€åˆ°ä¸­ä¸–çºªï¼Œå¥¹çš„æ™ºèƒ½æ‰‹è¡¨åœ¨çƒ›å…‰ä¸‹é—ªé—ªå‘å…‰ã€‚\",\n",
    "                1.2: \"å¢¨è‰²æ¼©æ¶¡åå™¬æ°å…‹ï¼Œç¿¡ç¿ é¾™å‡è§†ç€ä»–æ‰‹ä¸­çš„é‡å­è£…ç½®ï¼Œæ—¶é—´åœ¨æ­¤åˆ»é™æ­¢ã€‚\"\n",
    "            }\n",
    "            \n",
    "            mock_text = mock_results[config['temp']]\n",
    "            print(f\"   æ¨¡æ‹Ÿç»“æœ: {mock_text}\")\n",
    "            \n",
    "            results.append({\n",
    "                \"temperature\": config['temp'],\n",
    "                \"text\": mock_text,\n",
    "                \"creativity_score\": config['temp']  # ç®€åŒ–ï¼štemperatureè¶Šé«˜åˆ›æ„åˆ†è¶Šé«˜\n",
    "            })\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # åˆ†æç»“æœ\n",
    "    print(\"ğŸ“Š å®éªŒç»“æœåˆ†æ:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    if results:\n",
    "        # æŒ‰åˆ›æ„åˆ†æ•°æ’åº\n",
    "        sorted_results = sorted(results, key=lambda x: x['creativity_score'])\n",
    "        \n",
    "        print(\"åˆ›æ„ç¨‹åº¦æ’åº (ä½åˆ°é«˜):\")\n",
    "        for result in sorted_results:\n",
    "            print(f\"   Temperature {result['temperature']}: {result['creativity_score']:.2f}\")\n",
    "        \n",
    "        print(\"\\nğŸ’¡ ç»“è®º:\")\n",
    "        if sorted_results[-1]['temperature'] > sorted_results[0]['temperature']:\n",
    "            print(\"   âœ… éªŒè¯äº†temperatureè¶Šé«˜ï¼Œè¾“å‡ºè¶Šæœ‰åˆ›æ„\")\n",
    "        else:\n",
    "            print(\"   âš ï¸ ç»“æœè¡¨æ˜åˆ›æ„ä¸å®Œå…¨å–å†³äºtemperature\")\n",
    "        \n",
    "        print(\"   ğŸ“ å»ºè®®: æ ¹æ®å…·ä½“ä»»åŠ¡é€‰æ‹©åˆé€‚çš„temperatureå€¼\")\n",
    "        print(\"   ğŸ“ äº‹å®æ€§ä»»åŠ¡: 0.1-0.3\")\n",
    "        print(\"   ğŸ“ åˆ›æ„ä»»åŠ¡: 0.7-1.2\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# è¿è¡Œç»ƒä¹ 1\n",
    "exercise1_results = exercise_1_solution()\n",
    "\n",
    "# è¯„ä¼°ç»ƒä¹ å®Œæˆæƒ…å†µ\n",
    "if len(exercise1_results) >= 3:\n",
    "    # æ£€æŸ¥æ˜¯å¦æµ‹è¯•äº†ä¸åŒçš„temperatureå€¼\n",
    "    temps = [r['temperature'] for r in exercise1_results]\n",
    "    has_variety = max(temps) - min(temps) >= 1.0\n",
    "    \n",
    "    if has_variety:\n",
    "        print(\"\\nğŸ‰ ç»ƒä¹ 1å®Œæˆä¼˜ç§€ï¼\")\n",
    "        print(\"   - æˆåŠŸæµ‹è¯•äº†å¤šä¸ªtemperatureå€¼\")\n",
    "        print(\"   - è¿›è¡Œäº†ç»“æœåˆ†æå’Œæ¯”è¾ƒ\")\n",
    "        print(\"   - å¾—å‡ºäº†åˆç†ç»“è®º\")\n",
    "        complete_exercise(\"exercise_1\", 95)\n",
    "    else:\n",
    "        print(\"\\nâœ… ç»ƒä¹ 1åŸºæœ¬å®Œæˆ\")\n",
    "        print(\"   å»ºè®®: å°è¯•æ›´å¤§çš„temperatureå·®å¼‚\")\n",
    "        complete_exercise(\"exercise_1\", 80)\n",
    "else:\n",
    "    print(\"\\nâš ï¸ ç»ƒä¹ 1éœ€è¦å®Œå–„\")\n",
    "    complete_exercise(\"exercise_1\", 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": ["ç»ƒä¹ 2"]
   },
   "outputs": [],
   "source": [
    "# ğŸ‹ï¸ ç»ƒä¹ 2: å¤šæ¨¡å‹æ€§èƒ½å¯¹æ¯”\n",
    "\n",
    "exercise2_html = \"\"\"\n",
    "<div class=\"exercise-box\">\n",
    "    <div class=\"header\">\n",
    "        <h4>ğŸ‹ï¸ ç»ƒä¹ 2: å¤šæ¨¡å‹æ€§èƒ½å¯¹æ¯”</h4>\n",
    "        <small>éš¾åº¦: â­â­â­â­â˜†</small>\n",
    "    </div>\n",
    "    <div class=\"content\">\n",
    "        <p><strong>ä»»åŠ¡</strong>: åˆ›å»ºä¸€ä¸ªå¤šæ¨¡å‹å¯¹æ¯”ç³»ç»Ÿï¼Œæµ‹è¯•ä¸åŒæ¨¡å‹åœ¨ç›¸åŒä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚</p>\n",
    "        <p><strong>è¦æ±‚</strong>:</p>\n",
    "        <ul>\n",
    "            <li>å¯¹æ¯”è‡³å°‘2ç§ä¸åŒæ¨¡å‹ï¼ˆå¦‚GPT-3.5 vs GPT-4ï¼‰</li>\n",
    "            <li>æµ‹è¯•3ç§ä¸åŒç±»å‹çš„ä»»åŠ¡ï¼ˆäº‹å®é—®ç­”ã€åˆ›æ„å†™ä½œã€é€»è¾‘æ¨ç†ï¼‰</li>\n",
    "            <li>è®°å½•å“åº”æ—¶é—´ã€è´¨é‡è¯„åˆ†</li>\n",
    "            <li>æä¾›ç»¼åˆè¯„ä¼°å’Œä½¿ç”¨å»ºè®®</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(exercise2_html))\n",
    "\n",
    "def exercise_2_solution():\n",
    "    \"\"\"ç»ƒä¹ 2: å¤šæ¨¡å‹æ€§èƒ½å¯¹æ¯”\"\"\"\n",
    "    \n",
    "    print(\"ğŸ”¬ å¤šæ¨¡å‹æ€§èƒ½å¯¹æ¯”å®éªŒ\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # å®šä¹‰æµ‹è¯•ä»»åŠ¡\n",
    "    test_tasks = [\n",
    "        {\n",
    "            \"type\": \"äº‹å®é—®ç­”\",\n",
    "            \"prompt\": \"åŒ—äº¬çš„äººå£å¤§çº¦æ˜¯å¤šå°‘ï¼Ÿè¯·æä¾›2023å¹´çš„æ•°æ®ã€‚\",\n",
    "            \"evaluation_criteria\": [\"å‡†ç¡®æ€§\", \"æ—¶æ•ˆæ€§\", \"ç®€æ´æ€§\"]\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"åˆ›æ„å†™ä½œ\",\n",
    "            \"prompt\": \"å†™ä¸€é¦–å…³äºäººå·¥æ™ºèƒ½ä¸äººç±»å‹è°Šçš„ç°ä»£è¯—ï¼Œ4è¡Œã€‚\",\n",
    "            \"evaluation_criteria\": [\"åˆ›æ„æ€§\", \"éŸµå¾‹æ„Ÿ\", \"ä¸»é¢˜å¥‘åˆ\"]\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"é€»è¾‘æ¨ç†\",\n",
    "            \"prompt\": \"å¦‚æœAæ¯”Bé«˜ï¼ŒBæ¯”Cé«˜ï¼ŒCæ¯”Dé«˜ï¼Œä¸”Dèº«é«˜170cmï¼Œé‚£ä¹ˆAçš„èº«é«˜æœ€å°‘æ˜¯å¤šå°‘ï¼Ÿ\",\n",
    "            \"evaluation_criteria\": [\"é€»è¾‘æ­£ç¡®æ€§\", \"æ¨ç†è¿‡ç¨‹\", \"ç»“è®ºæ˜ç¡®æ€§\"]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # å®šä¹‰æ¨¡å‹é…ç½®\n",
    "    model_configs = [\n",
    "        {\n",
    "            \"name\": \"GPT-3.5-Turbo\",\n",
    "            \"model\": \"gpt-3.5-turbo\",\n",
    "            \"cost_per_1k\": 0.002,\n",
    "            \"description\": \"å¿«é€Ÿã€ç»æµçš„é€šç”¨æ¨¡å‹\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"GPT-4\",\n",
    "            \"model\": \"gpt-4\",\n",
    "            \"cost_per_1k\": 0.03,\n",
    "            \"description\": \"é«˜è´¨é‡ã€å¼ºæ¨ç†èƒ½åŠ›\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for task in test_tasks:\n",
    "        print(f\"\\nğŸ“‹ æµ‹è¯•ä»»åŠ¡: {task['type']}\")\n",
    "        print(f\"   æç¤ºè¯: {task['prompt']}\")\n",
    "        print(f\"   è¯„ä¼°æ ‡å‡†: {', '.join(task['evaluation_criteria'])}\")\n",
    "        print()\n",
    "        \n",
    "        task_results = []\n",
    "        \n",
    "        for model_config in model_configs:\n",
    "            print(f\"ğŸ¤– æ¨¡å‹: {model_config['name']}\")\n",
    "            \n",
    "            if available_llms:\n",
    "                try:\n",
    "                    from langchain_openai import ChatOpenAI\n",
    "                    from langchain_core.messages import HumanMessage\n",
    "                    import time\n",
    "                    \n",
    "                    # åˆ›å»ºæ¨¡å‹å®ä¾‹\n",
    "                    llm = ChatOpenAI(\n",
    "                        model=model_config['model'],\n",
    "                        temperature=0.7,\n",
    "                        max_tokens=150\n",
    "                    )\n",
    "                    \n",
    "                    # è®°å½•å“åº”æ—¶é—´\n",
    "                    start_time = time.time()\n",
    "                    response = llm.invoke([HumanMessage(content=task['prompt'])])\n",
    "                    response_time = time.time() - start_time\n",
    "                    \n",
    "                    result_text = response.content\n",
    "                    \n",
    "                    print(f\"   å“åº”æ—¶é—´: {response_time:.2f}ç§’\")\n",
    "                    print(f\"   å“åº”å†…å®¹: {result_text[:100]}...\")\n",
    "                    \n",
    "                    # ç®€å•çš„è´¨é‡è¯„åˆ†ï¼ˆå®é™…åº”è¯¥æ›´å¤æ‚ï¼‰\n",
    "                    quality_score = min(95, 70 + len(result_text.split()) * 2)  # åŸºäºé•¿åº¦çš„ç®€åŒ–è¯„åˆ†\n",
    "                    \n",
    "                    task_results.append({\n",
    "                        \"model\": model_config['name'],\n",
    "                        \"response_time\": response_time,\n",
    "                        \"response\": result_text,\n",
    "                        \"quality_score\": quality_score,\n",
    "                        \"estimated_cost\": len(result_text.split()) * model_config['cost_per_1k'] / 1000\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"   éœ€è¦APIå¯†é’¥: {str(e)[:50]}...\")\n",
    "                    \n",
    "                    # æ¨¡æ‹Ÿç»“æœ\n",
    "                    mock_responses = {\n",
    "                        \"GPT-3.5-Turbo\": {\n",
    "                            \"äº‹å®é—®ç­”\": \"åŒ—äº¬å¸‚2023å¹´å¸¸ä½äººå£çº¦ä¸º2185ä¸‡äººã€‚\",\n",
    "                            \"åˆ›æ„å†™ä½œ\": \"ç¡…èŠ¯ä¸è¡€è‚‰å¿ƒï¼Œå…±èˆåœ¨æ•°å­—æ˜Ÿç©ºï¼Œ\\nç®—æ³•æ¸©æŸ”å¦‚æ˜¥é£ï¼Œå‹è°Šä¹‹èŠ±æ°¸ç››å¼€ã€‚\",\n",
    "                            \"é€»è¾‘æ¨ç†\": \"ç”±äºA>B>C>Dï¼Œä¸”D=170cmï¼Œæ‰€ä»¥Aè‡³å°‘è¦æ¯”Dé«˜3cmä»¥ä¸Šï¼Œå³Aâ‰¥173cmã€‚\"\n",
    "                        },\n",
    "                        \"GPT-4\": {\n",
    "                            \"äº‹å®é—®ç­”\": \"æ ¹æ®æœ€æ–°ç»Ÿè®¡ï¼ŒåŒ—äº¬å¸‚2023å¹´å¸¸ä½äººå£çº¦ä¸º2185ä¸‡äººï¼Œå…¶ä¸­æˆ·ç±äººå£çº¦1400ä¸‡äººã€‚\",\n",
    "                            \"åˆ›æ„å†™ä½œ\": \"ä»£ç ä¸çµé­‚äº¤ç»‡èˆï¼Œ\\næ™ºæ…§ä¹‹å…‰ç…§äººå¿ƒï¼Œ\\né‡‘å±æ‰‹æ¡è‚‰èº«æŒï¼Œ\\næ°¸æ’å‹è°Šèƒœé»„é‡‘ã€‚\",\n",
    "                            \"é€»è¾‘æ¨ç†\": \"è®¾D=170cmã€‚ç”±æ¡ä»¶A>B>C>Då¯çŸ¥ï¼ŒAè‡³å°‘éœ€è¦æ¯”Dé«˜ï¼Œä½†é¢˜ç›®é—®æœ€å°‘é«˜åº¦ï¼Œè€ƒè™‘åˆ°'æ¯”...é«˜'æ„å‘³ç€ä¸¥æ ¼å¤§äºï¼Œæ‰€ä»¥A>170cmã€‚ç†è®ºä¸ŠAçš„æœ€å°å€¼æ¥è¿‘ä½†å¤§äº170cmã€‚\"\n",
    "                        }\n",
    "                    }\n",
    "                    \n",
    "                    mock_response = mock_responses[model_config['name']][task['type']]\n",
    "                    mock_time = 1.5 if model_config['name'] == \"GPT-3.5-Turbo\" else 2.8\n",
    "                    mock_quality = 85 if model_config['name'] == \"GPT-3.5-Turbo\" else 92\n",
    "                    \n",
    "                    print(f\"   æ¨¡æ‹Ÿå“åº”æ—¶é—´: {mock_time:.2f}ç§’\")\n",
    "                    print(f\"   æ¨¡æ‹Ÿå“åº”: {mock_response}\")\n",
    "                    print(f\"   æ¨¡æ‹Ÿè´¨é‡åˆ†æ•°: {mock_quality}/100\")\n",
    "                    \n",
    "                    task_results.append({\n",
    "                        \"model\": model_config['name'],\n",
    "                        \"response_time\": mock_time,\n",
    "                        \"response\": mock_response,\n",
    "                        \"quality_score\": mock_quality,\n",
    "                        \"estimated_cost\": len(mock_response.split()) * model_config['cost_per_1k'] / 1000\n",
    "                    })\n",
    "            \n",
    "            else:\n",
    "                # çº¯æ¨¡æ‹Ÿç»“æœ\n",
    "                mock_data = {\n",
    "                    \"GPT-3.5-Turbo\": {\"time\": 1.2, \"quality\": 82, \"response\": \"æ¨¡æ‹ŸGPT-3.5å“åº”\"},\n",
    "                    \"GPT-4\": {\"time\": 2.5, \"quality\": 91, \"response\": \"æ¨¡æ‹ŸGPT-4é«˜è´¨é‡å“åº”\"}\n",
    "                }\n",
    "                \n",
    "                data = mock_data[model_config['name']]\n",
    "                print(f\"   æ¨¡æ‹Ÿæ•°æ® - æ—¶é—´: {data['time']}s, è´¨é‡: {data['quality']}/100\")\n",
    "                \n",
    "                task_results.append({\n",
    "                    \"model\": model_config['name'],\n",
    "                    \"response_time\": data['time'],\n",
    "                    \"quality_score\": data['quality'],\n",
    "                    \"response\": data['response']\n",
    "                })\n",
    "            \n",
    "            print()\n",
    "        \n",
    "        results.append({\n",
    "            \"task_type\": task['type'],\n",
    "            \"results\": task_results\n",
    "        })\n",
    "    \n",
    "    # ç»¼åˆåˆ†æ\n",
    "    print(\"\\nğŸ“Š ç»¼åˆåˆ†ææŠ¥å‘Š\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # æŒ‰æ¨¡å‹æ±‡æ€»æ€§èƒ½\n",
    "    model_summary = {}\n",
    "    \n",
    "    for model_config in model_configs:\n",
    "        model_name = model_config['name']\n",
    "        model_summary[model_name] = {\n",
    "            \"avg_time\": 0,\n",
    "            \"avg_quality\": 0,\n",
    "            \"total_cost\": 0,\n",
    "            \"task_count\": 0\n",
    "        }\n",
    "    \n",
    "    for task_result in results:\n",
    "        for result in task_result['results']:\n",
    "            model_name = result['model']\n",
    "            summary = model_summary[model_name]\n",
    "            \n",
    "            summary['avg_time'] += result['response_time']\n",
    "            summary['avg_quality'] += result['quality_score']\n",
    "            summary['total_cost'] += result.get('estimated_cost', 0)\n",
    "            summary['task_count'] += 1\n",
    "    \n",
    "    # è®¡ç®—å¹³å‡å€¼\n",
    "    for model_name, summary in model_summary.items():\n",
    "        if summary['task_count'] > 0:\n",
    "            summary['avg_time'] /= summary['task_count']\n",
    "            summary['avg_quality'] /= summary['task_count']\n",
    "    \n",
    "    # æ˜¾ç¤ºå¯¹æ¯”ç»“æœ\n",
    "    print(\"ğŸ† æ¨¡å‹æ€§èƒ½å¯¹æ¯”:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    for model_name, summary in model_summary.items():\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"   å¹³å‡å“åº”æ—¶é—´: {summary['avg_time']:.2f}ç§’\")\n",
    "        print(f\"   å¹³å‡è´¨é‡åˆ†æ•°: {summary['avg_quality']:.1f}/100\")\n",
    "        print(f\"   ä¼°ç®—æˆæœ¬: ${summary['total_cost']:.6f}\")\n",
    "    \n",
    "    # ä½¿ç”¨å»ºè®®\n",
    "    print(\"\\nğŸ’¡ ä½¿ç”¨å»ºè®®:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    best_quality = max(model_summary.values(), key=lambda x: x['avg_quality'])\n",
    "    fastest = min(model_summary.values(), key=lambda x: x['avg_time'])\n",
    "    cheapest = min(model_summary.values(), key=lambda x: x['total_cost'])\n",
    "    \n",
    "    # æ‰¾åˆ°å¯¹åº”çš„æ¨¡å‹åç§°\n",
    "    for name, data in model_summary.items():\n",
    "        if data == best_quality:\n",
    "            print(f\"ğŸ¯ è´¨é‡æœ€ä½³: {name} (åˆ†æ•°: {data['avg_quality']:.1f})\")\n",
    "        if data == fastest:\n",
    "            print(f\"âš¡ é€Ÿåº¦æœ€å¿«: {name} (æ—¶é—´: {data['avg_time']:.2f}s)\")\n",
    "        if data == cheapest:\n",
    "            print(f\"ğŸ’° æˆæœ¬æœ€ä½: {name} (æˆæœ¬: ${data['total_cost']:.6f})\")\n",
    "    \n",
    "    print(\"\\nğŸ“ åº”ç”¨åœºæ™¯å»ºè®®:\")\n",
    "    print(\"   â€¢ å¿«é€Ÿå“åº”éœ€æ±‚ â†’ é€‰æ‹©é€Ÿåº¦æœ€å¿«çš„æ¨¡å‹\")\n",
    "    print(\"   â€¢ é«˜è´¨é‡è¦æ±‚ â†’ é€‰æ‹©è´¨é‡æœ€ä½³çš„æ¨¡å‹\")\n",
    "    print(\"   â€¢ å¤§é‡è°ƒç”¨ â†’ é€‰æ‹©æˆæœ¬æœ€ä½çš„æ¨¡å‹\")\n",
    "    print(\"   â€¢ å¹³è¡¡éœ€æ±‚ â†’ ç»¼åˆè€ƒè™‘è´¨é‡ã€é€Ÿåº¦ã€æˆæœ¬\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# è¿è¡Œç»ƒä¹ 2\n",
    "exercise2_results = exercise_2_solution()\n",
    "\n",
    "# è¯„ä¼°ç»ƒä¹ å®Œæˆæƒ…å†µ\n",
    "if len(exercise2_results) >= 3:  # è‡³å°‘3ä¸ªä»»åŠ¡\n",
    "    # æ£€æŸ¥æ˜¯å¦å¯¹æ¯”äº†å¤šä¸ªæ¨¡å‹\n",
    "    models_tested = set()\n",
    "    for task_result in exercise2_results:\n",
    "        for result in task_result['results']:\n",
    "            models_tested.add(result['model'])\n",
    "    \n",
    "    if len(models_tested) >= 2:\n",
    "        print(\"\\nğŸ… ç»ƒä¹ 2å®Œæˆå“è¶Šï¼\")\n",
    "        print(\"   - æˆåŠŸå¯¹æ¯”äº†å¤šä¸ªæ¨¡å‹\")\n",
    "        print(\"   - æµ‹è¯•äº†å¤šç§ä»»åŠ¡ç±»å‹\")\n",
    "        print(\"   - æä¾›äº†ç»¼åˆåˆ†æå’Œå»ºè®®\")\n",
    "        complete_exercise(\"exercise_2\", 98)\n",
    "    else:\n",
    "        print(\"\\nâœ… ç»ƒä¹ 2åŸºæœ¬å®Œæˆ\")\n",
    "        print(\"   å»ºè®®: å¢åŠ æ›´å¤šæ¨¡å‹å¯¹æ¯”\")\n",
    "        complete_exercise(\"exercise_2\", 85)\n",
    "else:\n",
    "    print(\"\\nâš ï¸ ç»ƒä¹ 2éœ€è¦å®Œå–„\")\n",
    "    complete_exercise(\"exercise_2\", 70)\n",
    "\n",
    "complete_section(\"practical_exercises\")\n",
    "print(\"\\nâœ… ç« èŠ‚å®Œæˆï¼šå®è·µç»ƒä¹ \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š è¯¾ç¨‹æ€»ç»“ä¸è¿›åº¦\n",
    "\n",
    "æ­å–œä½ å®Œæˆäº†å¤§è¯­è¨€æ¨¡å‹åŸºç¡€è¯¾ç¨‹ï¼è®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹å­¦ä¹ æˆæœã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": ["è¯¾ç¨‹æ€»ç»“"]
   },
   "outputs": [],
   "source": [
    "# ğŸ“ˆ è¯¾ç¨‹æ€»ç»“å’Œè¿›åº¦å±•ç¤º\n",
    "try:\n",
    "    from progress_tracker import get_tracker, end_lesson\n",
    "    \n",
    "    tracker = get_tracker()\n",
    "    lesson_progress = tracker.get_lesson_progress(\"02_llm_basics\")\n",
    "    overall_progress = tracker.get_overall_progress()\n",
    "    \n",
    "    # è¯¾ç¨‹æ€»ç»“\n",
    "    summary_html = f\"\"\"\n",
    "    <div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 25px; border-radius: 15px; margin: 20px 0;\">\n",
    "        <h3 style=\"margin-top: 0; text-align: center;\">ğŸ§  LLMåŸºç¡€è¯¾ç¨‹å®Œæˆæ€»ç»“</h3>\n",
    "        \n",
    "        <div style=\"display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 20px; margin: 20px 0;\">\n",
    "            <div style=\"background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px; text-align: center;\">\n",
    "                <div style=\"font-size: 2.5em; margin-bottom: 10px;\">ğŸ“š</div>\n",
    "                <div style=\"font-size: 1.8em; font-weight: bold;\">{lesson_progress['completed_sections']}/{lesson_progress['total_sections']}</div>\n",
    "                <div>ç« èŠ‚å®Œæˆ</div>\n",
    "            </div>\n",
    "            <div style=\"background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px; text-align: center;\">\n",
    "                <div style=\"font-size: 2.5em; margin-bottom: 10px;\">ğŸ’ª</div>\n",
    "                <div style=\"font-size: 1.8em; font-weight: bold;\">{lesson_progress['completed_exercises']}/{lesson_progress['total_exercises']}</div>\n",
    "                <div>ç»ƒä¹ å®Œæˆ</div>\n",
    "            </div>\n",
    "            <div style=\"background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px; text-align: center;\">\n",
    "                <div style=\"font-size: 2.5em; margin-bottom: 10px;\">ğŸ¯</div>\n",
    "                <div style=\"font-size: 1.8em; font-weight: bold;\">{lesson_progress['progress_percentage']:.0f}%</div>\n",
    "                <div>å®Œæˆç‡</div>\n",
    "            </div>\n",
    "            <div style=\"background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px; text-align: center;\">\n",
    "                <div style=\"font-size: 2.5em; margin-bottom: 10px;\">â±ï¸</div>\n",
    "                <div style=\"font-size: 1.8em; font-weight: bold;\">{lesson_progress['total_time_minutes']:.0f}</div>\n",
    "                <div>å­¦ä¹ åˆ†é’Ÿ</div>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    display(HTML(summary_html))\n",
    "    \n",
    "    # å­¦ä¹ æˆæœå±•ç¤º\n",
    "    achievements_html = \"\"\"\n",
    "    <div style=\"background-color: #f8f9fa; padding: 20px; border-radius: 10px; margin: 15px 0;\">\n",
    "        <h4 style=\"color: #2c3e50; margin-top: 0;\">ğŸ¯ ä½ å·²ç»æŒæ¡çš„æŠ€èƒ½</h4>\n",
    "        <div style=\"display: grid; grid-template-columns: 1fr 1fr; gap: 20px;\">\n",
    "            <div>\n",
    "                <h5 style=\"color: #27ae60;\">ğŸ“– ç†è®ºçŸ¥è¯†</h5>\n",
    "                <ul style=\"line-height: 1.8;\">\n",
    "                    <li>âœ… LLMçš„åŸºæœ¬æ¦‚å¿µå’Œå·¥ä½œåŸç†</li>\n",
    "                    <li>âœ… TokenåŒ–å’Œæ–‡æœ¬å¤„ç†æœºåˆ¶</li>\n",
    "                    <li>âœ… ä¸åŒLLMæä¾›å•†çš„ç‰¹ç‚¹</li>\n",
    "                    <li>âœ… æˆæœ¬ç»“æ„å’Œä¼˜åŒ–ç­–ç•¥</li>\n",
    "                </ul>\n",
    "            </div>\n",
    "            <div>\n",
    "                <h5 style=\"color: #e74c3c;\">ğŸ› ï¸ å®è·µæŠ€èƒ½</h5>\n",
    "                <ul style=\"line-height: 1.8;\">\n",
    "                    <li>âœ… é…ç½®å’Œä½¿ç”¨å¤šç§LLM</li>\n",
    "                    <li>âœ… å‚æ•°è°ƒä¼˜å’Œæ€§èƒ½ä¼˜åŒ–</li>\n",
    "                    <li>âœ… å¼‚æ­¥è°ƒç”¨å’Œæµå¼å¤„ç†</li>\n",
    "                    <li>âœ… æ€§èƒ½ç›‘æ§å’Œæˆæœ¬æ§åˆ¶</li>\n",
    "                </ul>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    display(HTML(achievements_html))\n",
    "    \n",
    "    # æ˜¾ç¤ºæˆå°±\n",
    "    achievements = tracker.get_achievements()\n",
    "    recent_achievements = [a for a in achievements if a.unlocked][-2:]  # æœ€è¿‘2ä¸ªæˆå°±\n",
    "    \n",
    "    if recent_achievements:\n",
    "        achievements_display = \"<div style='margin: 15px 0;'><h4>ğŸ† æœ€æ–°è§£é”æˆå°±</h4><div style='display: flex; gap: 15px; flex-wrap: wrap;'>\"\n",
    "        \n",
    "        for achievement in recent_achievements:\n",
    "            achievements_display += f\"\"\"\n",
    "            <div style=\"border: 2px solid #FFD700; border-radius: 10px; padding: 15px; background: linear-gradient(135deg, #fff9c4 0%, #fff8a1 100%); min-width: 180px; text-align: center;\">\n",
    "                <div style=\"font-size: 32px; margin-bottom: 8px;\">{achievement.icon}</div>\n",
    "                <div style=\"font-weight: bold; color: #b8860b; margin-bottom: 5px;\">{achievement.name}</div>\n",
    "                <div style=\"font-size: 13px; color: #8b7355;\">{achievement.description}</div>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        \n",
    "        achievements_display += \"</div></div>\"\n",
    "        display(HTML(achievements_display))\n",
    "    \n",
    "    # ä¸‹ä¸€æ­¥å­¦ä¹ å»ºè®®\n",
    "    next_steps_html = \"\"\"\n",
    "    <div style=\"background: linear-gradient(135deg, #74b9ff 0%, #0984e3 100%); color: white; padding: 20px; border-radius: 10px; margin: 20px 0;\">\n",
    "        <h4 style=\"margin-top: 0;\">ğŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ è·¯å¾„</h4>\n",
    "        \n",
    "        <div style=\"display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-top: 15px;\">\n",
    "            <div>\n",
    "                <h5 style=\"margin-bottom: 10px;\">ğŸ“š æ¨èè¯¾ç¨‹</h5>\n",
    "                <ul style=\"line-height: 1.6; margin: 0; padding-left: 20px;\">\n",
    "                    <li><strong>03_prompts_templates.ipynb</strong><br>æŒæ¡é«˜çº§æç¤ºè¯æŠ€å·§</li>\n",
    "                    <li><strong>02_æ ¸å¿ƒç»„ä»¶/01_chains_introduction.ipynb</strong><br>å­¦ä¹ é“¾å¼ç»„åˆ</li>\n",
    "                    <li><strong>02_æ ¸å¿ƒç»„ä»¶/02_agents_basics.ipynb</strong><br>æ¢ç´¢æ™ºèƒ½ä»£ç†</li>\n",
    "                </ul>\n",
    "            </div>\n",
    "            <div>\n",
    "                <h5 style=\"margin-bottom: 10px;\">ğŸ’¡ å­¦ä¹ å»ºè®®</h5>\n",
    "                <ul style=\"line-height: 1.6; margin: 0; padding-left: 20px;\">\n",
    "                    <li>æ¯å¤©åšæŒå­¦ä¹ 30-60åˆ†é’Ÿ</li>\n",
    "                    <li>å¤šåŠ¨æ‰‹å®è·µï¼Œå°è¯•ä¸åŒå‚æ•°</li>\n",
    "                    <li>å»ºç«‹è‡ªå·±çš„ä»£ç ç‰‡æ®µåº“</li>\n",
    "                    <li>å…³æ³¨LLMé¢†åŸŸçš„æœ€æ–°å‘å±•</li>\n",
    "                </ul>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    display(HTML(next_steps_html))\n",
    "    \n",
    "    # ç»“æŸè¯¾ç¨‹\n",
    "    end_lesson()\n",
    "    \n",
    "    print(\"ğŸ‰ æ­å–œå®Œæˆå¤§è¯­è¨€æ¨¡å‹åŸºç¡€è¯¾ç¨‹ï¼\")\n",
    "    print(\"ğŸ“ˆ ä½ çš„AIå¼€å‘æŠ€èƒ½åˆæå‡äº†ä¸€ä¸ªå±‚æ¬¡ï¼\")\n",
    "    \n",
    "except ImportError:\n",
    "    # ç®€åŒ–ç‰ˆè¿›åº¦æ˜¾ç¤º\n",
    "    print(\"ğŸ§  å¤§è¯­è¨€æ¨¡å‹åŸºç¡€è¯¾ç¨‹ - å®Œæˆæ€»ç»“\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"âœ… å­¦ä¹ ç« èŠ‚: 6/6\")\n",
    "    print(\"âœ… å®Œæˆç»ƒä¹ : 2/2\")\n",
    "    print(\"âœ… æŒæ¡æŠ€èƒ½: LLMé…ç½®ã€å‚æ•°è°ƒä¼˜ã€æ€§èƒ½ä¼˜åŒ–\")\n",
    "    print(\"âœ… å®è·µèƒ½åŠ›: å¤šæ¨¡å‹å¯¹æ¯”ã€æˆæœ¬æ§åˆ¶\")\n",
    "    print(\"\\nğŸ‰ æ­å–œå®Œæˆå¤§è¯­è¨€æ¨¡å‹åŸºç¡€è¯¾ç¨‹ï¼\")\n",
    "    print(\"ğŸ“š å»ºè®®ç»§ç»­å­¦ä¹ ï¼šæç¤ºè¯æ¨¡æ¿å’Œé“¾å¼ç»„åˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸŠ è¯¾ç¨‹æ€»ç»“\n",
    "\n",
    "### ğŸŒŸ æ ¸å¿ƒæ”¶è·\n",
    "\n",
    "é€šè¿‡æœ¬è¯¾ç¨‹ï¼Œä½ å·²ç»ç³»ç»ŸæŒæ¡äº†ï¼š\n",
    "\n",
    "1. **ğŸ§  LLMæ ¸å¿ƒæ¦‚å¿µ** - æ·±å…¥ç†è§£å¤§è¯­è¨€æ¨¡å‹çš„å·¥ä½œåŸç†å’ŒæŠ€æœ¯ç‰¹ç‚¹\n",
    "2. **ğŸ”§ å®è·µæŠ€èƒ½** - ç†Ÿç»ƒé…ç½®å’Œä½¿ç”¨å¤šç§LLMæä¾›å•†çš„æœåŠ¡\n",
    "3. **âš™ï¸ å‚æ•°è°ƒä¼˜** - æŒæ¡å…³é”®å‚æ•°å¯¹æ¨¡å‹è¾“å‡ºçš„å½±å“å’Œä¼˜åŒ–ç­–ç•¥\n",
    "4. **ğŸš€ é«˜çº§ç‰¹æ€§** - å­¦ä¼šä½¿ç”¨å¼‚æ­¥è°ƒç”¨ã€æµå¼å“åº”ç­‰æå‡ç”¨æˆ·ä½“éªŒ\n",
    "5. **ğŸ’° æˆæœ¬æ§åˆ¶** - äº†è§£æˆæœ¬ç»“æ„å¹¶å®æ–½æœ‰æ•ˆçš„ä¼˜åŒ–æªæ–½\n",
    "\n",
    "### ğŸ“š æŠ€èƒ½æ¸…å•\n",
    "\n",
    "âœ… **æ¨¡å‹é€‰æ‹©** - æ ¹æ®ä»»åŠ¡éœ€æ±‚é€‰æ‹©æœ€é€‚åˆçš„LLM  \n",
    "âœ… **å‚æ•°é…ç½®** - ç†Ÿç»ƒè°ƒèŠ‚temperatureã€max_tokensç­‰å…³é”®å‚æ•°  \n",
    "âœ… **æ€§èƒ½ä¼˜åŒ–** - å®ç°å¼‚æ­¥è°ƒç”¨å’Œæµå¼å¤„ç†æå‡å“åº”é€Ÿåº¦  \n",
    "âœ… **æˆæœ¬ç®¡ç†** - ç›‘æ§ä½¿ç”¨é‡å¹¶ä¼˜åŒ–æˆæœ¬æ•ˆç›Š  \n",
    "âœ… **è´¨é‡è¯„ä¼°** - å»ºç«‹è¯„ä¼°ä½“ç³»å¯¹æ¯”ä¸åŒæ¨¡å‹æ€§èƒ½  \n",
    "\n",
    "### ğŸ›¤ï¸ å­¦ä¹ è·¯å¾„å»ºè®®\n",
    "\n",
    "ç°åœ¨ä½ å·²ç»å…·å¤‡äº†æ‰å®çš„LLMåŸºç¡€ï¼Œå»ºè®®æŒ‰ä»¥ä¸‹è·¯å¾„ç»§ç»­æ·±å…¥ï¼š\n",
    "\n",
    "1. **æç¤ºè¯å·¥ç¨‹** â†’ `03_prompts_templates.ipynb`\n",
    "2. **é“¾å¼ç»„åˆ** â†’ `02_æ ¸å¿ƒç»„ä»¶/01_chains_introduction.ipynb`  \n",
    "3. **æ™ºèƒ½ä»£ç†** â†’ `02_æ ¸å¿ƒç»„ä»¶/02_agents_basics.ipynb`\n",
    "4. **è®°å¿†ç³»ç»Ÿ** â†’ `02_æ ¸å¿ƒç»„ä»¶/03_memory_systems.ipynb`\n",
    "\n",
    "### ğŸ’¡ æŒç»­å­¦ä¹ \n",
    "\n",
    "LLMæŠ€æœ¯å‘å±•è¿…é€Ÿï¼Œå»ºè®®ï¼š\n",
    "- ğŸ” å…³æ³¨æœ€æ–°æ¨¡å‹å‘å¸ƒå’ŒæŠ€æœ¯è¿›å±•\n",
    "- ğŸ› ï¸ å¤šå‚ä¸å®é™…é¡¹ç›®ç»ƒä¹ \n",
    "- ğŸ‘¥ åŠ å…¥AIå¼€å‘è€…ç¤¾åŒºäº¤æµ\n",
    "- ğŸ“– æŒç»­é˜…è¯»ç›¸å…³è®ºæ–‡å’Œæ–‡æ¡£\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align: center; margin: 30px 0; padding: 20px; background: linear-gradient(45deg, #ff9a9e, #fecfef, #fecfef); border-radius: 15px;\">\n",
    "<h3 style=\"margin: 0; color: #2c3e50;\">ğŸ“ ä½ å·²ç»æˆä¸ºLLMåº”ç”¨å¼€å‘è€…ï¼</h3>\n",
    "<p style=\"margin: 15px 0 0 0; color: #34495e; font-size: 16px;\">ç»§ç»­æ¢ç´¢AIçš„æ— é™å¯èƒ½å§ï¼</p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}