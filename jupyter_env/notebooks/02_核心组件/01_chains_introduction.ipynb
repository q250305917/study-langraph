{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ”— LangChainé“¾ç³»ç»Ÿå…¥é—¨\n",
    "\n",
    "## ğŸ¯ å­¦ä¹ ç›®æ ‡\n",
    "\n",
    "åœ¨æœ¬è¯¾ç¨‹ä¸­ï¼Œä½ å°†å­¦ä¹ ï¼š\n",
    "- Chainï¼ˆé“¾ï¼‰çš„æ ¸å¿ƒæ¦‚å¿µå’ŒåŸç†\n",
    "- åŸºç¡€é“¾ç±»å‹å’Œä½¿ç”¨æ–¹æ³•\n",
    "- è‡ªå®šä¹‰é“¾çš„åˆ›å»ºå’Œé…ç½®\n",
    "- é“¾çš„ç»„åˆå’ŒåµŒå¥—æŠ€æœ¯\n",
    "- é”™è¯¯å¤„ç†å’Œè°ƒè¯•æŠ€å·§\n",
    "- å®é™…åº”ç”¨åœºæ™¯å’Œæœ€ä½³å®è·µ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆå§‹åŒ–å­¦ä¹ ç¯å¢ƒ\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../utils')\n",
    "\n",
    "from progress_tracker import ProgressTracker\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# åˆå§‹åŒ–è¿›åº¦è¿½è¸ªå™¨\n",
    "tracker = ProgressTracker()\n",
    "lesson_id = \"01_chains_introduction\"\n",
    "tracker.start_lesson(lesson_id, \"LangChainé“¾ç³»ç»Ÿå…¥é—¨\")\n",
    "\n",
    "print(\"ğŸš€ æ¬¢è¿æ¥åˆ°LangChainé“¾ç³»ç»Ÿè¯¾ç¨‹ï¼\")\n",
    "print(\"ğŸ“Š æ­£åœ¨åˆå§‹åŒ–å­¦ä¹ ç¯å¢ƒ...\")\n",
    "\n",
    "# æ˜¾ç¤ºå­¦ä¹ è¿›åº¦\n",
    "tracker.display_progress_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ğŸ” ä»€ä¹ˆæ˜¯Chainï¼ˆé“¾ï¼‰ï¼Ÿ\n",
    "\n",
    "### 1.1 åŸºæœ¬æ¦‚å¿µ\n",
    "\n",
    "Chainæ˜¯LangChainçš„æ ¸å¿ƒæ¦‚å¿µï¼Œå®ƒå°†å¤šä¸ªç»„ä»¶ï¼ˆå¦‚æç¤ºè¯ã€LLMã€è¾“å‡ºè§£æå™¨ï¼‰è¿æ¥æˆä¸€ä¸ªå®Œæ•´çš„å¤„ç†æµç¨‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¯å¢ƒé…ç½®æ£€æŸ¥\n",
    "try:\n",
    "    from langchain.chains import LLMChain, SimpleSequentialChain, SequentialChain\n",
    "    from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "    from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "    from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from dotenv import load_dotenv\n",
    "    import json\n",
    "    \n",
    "    # åŠ è½½ç¯å¢ƒå˜é‡\n",
    "    load_dotenv()\n",
    "    \n",
    "    print(\"âœ… LangChainé“¾æ¨¡å—åŠ è½½æˆåŠŸ\")\n",
    "    \n",
    "    # æ£€æŸ¥APIå¯†é’¥\n",
    "    if os.getenv('OPENAI_API_KEY'):\n",
    "        print(\"âœ… OpenAI APIå¯†é’¥å·²é…ç½®\")\n",
    "        has_api_key = True\n",
    "        # åˆå§‹åŒ–LLMï¼ˆä½¿ç”¨è¾ƒå°çš„æ¨¡å‹ä»¥èŠ‚çœæˆæœ¬ï¼‰\n",
    "        llm = ChatOpenAI(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            temperature=0.7,\n",
    "            max_tokens=500\n",
    "        )\n",
    "    else:\n",
    "        print(\"âš ï¸ æœªæ£€æµ‹åˆ°OpenAI APIå¯†é’¥ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼\")\n",
    "        has_api_key = False\n",
    "        \n",
    "        # åˆ›å»ºæ¨¡æ‹ŸLLMç±»\n",
    "        class MockLLM:\n",
    "            def invoke(self, prompt):\n",
    "                return f\"[æ¨¡æ‹Ÿå“åº”] åŸºäºæç¤ºè¯: {prompt[:50]}...\"\n",
    "            \n",
    "            def __call__(self, prompt):\n",
    "                return self.invoke(prompt)\n",
    "        \n",
    "        llm = MockLLM()\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"âŒ å¯¼å…¥é”™è¯¯: {e}\")\n",
    "    print(\"ğŸ’¡ è¯·è¿è¡Œ: pip install langchain langchain-openai\")\n",
    "    has_api_key = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Chainçš„ç»„æˆéƒ¨åˆ†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ§© Chainçš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ï¼š\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "components = {\n",
    "    \"ğŸ“ æç¤ºè¯æ¨¡æ¿ (PromptTemplate)\": \"å®šä¹‰è¾“å…¥æ ¼å¼å’ŒæŒ‡ä»¤\",\n",
    "    \"ğŸ¤– è¯­è¨€æ¨¡å‹ (LLM)\": \"å¤„ç†æ–‡æœ¬ç”Ÿæˆçš„æ ¸å¿ƒå¼•æ“\",\n",
    "    \"ğŸ”§ è¾“å‡ºè§£æå™¨ (OutputParser)\": \"å¤„ç†å’Œæ ¼å¼åŒ–LLMè¾“å‡º\",\n",
    "    \"ğŸ”— è¿æ¥å™¨ (Runnable)\": \"è¿æ¥å„ä¸ªç»„ä»¶çš„ç®¡é“\",\n",
    "    \"ğŸ“Š è®°å¿†ç³»ç»Ÿ (Memory)\": \"å­˜å‚¨å¯¹è¯å†å²å’Œä¸Šä¸‹æ–‡\",\n",
    "    \"ğŸ› ï¸ å·¥å…·é›† (Tools)\": \"æ‰©å±•é“¾çš„åŠŸèƒ½å’Œèƒ½åŠ›\"\n",
    "}\n",
    "\n",
    "for component, description in components.items():\n",
    "    print(f\"{component}: {description}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ é“¾çš„å·¥ä½œæµç¨‹ï¼š\")\n",
    "workflow = [\n",
    "    \"1. æ¥æ”¶ç”¨æˆ·è¾“å…¥\",\n",
    "    \"2. åº”ç”¨æç¤ºè¯æ¨¡æ¿æ ¼å¼åŒ–\",\n",
    "    \"3. å‘é€ç»™è¯­è¨€æ¨¡å‹å¤„ç†\",\n",
    "    \"4. è§£æå’Œæ ¼å¼åŒ–è¾“å‡º\",\n",
    "    \"5. è¿”å›æœ€ç»ˆç»“æœ\"\n",
    "]\n",
    "\n",
    "for step in workflow:\n",
    "    print(f\"   {step}\")\n",
    "\n",
    "# è®°å½•å®ŒæˆçŠ¶æ€\n",
    "tracker.complete_exercise(lesson_id, \"chain_concepts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ğŸ”§ åŸºç¡€é“¾ç±»å‹\n",
    "\n",
    "### 2.1 ç®€å•LLMé“¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# åˆ›å»ºç®€å•çš„æç¤ºè¯æ¨¡æ¿\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"audience\"],\n",
    "    template=\"\"\"\n",
    "è¯·ä¸º{audience}å†™ä¸€ç¯‡å…³äº{topic}çš„ç®€çŸ­ä»‹ç»æ–‡ç« ã€‚\n",
    "\n",
    "è¦æ±‚ï¼š\n",
    "- å†…å®¹å‡†ç¡®ä¸”æ˜“æ‡‚\n",
    "- é•¿åº¦æ§åˆ¶åœ¨200å­—ä»¥å†…\n",
    "- è¯­è¨€é£æ ¼é€‚åˆç›®æ ‡å—ä¼—\n",
    "\n",
    "æ–‡ç« ï¼š\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# åˆ›å»ºè¾“å‡ºè§£æå™¨\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# æ„å»ºé“¾ï¼ˆä½¿ç”¨LCEL - LangChain Expression Languageï¼‰\n",
    "simple_chain = prompt_template | llm | output_parser\n",
    "\n",
    "print(\"ğŸ”— ç®€å•LLMé“¾ç¤ºä¾‹ï¼š\")\n",
    "print(\"é“¾ç»“æ„: PromptTemplate | LLM | OutputParser\")\n",
    "\n",
    "# æµ‹è¯•é“¾\n",
    "try:\n",
    "    if has_api_key:\n",
    "        result = simple_chain.invoke({\n",
    "            \"topic\": \"æœºå™¨å­¦ä¹ \",\n",
    "            \"audience\": \"é«˜ä¸­å­¦ç”Ÿ\"\n",
    "        })\n",
    "        print(f\"\\nâœ… é“¾æ‰§è¡Œç»“æœï¼š\\n{result}\")\n",
    "    else:\n",
    "        # æ¨¡æ‹Ÿæ‰§è¡Œ\n",
    "        formatted_prompt = prompt_template.format(\n",
    "            topic=\"æœºå™¨å­¦ä¹ \",\n",
    "            audience=\"é«˜ä¸­å­¦ç”Ÿ\"\n",
    "        )\n",
    "        print(f\"\\nğŸ“ æ ¼å¼åŒ–çš„æç¤ºè¯ï¼š\\n{formatted_prompt}\")\n",
    "        print(\"\\nğŸ¤– æ¨¡æ‹Ÿè¾“å‡º: [è¿™é‡Œä¼šæ˜¯LLMç”Ÿæˆçš„å…³äºæœºå™¨å­¦ä¹ çš„é«˜ä¸­ç”Ÿä»‹ç»æ–‡ç« ]\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ é“¾æ‰§è¡Œå¤±è´¥: {e}\")\n",
    "\n",
    "# è®°å½•å®ŒæˆçŠ¶æ€\n",
    "tracker.complete_exercise(lesson_id, \"simple_llm_chain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 é¡ºåºé“¾ (Sequential Chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆæ–‡ç« å¤§çº²\n",
    "outline_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"\"\"\n",
    "è¯·ä¸ºä¸»é¢˜\"{topic}\"åˆ›å»ºä¸€ä¸ªæ–‡ç« å¤§çº²ã€‚\n",
    "\n",
    "è¦æ±‚ï¼š\n",
    "- åŒ…å«3-5ä¸ªä¸»è¦éƒ¨åˆ†\n",
    "- æ¯ä¸ªéƒ¨åˆ†æœ‰ç®€çŸ­è¯´æ˜\n",
    "- é€»è¾‘æ¸…æ™°ï¼Œç»“æ„åˆç†\n",
    "\n",
    "å¤§çº²ï¼š\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# ç¬¬äºŒæ­¥ï¼šåŸºäºå¤§çº²å†™æ–‡ç« \n",
    "article_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"outline\"],\n",
    "    template=\"\"\"\n",
    "ä¸»é¢˜ï¼š{topic}\n",
    "å¤§çº²ï¼š{outline}\n",
    "\n",
    "è¯·æ ¹æ®ä¸Šè¿°å¤§çº²å†™ä¸€ç¯‡å®Œæ•´çš„æ–‡ç« ã€‚\n",
    "\n",
    "è¦æ±‚ï¼š\n",
    "- ä¸¥æ ¼æŒ‰ç…§å¤§çº²ç»“æ„\n",
    "- å†…å®¹å……å®ï¼Œé€»è¾‘æ¸…æ™°\n",
    "- è¯­è¨€æµç•…ï¼Œè¡¨è¾¾å‡†ç¡®\n",
    "\n",
    "æ–‡ç« ï¼š\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆæ–‡ç« æ‘˜è¦\n",
    "summary_prompt = PromptTemplate(\n",
    "    input_variables=[\"article\"],\n",
    "    template=\"\"\"\n",
    "è¯·ä¸ºä»¥ä¸‹æ–‡ç« ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ‘˜è¦ï¼š\n",
    "\n",
    "{article}\n",
    "\n",
    "æ‘˜è¦è¦æ±‚ï¼š\n",
    "- 50å­—ä»¥å†…\n",
    "- çªå‡ºæ ¸å¿ƒè¦ç‚¹\n",
    "- è¯­è¨€ç®€æ´æ˜äº†\n",
    "\n",
    "æ‘˜è¦ï¼š\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# æ„å»ºé¡ºåºé“¾\n",
    "def create_sequential_chain():\n",
    "    \"\"\"åˆ›å»ºé¡ºåºé“¾ï¼šå¤§çº² -> æ–‡ç«  -> æ‘˜è¦\"\"\"\n",
    "    \n",
    "    # æ­¥éª¤1ï¼šç”Ÿæˆå¤§çº²\n",
    "    outline_chain = outline_prompt | llm | StrOutputParser()\n",
    "    \n",
    "    # æ­¥éª¤2ï¼šåŸºäºå¤§çº²å†™æ–‡ç« \n",
    "    def create_article_input(inputs):\n",
    "        \"\"\"ç»„åˆå¤§çº²å’Œä¸»é¢˜ä½œä¸ºæ–‡ç« ç”Ÿæˆçš„è¾“å…¥\"\"\"\n",
    "        return {\n",
    "            \"topic\": inputs[\"topic\"],\n",
    "            \"outline\": inputs[\"outline\"]\n",
    "        }\n",
    "    \n",
    "    article_chain = article_prompt | llm | StrOutputParser()\n",
    "    \n",
    "    # æ­¥éª¤3ï¼šç”Ÿæˆæ‘˜è¦\n",
    "    summary_chain = summary_prompt | llm | StrOutputParser()\n",
    "    \n",
    "    # ç»„åˆå®Œæ•´çš„é¡ºåºé“¾\n",
    "    full_chain = {\n",
    "        \"topic\": RunnablePassthrough(),\n",
    "        \"outline\": outline_chain\n",
    "    } | RunnableLambda(create_article_input) | {\n",
    "        \"article\": article_chain,\n",
    "        \"topic\": RunnablePassthrough()\n",
    "    } | {\n",
    "        \"summary\": summary_chain,\n",
    "        \"article\": lambda x: x[\"article\"],\n",
    "        \"topic\": lambda x: x[\"topic\"]\n",
    "    }\n",
    "    \n",
    "    return full_chain\n",
    "\n",
    "print(\"ğŸ”— é¡ºåºé“¾ç¤ºä¾‹ï¼šå¤§çº² â†’ æ–‡ç«  â†’ æ‘˜è¦\")\n",
    "print(\"é“¾ç»“æ„: [ä¸»é¢˜] â†’ [å¤§çº²ç”Ÿæˆ] â†’ [æ–‡ç« å†™ä½œ] â†’ [æ‘˜è¦ç”Ÿæˆ]\")\n",
    "\n",
    "# æµ‹è¯•é¡ºåºé“¾\n",
    "if has_api_key:\n",
    "    try:\n",
    "        sequential_chain = create_sequential_chain()\n",
    "        result = sequential_chain.invoke({\"topic\": \"äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹\"})\n",
    "        \n",
    "        print(\"\\nâœ… é¡ºåºé“¾æ‰§è¡Œç»“æœï¼š\")\n",
    "        print(f\"ä¸»é¢˜: {result['topic']}\")\n",
    "        print(f\"\\næ–‡ç« æ‘˜è¦: {result['summary']}\")\n",
    "        print(f\"\\nå®Œæ•´æ–‡ç« : {result['article'][:200]}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ é¡ºåºé“¾æ‰§è¡Œå¤±è´¥: {e}\")\nelse:\n",
    "    # æ¨¡æ‹Ÿæ‰§è¡Œæµç¨‹\n",
    "    print(\"\\nğŸ”„ æ¨¡æ‹Ÿé¡ºåºé“¾æ‰§è¡Œæµç¨‹ï¼š\")\n",
    "    steps = [\n",
    "        \"1. è¾“å…¥ä¸»é¢˜: 'äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹'\",\n",
    "        \"2. ç”Ÿæˆå¤§çº²: [AIèµ·æº â†’ å‘å±•é˜¶æ®µ â†’ ç°çŠ¶ â†’ æœªæ¥]\",\n",
    "        \"3. åŸºäºå¤§çº²å†™æ–‡ç« : [è¯¦ç»†çš„AIå‘å±•å†ç¨‹æ–‡ç« ]\",\n",
    "        \"4. ç”Ÿæˆæ‘˜è¦: [50å­—å†…çš„æ ¸å¿ƒè¦ç‚¹æ‘˜è¦]\"\n",
    "    ]\n",
    "    \n",
    "    for step in steps:\n",
    "        print(f\"   {step}\")\n",
    "\n",
    "# è®°å½•å®ŒæˆçŠ¶æ€\n",
    "tracker.complete_exercise(lesson_id, \"sequential_chain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 å¹¶è¡Œé“¾ (Parallel Chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "# å®šä¹‰å¤šä¸ªå¹¶è¡Œå¤„ç†çš„å­é“¾\n",
    "\n",
    "# å­é“¾1ï¼šæŠ€æœ¯åˆ†æ\n",
    "technical_prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"\"\"\n",
    "ä»æŠ€æœ¯è§’åº¦åˆ†æäº§å“\"{product}\"ï¼š\n",
    "\n",
    "è¯·åˆ†æï¼š\n",
    "- æ ¸å¿ƒæŠ€æœ¯ç‰¹ç‚¹\n",
    "- æŠ€æœ¯ä¼˜åŠ¿\n",
    "- æŠ€æœ¯æŒ‘æˆ˜\n",
    "- æŠ€æœ¯å‘å±•è¶‹åŠ¿\n",
    "\n",
    "æŠ€æœ¯åˆ†æï¼š\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# å­é“¾2ï¼šå¸‚åœºåˆ†æ\n",
    "market_prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"\"\"\n",
    "ä»å¸‚åœºè§’åº¦åˆ†æäº§å“\"{product}\"ï¼š\n",
    "\n",
    "è¯·åˆ†æï¼š\n",
    "- ç›®æ ‡å¸‚åœº\n",
    "- ç«äº‰ç¯å¢ƒ\n",
    "- å¸‚åœºæœºä¼š\n",
    "- é£é™©å› ç´ \n",
    "\n",
    "å¸‚åœºåˆ†æï¼š\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# å­é“¾3ï¼šç”¨æˆ·ä½“éªŒåˆ†æ\n",
    "ux_prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"\"\"\n",
    "ä»ç”¨æˆ·ä½“éªŒè§’åº¦åˆ†æäº§å“\"{product}\"ï¼š\n",
    "\n",
    "è¯·åˆ†æï¼š\n",
    "- ç”¨æˆ·éœ€æ±‚åŒ¹é…åº¦\n",
    "- ä½¿ç”¨ä¾¿åˆ©æ€§\n",
    "- ç”¨æˆ·æ»¡æ„åº¦\n",
    "- æ”¹è¿›å»ºè®®\n",
    "\n",
    "ç”¨æˆ·ä½“éªŒåˆ†æï¼š\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# åˆ›å»ºå¹¶è¡Œé“¾\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"technical_analysis\": technical_prompt | llm | StrOutputParser(),\n",
    "    \"market_analysis\": market_prompt | llm | StrOutputParser(),\n",
    "    \"ux_analysis\": ux_prompt | llm | StrOutputParser()\n",
    "})\n",
    "\n",
    "# ç»¼åˆåˆ†æé“¾\n",
    "synthesis_prompt = PromptTemplate(\n",
    "    input_variables=[\"technical\", \"market\", \"ux\", \"product\"],\n",
    "    template=\"\"\"\n",
    "äº§å“ï¼š{product}\n",
    "\n",
    "æŠ€æœ¯åˆ†æï¼š{technical}\n",
    "\n",
    "å¸‚åœºåˆ†æï¼š{market}\n",
    "\n",
    "ç”¨æˆ·ä½“éªŒåˆ†æï¼š{ux}\n",
    "\n",
    "è¯·åŸºäºä»¥ä¸Šä¸‰ä¸ªè§’åº¦çš„åˆ†æï¼Œæä¾›ä¸€ä¸ªç»¼åˆçš„äº§å“è¯„ä¼°å’Œå»ºè®®ï¼š\n",
    "\n",
    "ç»¼åˆè¯„ä¼°ï¼š\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# å®Œæ•´çš„å¹¶è¡Œ-ç»¼åˆé“¾\n",
    "def create_comprehensive_analysis_chain():\n",
    "    \"\"\"åˆ›å»ºç»¼åˆåˆ†æé“¾ï¼šå¹¶è¡Œåˆ†æ + ç»¼åˆè¯„ä¼°\"\"\"\n",
    "    \n",
    "    def prepare_synthesis_input(parallel_results):\n",
    "        \"\"\"å‡†å¤‡ç»¼åˆåˆ†æçš„è¾“å…¥\"\"\"\n",
    "        return {\n",
    "            \"product\": parallel_results[\"product\"],\n",
    "            \"technical\": parallel_results[\"technical_analysis\"],\n",
    "            \"market\": parallel_results[\"market_analysis\"],\n",
    "            \"ux\": parallel_results[\"ux_analysis\"]\n",
    "        }\n",
    "    \n",
    "    # ç»„åˆé“¾ï¼šå¹¶è¡Œåˆ†æ -> ç»¼åˆè¯„ä¼°\n",
    "    full_chain = {\n",
    "        \"product\": RunnablePassthrough(),\n",
    "        **parallel_chain.map()\n",
    "    } | RunnableLambda(prepare_synthesis_input) | synthesis_prompt | llm | StrOutputParser()\n",
    "    \n",
    "    return full_chain\n",
    "\n",
    "print(\"ğŸ”— å¹¶è¡Œé“¾ç¤ºä¾‹ï¼šå¤šè§’åº¦äº§å“åˆ†æ\")\n",
    "print(\"é“¾ç»“æ„: [äº§å“] â†’ [æŠ€æœ¯åˆ†æ || å¸‚åœºåˆ†æ || ç”¨æˆ·ä½“éªŒåˆ†æ] â†’ [ç»¼åˆè¯„ä¼°]\")\n",
    "\n",
    "# æµ‹è¯•å¹¶è¡Œé“¾\n",
    "if has_api_key:\n",
    "    try:\n",
    "        # ä»…æµ‹è¯•å¹¶è¡Œåˆ†æéƒ¨åˆ†ï¼ˆé¿å…å¤æ‚çš„é“¾ç»„åˆï¼‰\n",
    "        test_input = {\"product\": \"ChatGPT\"}\n",
    "        parallel_results = parallel_chain.invoke(test_input)\n",
    "        \n",
    "        print(\"\\nâœ… å¹¶è¡Œé“¾æ‰§è¡Œç»“æœï¼š\")\n",
    "        print(f\"\\nğŸ”§ æŠ€æœ¯åˆ†æï¼š\\n{parallel_results['technical_analysis'][:150]}...\")\n",
    "        print(f\"\\nğŸ“ˆ å¸‚åœºåˆ†æï¼š\\n{parallel_results['market_analysis'][:150]}...\")\n",
    "        print(f\"\\nğŸ‘¥ ç”¨æˆ·ä½“éªŒåˆ†æï¼š\\n{parallel_results['ux_analysis'][:150]}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ å¹¶è¡Œé“¾æ‰§è¡Œå¤±è´¥: {e}\")\nelse:\n",
    "    # æ¨¡æ‹Ÿæ‰§è¡Œæµç¨‹\n",
    "    print(\"\\nğŸ”„ æ¨¡æ‹Ÿå¹¶è¡Œé“¾æ‰§è¡Œæµç¨‹ï¼š\")\n",
    "    print(\"è¾“å…¥: 'ChatGPT'\")\n",
    "    print(\"\\nåŒæ—¶æ‰§è¡Œä¸‰ä¸ªåˆ†æï¼š\")\n",
    "    print(\"   ğŸ”§ æŠ€æœ¯åˆ†æ: [GPTæ¶æ„ã€è®­ç»ƒæ–¹æ³•ã€æŠ€æœ¯åˆ›æ–°...]\")\n",
    "    print(\"   ğŸ“ˆ å¸‚åœºåˆ†æ: [AIå¸‚åœºã€ç«äº‰å¯¹æ‰‹ã€å•†ä¸šæ¨¡å¼...]\")\n",
    "    print(\"   ğŸ‘¥ ç”¨æˆ·ä½“éªŒ: [æ˜“ç”¨æ€§ã€å“åº”é€Ÿåº¦ã€æ»¡æ„åº¦...]\")\n",
    "    print(\"\\næœ€åç»¼åˆ: [åŸºäºä¸‰ä¸ªåˆ†æçš„ç»¼åˆè¯„ä¼°å’Œå»ºè®®]\")\n",
    "\n",
    "# è®°å½•å®ŒæˆçŠ¶æ€\n",
    "tracker.complete_exercise(lesson_id, \"parallel_chain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ğŸ› ï¸ è‡ªå®šä¹‰é“¾çš„åˆ›å»º\n",
    "\n",
    "### 3.1 åˆ›å»ºè‡ªå®šä¹‰æ•°æ®å¤„ç†é“¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from typing import Dict, Any, List\n",
    "import re\n",
    "import json\n",
    "\n",
    "class CustomDataProcessingChain:\n",
    "    \"\"\"è‡ªå®šä¹‰æ•°æ®å¤„ç†é“¾\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.setup_chain()\n",
    "    \n",
    "    def data_cleaner(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"æ•°æ®æ¸…ç†æ­¥éª¤\"\"\"\n",
    "        text = data.get(\"text\", \"\")\n",
    "        \n",
    "        # åŸºç¡€æ¸…ç†\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', text)  # æ ‡å‡†åŒ–ç©ºç™½å­—ç¬¦\n",
    "        cleaned_text = re.sub(r'[^\\w\\s\\u4e00-\\u9fff.,!?]', '', cleaned_text)  # ä¿ç•™åŸºæœ¬å­—ç¬¦\n",
    "        cleaned_text = cleaned_text.strip()\n",
    "        \n",
    "        return {\n",
    "            \"original_text\": text,\n",
    "            \"cleaned_text\": cleaned_text,\n",
    "            \"char_count\": len(cleaned_text),\n",
    "            \"word_count\": len(cleaned_text.split())\n",
    "        }\n",
    "    \n",
    "    def sentiment_analyzer(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"æƒ…æ„Ÿåˆ†ææ­¥éª¤\"\"\"\n",
    "        text = data[\"cleaned_text\"]\n",
    "        \n",
    "        sentiment_prompt = PromptTemplate(\n",
    "            input_variables=[\"text\"],\n",
    "            template=\"\"\"\n",
    "è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘ï¼š\n",
    "\n",
    "æ–‡æœ¬ï¼š{text}\n",
    "\n",
    "è¯·è¾“å‡ºJSONæ ¼å¼çš„åˆ†æç»“æœï¼š\n",
    "{{\n",
    "    \"sentiment\": \"positive/negative/neutral\",\n",
    "    \"confidence\": 0.0-1.0,\n",
    "    \"key_emotions\": [\"emotion1\", \"emotion2\"],\n",
    "    \"reasoning\": \"åˆ†æç†ç”±\"\n",
    "}}\n",
    "\n",
    "åˆ†æç»“æœï¼š\n",
    "\"\"\"\n",
    "        )\n",
    "        \n",
    "        if hasattr(self.llm, 'invoke'):\n",
    "            # çœŸå®LLMè°ƒç”¨\n",
    "            try:\n",
    "                chain = sentiment_prompt | self.llm | StrOutputParser()\n",
    "                result = chain.invoke({\"text\": text})\n",
    "                \n",
    "                # å°è¯•è§£æJSON\n",
    "                try:\n",
    "                    sentiment_data = json.loads(result)\n",
    "                except json.JSONDecodeError:\n",
    "                    # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼\n",
    "                    sentiment_data = {\n",
    "                        \"sentiment\": \"neutral\",\n",
    "                        \"confidence\": 0.5,\n",
    "                        \"key_emotions\": [\"neutral\"],\n",
    "                        \"reasoning\": \"è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼\"\n",
    "                    }\n",
    "            except:\n",
    "                sentiment_data = {\n",
    "                    \"sentiment\": \"neutral\",\n",
    "                    \"confidence\": 0.5,\n",
    "                    \"key_emotions\": [\"neutral\"],\n",
    "                    \"reasoning\": \"LLMè°ƒç”¨å¤±è´¥\"\n",
    "                }\n",
    "        else:\n",
    "            # æ¨¡æ‹Ÿæƒ…æ„Ÿåˆ†æ\n",
    "            positive_words = ['å¥½', 'æ£’', 'ä¼˜ç§€', 'å–œæ¬¢', 'æ»¡æ„', 'good', 'great', 'excellent']\n",
    "            negative_words = ['å', 'å·®', 'ç³Ÿç³•', 'è®¨åŒ', 'å¤±æœ›', 'bad', 'terrible', 'awful']\n",
    "            \n",
    "            positive_count = sum(1 for word in positive_words if word in text.lower())\n",
    "            negative_count = sum(1 for word in negative_words if word in text.lower())\n",
    "            \n",
    "            if positive_count > negative_count:\n",
    "                sentiment = \"positive\"\n",
    "                confidence = min(0.9, 0.6 + positive_count * 0.1)\n",
    "            elif negative_count > positive_count:\n",
    "                sentiment = \"negative\"\n",
    "                confidence = min(0.9, 0.6 + negative_count * 0.1)\n",
    "            else:\n",
    "                sentiment = \"neutral\"\n",
    "                confidence = 0.7\n",
    "            \n",
    "            sentiment_data = {\n",
    "                \"sentiment\": sentiment,\n",
    "                \"confidence\": confidence,\n",
    "                \"key_emotions\": [sentiment],\n",
    "                \"reasoning\": f\"åŸºäºå…³é”®è¯åˆ†æï¼šç§¯æè¯{positive_count}ä¸ªï¼Œæ¶ˆæè¯{negative_count}ä¸ª\"\n",
    "            }\n",
    "        \n",
    "        # åˆå¹¶æ•°æ®\n",
    "        data.update(sentiment_data)\n",
    "        return data\n",
    "    \n",
    "    def summary_generator(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"æ‘˜è¦ç”Ÿæˆæ­¥éª¤\"\"\"\n",
    "        text = data[\"cleaned_text\"]\n",
    "        sentiment = data[\"sentiment\"]\n",
    "        \n",
    "        summary_prompt = PromptTemplate(\n",
    "            input_variables=[\"text\", \"sentiment\"],\n",
    "            template=\"\"\"\n",
    "è¯·ä¸ºä»¥ä¸‹æ–‡æœ¬ç”Ÿæˆæ‘˜è¦ï¼š\n",
    "\n",
    "æ–‡æœ¬ï¼š{text}\n",
    "æƒ…æ„Ÿå€¾å‘ï¼š{sentiment}\n",
    "\n",
    "è¯·ç”Ÿæˆä¸€ä¸ª30å­—ä»¥å†…çš„æ‘˜è¦ï¼Œçªå‡ºæ ¸å¿ƒå†…å®¹å’Œæƒ…æ„Ÿç‰¹å¾ï¼š\n",
    "\n",
    "æ‘˜è¦ï¼š\n",
    "\"\"\"\n",
    "        )\n",
    "        \n",
    "        if hasattr(self.llm, 'invoke'):\n",
    "            try:\n",
    "                chain = summary_prompt | self.llm | StrOutputParser()\n",
    "                summary = chain.invoke({\"text\": text, \"sentiment\": sentiment})\n",
    "            except:\n",
    "                summary = f\"[{sentiment}æƒ…æ„Ÿ] {text[:20]}...\"\n",
    "        else:\n",
    "            # ç®€å•æ‘˜è¦ç”Ÿæˆ\n",
    "            words = text.split()[:10]  # å–å‰10ä¸ªè¯\n",
    "            summary = f\"[{sentiment}æƒ…æ„Ÿ] {' '.join(words)}...\"\n",
    "        \n",
    "        data[\"summary\"] = summary\n",
    "        return data\n",
    "    \n",
    "    def setup_chain(self):\n",
    "        \"\"\"è®¾ç½®å¤„ç†é“¾\"\"\"\n",
    "        self.chain = (\n",
    "            RunnableLambda(self.data_cleaner) |\n",
    "            RunnableLambda(self.sentiment_analyzer) |\n",
    "            RunnableLambda(self.summary_generator)\n",
    "        )\n",
    "    \n",
    "    def process(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"å¤„ç†æ–‡æœ¬\"\"\"\n",
    "        return self.chain.invoke({\"text\": text})\n",
    "\n",
    "# åˆ›å»ºè‡ªå®šä¹‰å¤„ç†é“¾å®ä¾‹\n",
    "custom_chain = CustomDataProcessingChain(llm)\n",
    "\n",
    "print(\"ğŸ› ï¸ è‡ªå®šä¹‰æ•°æ®å¤„ç†é“¾\")\n",
    "print(\"å¤„ç†æµç¨‹: æ•°æ®æ¸…ç† â†’ æƒ…æ„Ÿåˆ†æ â†’ æ‘˜è¦ç”Ÿæˆ\")\n",
    "\n",
    "# æµ‹è¯•è‡ªå®šä¹‰é“¾\n",
    "test_texts = [\n",
    "    \"è¿™ä¸ªäº§å“çœŸçš„å¤ªæ£’äº†ï¼æˆ‘éå¸¸æ»¡æ„ï¼Œæ¨èç»™æ‰€æœ‰äººã€‚ç•Œé¢è®¾è®¡å¾ˆå¥½ï¼ŒåŠŸèƒ½ä¹Ÿå¾ˆå¼ºå¤§ã€‚\",\n",
    "    \"ä½¿ç”¨ä½“éªŒå¾ˆå·®ï¼Œbugå¤ªå¤šäº†ï¼Œå®Œå…¨ä¸æ¨èã€‚å®¢æœæ€åº¦ä¹Ÿä¸å¥½ï¼Œå¾ˆå¤±æœ›ã€‚\",\n",
    "    \"äº§å“è¿˜å¯ä»¥ï¼Œæœ‰ä¼˜ç‚¹ä¹Ÿæœ‰ç¼ºç‚¹ã€‚ä»·æ ¼åˆç†ï¼Œä½†åŠŸèƒ½éœ€è¦æ”¹è¿›ã€‚\"\n",
    "]\n",
    "\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    print(f\"\\nğŸ“ æµ‹è¯•æ–‡æœ¬{i}ï¼š{text[:30]}...\")\n",
    "    \n",
    "    try:\n",
    "        result = custom_chain.process(text)\n",
    "        \n",
    "        print(f\"   æƒ…æ„Ÿï¼š{result['sentiment']} (ç½®ä¿¡åº¦: {result['confidence']:.2f})\")\n",
    "        print(f\"   æ‘˜è¦ï¼š{result['summary']}\")\n",
    "        print(f\"   å­—æ•°ï¼š{result['char_count']} | è¯æ•°ï¼š{result['word_count']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ å¤„ç†å¤±è´¥: {e}\")\n",
    "\n",
    "# è®°å½•å®ŒæˆçŠ¶æ€\n",
    "tracker.complete_exercise(lesson_id, \"custom_chain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 é“¾çš„é”™è¯¯å¤„ç†å’Œè°ƒè¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "import logging\n",
    "import time\n",
    "from typing import Union\n",
    "\n",
    "class RobustChain:\n",
    "    \"\"\"å…·æœ‰é”™è¯¯å¤„ç†å’Œè°ƒè¯•åŠŸèƒ½çš„é“¾\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, debug=True):\n",
    "        self.llm = llm\n",
    "        self.debug = debug\n",
    "        self.execution_log = []\n",
    "        \n",
    "        # é…ç½®æ—¥å¿—\n",
    "        if debug:\n",
    "            logging.basicConfig(level=logging.INFO)\n",
    "            self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def log_step(self, step_name: str, input_data: Any, output_data: Any = None, error: Exception = None):\n",
    "        \"\"\"è®°å½•æ‰§è¡Œæ­¥éª¤\"\"\"\n",
    "        log_entry = {\n",
    "            \"timestamp\": time.time(),\n",
    "            \"step\": step_name,\n",
    "            \"input\": str(input_data)[:100] + \"...\" if len(str(input_data)) > 100 else str(input_data),\n",
    "            \"output\": str(output_data)[:100] + \"...\" if output_data and len(str(output_data)) > 100 else str(output_data),\n",
    "            \"error\": str(error) if error else None,\n",
    "            \"status\": \"error\" if error else \"success\"\n",
    "        }\n",
    "        \n",
    "        self.execution_log.append(log_entry)\n",
    "        \n",
    "        if self.debug:\n",
    "            if error:\n",
    "                self.logger.error(f\"âŒ {step_name}: {error}\")\n",
    "            else:\n",
    "                self.logger.info(f\"âœ… {step_name}: æˆåŠŸ\")\n",
    "    \n",
    "    def safe_step(self, step_name: str, func, input_data, fallback_value=None):\n",
    "        \"\"\"å®‰å…¨æ‰§è¡Œæ­¥éª¤ï¼ŒåŒ…å«é”™è¯¯å¤„ç†\"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            result = func(input_data)\n",
    "            execution_time = time.time() - start_time\n",
    "            \n",
    "            self.log_step(step_name, input_data, result)\n",
    "            \n",
    "            if self.debug:\n",
    "                print(f\"â±ï¸ {step_name} æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’\")\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log_step(step_name, input_data, error=e)\n",
    "            \n",
    "            if fallback_value is not None:\n",
    "                print(f\"âš ï¸ {step_name} å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨å€¼: {fallback_value}\")\n",
    "                return fallback_value\n",
    "            else:\n",
    "                print(f\"âŒ {step_name} å¤±è´¥: {e}\")\n",
    "                raise e\n",
    "    \n",
    "    def input_validator(self, data):\n",
    "        \"\"\"è¾“å…¥éªŒè¯æ­¥éª¤\"\"\"\n",
    "        if not isinstance(data, dict):\n",
    "            raise ValueError(\"è¾“å…¥å¿…é¡»æ˜¯å­—å…¸æ ¼å¼\")\n",
    "        \n",
    "        if \"text\" not in data:\n",
    "            raise ValueError(\"è¾“å…¥å¿…é¡»åŒ…å«'text'å­—æ®µ\")\n",
    "        \n",
    "        if not data[\"text\"].strip():\n",
    "            raise ValueError(\"æ–‡æœ¬å†…å®¹ä¸èƒ½ä¸ºç©º\")\n",
    "        \n",
    "        if len(data[\"text\"]) > 1000:\n",
    "            print(\"âš ï¸ æ–‡æœ¬é•¿åº¦è¶…è¿‡1000å­—ç¬¦ï¼Œå°†è¢«æˆªæ–­\")\n",
    "            data[\"text\"] = data[\"text\"][:1000]\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def text_processor(self, data):\n",
    "        \"\"\"æ–‡æœ¬å¤„ç†æ­¥éª¤\"\"\"\n",
    "        text = data[\"text\"]\n",
    "        \n",
    "        # åŸºç¡€å¤„ç†\n",
    "        processed_text = text.strip().replace(\"\\n\", \" \")\n",
    "        \n",
    "        # æ¨¡æ‹Ÿå¯èƒ½çš„å¤„ç†é”™è¯¯\n",
    "        if \"ERROR\" in text.upper():\n",
    "            raise RuntimeError(\"æ£€æµ‹åˆ°é”™è¯¯æ ‡è®°ï¼Œå¤„ç†ä¸­æ–­\")\n",
    "        \n",
    "        return {\n",
    "            **data,\n",
    "            \"processed_text\": processed_text,\n",
    "            \"processing_timestamp\": time.time()\n",
    "        }\n",
    "    \n",
    "    def llm_caller(self, data):\n",
    "        \"\"\"LLMè°ƒç”¨æ­¥éª¤\"\"\"\n",
    "        text = data[\"processed_text\"]\n",
    "        \n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"text\"],\n",
    "            template=\"è¯·å¯¹ä»¥ä¸‹æ–‡æœ¬è¿›è¡Œåˆ†æå’Œæ€»ç»“ï¼š\\n\\n{text}\\n\\nåˆ†æç»“æœï¼š\"\n",
    "        )\n",
    "        \n",
    "        if hasattr(self.llm, 'invoke'):\n",
    "            chain = prompt | self.llm | StrOutputParser()\n",
    "            result = chain.invoke({\"text\": text})\n",
    "        else:\n",
    "            # æ¨¡æ‹ŸLLMè¾“å‡º\n",
    "            result = f\"[æ¨¡æ‹Ÿåˆ†æ] æ–‡æœ¬é•¿åº¦: {len(text)}å­—ç¬¦ï¼Œå†…å®¹æ‘˜è¦: {text[:50]}...\"\n",
    "        \n",
    "        return {\n",
    "            **data,\n",
    "            \"llm_result\": result,\n",
    "            \"llm_timestamp\": time.time()\n",
    "        }\n",
    "    \n",
    "    def output_formatter(self, data):\n",
    "        \"\"\"è¾“å‡ºæ ¼å¼åŒ–æ­¥éª¤\"\"\"\n",
    "        return {\n",
    "            \"original_text\": data[\"text\"],\n",
    "            \"processed_text\": data[\"processed_text\"],\n",
    "            \"analysis_result\": data[\"llm_result\"],\n",
    "            \"processing_info\": {\n",
    "                \"text_length\": len(data[\"text\"]),\n",
    "                \"processing_time\": data.get(\"llm_timestamp\", 0) - data.get(\"processing_timestamp\", 0)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def process(self, input_data):\n",
    "        \"\"\"æ‰§è¡Œå®Œæ•´çš„å¤„ç†æµç¨‹\"\"\"\n",
    "        print(\"ğŸ”„ å¼€å§‹æ‰§è¡Œé“¾å¤„ç†æµç¨‹...\")\n",
    "        \n",
    "        # æ¸…ç©ºæ‰§è¡Œæ—¥å¿—\n",
    "        self.execution_log = []\n",
    "        \n",
    "        try:\n",
    "            # æ­¥éª¤1ï¼šè¾“å…¥éªŒè¯\n",
    "            data = self.safe_step(\n",
    "                \"è¾“å…¥éªŒè¯\", \n",
    "                self.input_validator, \n",
    "                input_data\n",
    "            )\n",
    "            \n",
    "            # æ­¥éª¤2ï¼šæ–‡æœ¬å¤„ç†\n",
    "            data = self.safe_step(\n",
    "                \"æ–‡æœ¬å¤„ç†\", \n",
    "                self.text_processor, \n",
    "                data,\n",
    "                fallback_value={**data, \"processed_text\": data[\"text\"], \"processing_timestamp\": time.time()}\n",
    "            )\n",
    "            \n",
    "            # æ­¥éª¤3ï¼šLLMè°ƒç”¨\n",
    "            data = self.safe_step(\n",
    "                \"LLMåˆ†æ\", \n",
    "                self.llm_caller, \n",
    "                data,\n",
    "                fallback_value={**data, \"llm_result\": \"[åˆ†æå¤±è´¥] ä½¿ç”¨å¤‡ç”¨ç»“æœ\", \"llm_timestamp\": time.time()}\n",
    "            )\n",
    "            \n",
    "            # æ­¥éª¤4ï¼šè¾“å‡ºæ ¼å¼åŒ–\n",
    "            result = self.safe_step(\n",
    "                \"è¾“å‡ºæ ¼å¼åŒ–\", \n",
    "                self.output_formatter, \n",
    "                data\n",
    "            )\n",
    "            \n",
    "            print(\"âœ… é“¾å¤„ç†å®Œæˆ\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ é“¾å¤„ç†å¤±è´¥: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_execution_log(self):\n",
    "        \"\"\"è·å–æ‰§è¡Œæ—¥å¿—\"\"\"\n",
    "        return self.execution_log\n",
    "    \n",
    "    def print_execution_summary(self):\n",
    "        \"\"\"æ‰“å°æ‰§è¡Œæ‘˜è¦\"\"\"\n",
    "        print(\"\\nğŸ“Š æ‰§è¡Œæ‘˜è¦ï¼š\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        for i, log in enumerate(self.execution_log, 1):\n",
    "            status_icon = \"âœ…\" if log[\"status\"] == \"success\" else \"âŒ\"\n",
    "            print(f\"{i}. {status_icon} {log['step']}\")\n",
    "            \n",
    "            if log[\"error\"]:\n",
    "                print(f\"   é”™è¯¯: {log['error']}\")\n",
    "\n",
    "# åˆ›å»ºå¼ºå¥é“¾å®ä¾‹\n",
    "robust_chain = RobustChain(llm, debug=True)\n",
    "\n",
    "print(\"ğŸ›¡ï¸ å¼ºå¥é“¾æµ‹è¯•ï¼šé”™è¯¯å¤„ç†å’Œè°ƒè¯•\")\n",
    "\n",
    "# æµ‹è¯•ç”¨ä¾‹\n",
    "test_cases = [\n",
    "    {\"text\": \"è¿™æ˜¯ä¸€ä¸ªæ­£å¸¸çš„æµ‹è¯•æ–‡æœ¬ï¼Œåº”è¯¥å¯ä»¥æˆåŠŸå¤„ç†ã€‚\"},\n",
    "    {\"text\": \"\"},  # ç©ºæ–‡æœ¬æµ‹è¯•\n",
    "    {\"text\": \"è¿™ä¸ªæ–‡æœ¬åŒ…å«ERRORæ ‡è®°ï¼Œä¼šè§¦å‘å¤„ç†é”™è¯¯ã€‚\"},  # é”™è¯¯è§¦å‘æµ‹è¯•\n",
    "    {\"invalid_key\": \"æ— æ•ˆçš„è¾“å…¥æ ¼å¼\"},  # æ— æ•ˆè¾“å…¥æµ‹è¯•\n",
    "]\n",
    "\n",
    "for i, test_case in enumerate(test_cases, 1):\n",
    "    print(f\"\\nğŸ§ª æµ‹è¯•ç”¨ä¾‹{i}ï¼š\")\n",
    "    result = robust_chain.process(test_case)\n",
    "    \n",
    "    if result:\n",
    "        print(f\"ğŸ“¤ è¾“å‡º: {result['analysis_result'][:100]}...\")\n",
    "    \n",
    "    # æ‰“å°æ‰§è¡Œæ‘˜è¦\n",
    "    robust_chain.print_execution_summary()\n",
    "\n",
    "# è®°å½•å®ŒæˆçŠ¶æ€\n",
    "tracker.complete_exercise(lesson_id, \"error_handling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ğŸ¯ å®æˆ˜ç»ƒä¹ \n",
    "\n",
    "### ç»ƒä¹ 1ï¼šæ„å»ºæ™ºèƒ½å®¢æœé“¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ¯ ç»ƒä¹ 1ï¼šæ™ºèƒ½å®¢æœé“¾\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# ä»»åŠ¡ï¼šåˆ›å»ºä¸€ä¸ªæ™ºèƒ½å®¢æœå¤„ç†é“¾ï¼ŒåŒ…å«ï¼š\n",
    "# 1. æ„å›¾è¯†åˆ«\n",
    "# 2. æƒ…ç»ªåˆ†æ\n",
    "# 3. çŸ¥è¯†åº“æŸ¥è¯¢\n",
    "# 4. å“åº”ç”Ÿæˆ\n",
    "\n",
    "class CustomerServiceChain:\n",
    "    \"\"\"æ™ºèƒ½å®¢æœå¤„ç†é“¾\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        # ç®€å•çš„çŸ¥è¯†åº“\n",
    "        self.knowledge_base = {\n",
    "            \"è®¢å•æŸ¥è¯¢\": \"æ‚¨å¯ä»¥åœ¨'æˆ‘çš„è®¢å•'é¡µé¢æŸ¥çœ‹è®¢å•çŠ¶æ€ï¼Œæˆ–æä¾›è®¢å•å·è®©æˆ‘å¸®æ‚¨æŸ¥è¯¢ã€‚\",\n",
    "            \"é€€æ¢è´§\": \"å•†å“åœ¨7å¤©å†…å¯ä»¥ç”³è¯·é€€æ¢è´§ï¼Œè¯·ç¡®ä¿å•†å“æœªæ‹†å°ã€‚è¯¦ç»†æµç¨‹è¯·æŸ¥çœ‹é€€æ¢è´§æ”¿ç­–ã€‚\",\n",
    "            \"æ”¯ä»˜é—®é¢˜\": \"æˆ‘ä»¬æ”¯æŒæ”¯ä»˜å®ã€å¾®ä¿¡æ”¯ä»˜ã€é“¶è¡Œå¡ç­‰å¤šç§æ”¯ä»˜æ–¹å¼ã€‚å¦‚é‡æ”¯ä»˜é—®é¢˜ï¼Œè¯·è”ç³»æ”¯ä»˜å¹³å°å®¢æœã€‚\",\n",
    "            \"é…é€æŸ¥è¯¢\": \"è®¢å•å‘è´§åï¼Œæ‚¨ä¼šæ”¶åˆ°ç‰©æµä¿¡æ¯ã€‚ä¸€èˆ¬3-5ä¸ªå·¥ä½œæ—¥é€è¾¾ï¼Œåè¿œåœ°åŒºå¯èƒ½éœ€è¦7-10å¤©ã€‚\",\n",
    "            \"äº§å“å’¨è¯¢\": \"è¯·å‘Šè¯‰æˆ‘æ‚¨æƒ³äº†è§£çš„å…·ä½“äº§å“ä¿¡æ¯ï¼Œæˆ‘ä¼šä¸ºæ‚¨è¯¦ç»†ä»‹ç»äº§å“ç‰¹æ€§å’Œä¼˜åŠ¿ã€‚\",\n",
    "            \"æŠ•è¯‰å»ºè®®\": \"éå¸¸æ„Ÿè°¢æ‚¨çš„åé¦ˆï¼Œæˆ‘ä»¬ä¼šè®¤çœŸå¯¹å¾…æ¯ä¸€ä¸ªå»ºè®®ï¼Œå¹¶æŒç»­æ”¹è¿›æˆ‘ä»¬çš„æœåŠ¡ã€‚\"\n",
    "        }\n",
    "    \n",
    "    def intent_recognition(self, user_input: str) -> str:\n",
    "        \"\"\"æ„å›¾è¯†åˆ«\"\"\"\n",
    "        intent_prompt = PromptTemplate(\n",
    "            input_variables=[\"user_input\", \"intents\"],\n",
    "            template=\"\"\"\n",
    "è¯·è¯†åˆ«ç”¨æˆ·è¾“å…¥çš„æ„å›¾ç±»åˆ«ï¼š\n",
    "\n",
    "ç”¨æˆ·è¾“å…¥ï¼š{user_input}\n",
    "\n",
    "å¯èƒ½çš„æ„å›¾ç±»åˆ«ï¼š{intents}\n",
    "\n",
    "è¯·ä»ä¸Šè¿°ç±»åˆ«ä¸­é€‰æ‹©æœ€åŒ¹é…çš„ä¸€ä¸ªï¼Œåªè¾“å‡ºç±»åˆ«åç§°ï¼š\n",
    "\"\"\"\n",
    "        )\n",
    "        \n",
    "        intents = list(self.knowledge_base.keys())\n",
    "        \n",
    "        if hasattr(self.llm, 'invoke'):\n",
    "            try:\n",
    "                chain = intent_prompt | self.llm | StrOutputParser()\n",
    "                result = chain.invoke({\n",
    "                    \"user_input\": user_input,\n",
    "                    \"intents\": \", \".join(intents)\n",
    "                })\n",
    "                \n",
    "                # ç®€å•åŒ¹é…éªŒè¯\n",
    "                for intent in intents:\n",
    "                    if intent in result:\n",
    "                        return intent\n",
    "                \n",
    "                return \"äº§å“å’¨è¯¢\"  # é»˜è®¤æ„å›¾\n",
    "                \n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # ç®€å•å…³é”®è¯åŒ¹é…ä½œä¸ºå¤‡ç”¨\n",
    "        user_lower = user_input.lower()\n",
    "        \n",
    "        if any(word in user_lower for word in ['è®¢å•', 'æŸ¥è¯¢', 'å•å·']):\n",
    "            return \"è®¢å•æŸ¥è¯¢\"\n",
    "        elif any(word in user_lower for word in ['é€€è´§', 'æ¢è´§', 'é€€æ¬¾']):\n",
    "            return \"é€€æ¢è´§\"\n",
    "        elif any(word in user_lower for word in ['æ”¯ä»˜', 'ä»˜æ¬¾', 'æ‰£è´¹']):\n",
    "            return \"æ”¯ä»˜é—®é¢˜\"\n",
    "        elif any(word in user_lower for word in ['ç‰©æµ', 'é…é€', 'å¿«é€’']):\n",
    "            return \"é…é€æŸ¥è¯¢\"\n",
    "        elif any(word in user_lower for word in ['æŠ•è¯‰', 'å»ºè®®', 'æ„è§']):\n",
    "            return \"æŠ•è¯‰å»ºè®®\"\n",
    "        else:\n",
    "            return \"äº§å“å’¨è¯¢\"\n",
    "    \n",
    "    def emotion_analysis(self, user_input: str) -> dict:\n",
    "        \"\"\"æƒ…ç»ªåˆ†æ\"\"\"\n",
    "        # ç®€å•çš„æƒ…ç»ªè¯å…¸\n",
    "        positive_words = ['æ»¡æ„', 'å–œæ¬¢', 'å¥½', 'æ£’', 'èµ', 'è°¢è°¢']\n",
    "        negative_words = ['ä¸æ»¡', 'å¤±æœ›', 'å·®', 'çƒ‚', 'ç³Ÿç³•', 'æ„¤æ€’', 'æŠ•è¯‰']\n",
    "        urgent_words = ['æ€¥', 'ç´§æ€¥', 'ç«‹å³', 'é©¬ä¸Š', 'å°½å¿«']\n",
    "        \n",
    "        user_lower = user_input.lower()\n",
    "        \n",
    "        emotion_score = 0\n",
    "        urgency = False\n",
    "        \n",
    "        for word in positive_words:\n",
    "            if word in user_lower:\n",
    "                emotion_score += 1\n",
    "        \n",
    "        for word in negative_words:\n",
    "            if word in user_lower:\n",
    "                emotion_score -= 1\n",
    "        \n",
    "        for word in urgent_words:\n",
    "            if word in user_lower:\n",
    "                urgency = True\n",
    "        \n",
    "        if emotion_score > 0:\n",
    "            emotion = \"positive\"\n",
    "        elif emotion_score < 0:\n",
    "            emotion = \"negative\"\n",
    "        else:\n",
    "            emotion = \"neutral\"\n",
    "        \n",
    "        return {\n",
    "            \"emotion\": emotion,\n",
    "            \"urgency\": urgency,\n",
    "            \"score\": emotion_score\n",
    "        }\n",
    "    \n",
    "    def generate_response(self, user_input: str, intent: str, emotion_info: dict) -> str:\n",
    "        \"\"\"ç”Ÿæˆå“åº”\"\"\"\n",
    "        # ä»çŸ¥è¯†åº“è·å–åŸºç¡€å›ç­”\n",
    "        base_answer = self.knowledge_base.get(intent, \"æ„Ÿè°¢æ‚¨çš„å’¨è¯¢ï¼Œæˆ‘ä¼šä¸ºæ‚¨è½¬æ¥ä¸“ä¸šå®¢æœã€‚\")\n",
    "        \n",
    "        # æ ¹æ®æƒ…ç»ªè°ƒæ•´è¯­è°ƒ\n",
    "        if emotion_info[\"emotion\"] == \"negative\":\n",
    "            prefix = \"æˆ‘ç†è§£æ‚¨çš„å›°æ‰°ï¼Œéå¸¸æŠ±æ­‰ç»™æ‚¨å¸¦æ¥ä¸ä¾¿ã€‚\"\n",
    "        elif emotion_info[\"emotion\"] == \"positive\":\n",
    "            prefix = \"æ„Ÿè°¢æ‚¨çš„ä¿¡ä»»å’Œæ”¯æŒï¼\"\n",
    "        else:\n",
    "            prefix = \"æ„Ÿè°¢æ‚¨çš„å’¨è¯¢ï¼Œ\"\n",
    "        \n",
    "        # ç´§æ€¥æƒ…å†µå¤„ç†\n",
    "        if emotion_info[\"urgency\"]:\n",
    "            urgency_note = \"\\n\\nâš¡ æˆ‘æ³¨æ„åˆ°æ‚¨çš„éœ€æ±‚æ¯”è¾ƒç´§æ€¥ï¼Œæˆ‘ä¼šä¼˜å…ˆä¸ºæ‚¨å¤„ç†ã€‚\"\n",
    "        else:\n",
    "            urgency_note = \"\"\n",
    "        \n",
    "        return f\"{prefix}{base_answer}{urgency_note}\"\n",
    "    \n",
    "    def process_customer_query(self, user_input: str) -> dict:\n",
    "        \"\"\"å¤„ç†å®¢æˆ·æŸ¥è¯¢\"\"\"\n",
    "        print(f\"ğŸ¤– å¤„ç†å®¢æˆ·æŸ¥è¯¢: {user_input}\")\n",
    "        \n",
    "        # æ­¥éª¤1ï¼šæ„å›¾è¯†åˆ«\n",
    "        intent = self.intent_recognition(user_input)\n",
    "        print(f\"   ğŸ¯ è¯†åˆ«æ„å›¾: {intent}\")\n",
    "        \n",
    "        # æ­¥éª¤2ï¼šæƒ…ç»ªåˆ†æ\n",
    "        emotion_info = self.emotion_analysis(user_input)\n",
    "        print(f\"   ğŸ˜Š æƒ…ç»ªåˆ†æ: {emotion_info['emotion']} (ç´§æ€¥: {emotion_info['urgency']})\")\n",
    "        \n",
    "        # æ­¥éª¤3ï¼šç”Ÿæˆå“åº”\n",
    "        response = self.generate_response(user_input, intent, emotion_info)\n",
    "        print(f\"   ğŸ’¬ ç”Ÿæˆå“åº”: {response[:50]}...\")\n",
    "        \n",
    "        return {\n",
    "            \"user_input\": user_input,\n",
    "            \"intent\": intent,\n",
    "            \"emotion\": emotion_info,\n",
    "            \"response\": response\n",
    "        }\n",
    "\n",
    "# åˆ›å»ºå®¢æœé“¾\n",
    "customer_service = CustomerServiceChain(llm)\n",
    "\n",
    "# æµ‹è¯•å®¢æœåœºæ™¯\n",
    "test_queries = [\n",
    "    \"æˆ‘çš„è®¢å•ä»€ä¹ˆæ—¶å€™èƒ½å‘è´§ï¼Ÿæˆ‘æ¯”è¾ƒæ€¥ç”¨\",\n",
    "    \"è¿™ä¸ªäº§å“è´¨é‡å¤ªå·®äº†ï¼Œæˆ‘è¦é€€è´§ï¼\",\n",
    "    \"ä½ ä»¬çš„æœåŠ¡å¾ˆä¸é”™ï¼Œæƒ³å’¨è¯¢ä¸€ä¸‹æ–°äº§å“çš„åŠŸèƒ½\",\n",
    "    \"æ”¯ä»˜çš„æ—¶å€™å‡ºç°é”™è¯¯ï¼Œæ‰£äº†é’±ä½†æ˜¯è®¢å•æ²¡æˆåŠŸ\",\n",
    "    \"ç‰©æµä¿¡æ¯ä¸‰å¤©æ²¡æ›´æ–°äº†ï¼ŒåŒ…è£¹åˆ°å“ªé‡Œäº†ï¼Ÿ\"\n",
    "]\n",
    "\n",
    "print(\"\\nğŸ§ª å®¢æœé“¾æµ‹è¯•ï¼š\")\n",
    "for query in test_queries:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    result = customer_service.process_customer_query(query)\n",
    "    print(f\"\\nğŸ“ æœ€ç»ˆå“åº”ï¼š\\n{result['response']}\")\n",
    "\n",
    "print(\"\\nâœ… ç»ƒä¹ 1å®Œæˆï¼ä½ å·²ç»å­¦ä¼šäº†å¦‚ä½•æ„å»ºæ™ºèƒ½å®¢æœå¤„ç†é“¾ã€‚\")\n",
    "\n",
    "# è®°å½•ç»ƒä¹ å®Œæˆ\n",
    "tracker.complete_exercise(lesson_id, \"exercise_customer_service\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç»ƒä¹ 2ï¼šå¤šæ­¥éª¤æ•°æ®åˆ†æé“¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ¯ ç»ƒä¹ 2ï¼šå¤šæ­¥éª¤æ•°æ®åˆ†æé“¾\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# ä»»åŠ¡ï¼šåˆ›å»ºä¸€ä¸ªæ•°æ®åˆ†æé“¾ï¼Œèƒ½å¤Ÿï¼š\n",
    "# 1. æ•°æ®éªŒè¯å’Œæ¸…ç†\n",
    "# 2. ç»Ÿè®¡åˆ†æ\n",
    "# 3. è¶‹åŠ¿åˆ†æ\n",
    "# 4. ç”ŸæˆæŠ¥å‘Š\n",
    "\n",
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "class DataAnalysisChain:\n",
    "    \"\"\"æ•°æ®åˆ†æé“¾\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "    \n",
    "    def validate_and_clean_data(self, raw_data: List[Dict]) -> Dict:\n",
    "        \"\"\"æ•°æ®éªŒè¯å’Œæ¸…ç†\"\"\"\n",
    "        print(\"ğŸ” æ­¥éª¤1ï¼šæ•°æ®éªŒè¯å’Œæ¸…ç†\")\n",
    "        \n",
    "        valid_records = []\n",
    "        errors = []\n",
    "        \n",
    "        for i, record in enumerate(raw_data):\n",
    "            try:\n",
    "                # éªŒè¯å¿…éœ€å­—æ®µ\n",
    "                if not all(key in record for key in ['date', 'value']):\n",
    "                    errors.append(f\"è®°å½•{i}: ç¼ºå°‘å¿…éœ€å­—æ®µ\")\n",
    "                    continue\n",
    "                \n",
    "                # æ•°æ®ç±»å‹è½¬æ¢\n",
    "                cleaned_record = {\n",
    "                    'date': record['date'],\n",
    "                    'value': float(record['value']),\n",
    "                    'category': record.get('category', 'unknown')\n",
    "                }\n",
    "                \n",
    "                # æ•°æ®èŒƒå›´éªŒè¯\n",
    "                if cleaned_record['value'] < 0:\n",
    "                    errors.append(f\"è®°å½•{i}: å€¼ä¸èƒ½ä¸ºè´Ÿæ•°\")\n",
    "                    continue\n",
    "                \n",
    "                valid_records.append(cleaned_record)\n",
    "                \n",
    "            except Exception as e:\n",
    "                errors.append(f\"è®°å½•{i}: {str(e)}\")\n",
    "        \n",
    "        print(f\"   âœ… æœ‰æ•ˆè®°å½•: {len(valid_records)}\")\n",
    "        print(f\"   âŒ é”™è¯¯è®°å½•: {len(errors)}\")\n",
    "        \n",
    "        return {\n",
    "            'valid_data': valid_records,\n",
    "            'errors': errors,\n",
    "            'total_records': len(raw_data),\n",
    "            'valid_rate': len(valid_records) / len(raw_data) if raw_data else 0\n",
    "        }\n",
    "    \n",
    "    def statistical_analysis(self, cleaned_data: Dict) -> Dict:\n",
    "        \"\"\"ç»Ÿè®¡åˆ†æ\"\"\"\n",
    "        print(\"ğŸ“Š æ­¥éª¤2ï¼šç»Ÿè®¡åˆ†æ\")\n",
    "        \n",
    "        valid_data = cleaned_data['valid_data']\n",
    "        \n",
    "        if not valid_data:\n",
    "            return {'error': 'æ²¡æœ‰æœ‰æ•ˆæ•°æ®è¿›è¡Œåˆ†æ'}\n",
    "        \n",
    "        values = [record['value'] for record in valid_data]\n",
    "        categories = [record['category'] for record in valid_data]\n",
    "        \n",
    "        # åŸºç¡€ç»Ÿè®¡\n",
    "        stats = {\n",
    "            'count': len(values),\n",
    "            'sum': sum(values),\n",
    "            'mean': sum(values) / len(values),\n",
    "            'min': min(values),\n",
    "            'max': max(values),\n",
    "            'range': max(values) - min(values)\n",
    "        }\n",
    "        \n",
    "        # åˆ†ç±»ç»Ÿè®¡\n",
    "        category_stats = {}\n",
    "        for category in set(categories):\n",
    "            category_values = [v for i, v in enumerate(values) if categories[i] == category]\n",
    "            category_stats[category] = {\n",
    "                'count': len(category_values),\n",
    "                'sum': sum(category_values),\n",
    "                'mean': sum(category_values) / len(category_values)\n",
    "            }\n",
    "        \n",
    "        print(f\"   ğŸ“ˆ å¹³å‡å€¼: {stats['mean']:.2f}\")\n",
    "        print(f\"   ğŸ“‰ èŒƒå›´: {stats['min']:.2f} - {stats['max']:.2f}\")\n",
    "        print(f\"   ğŸ“Š ç±»åˆ«æ•°: {len(category_stats)}\")\n",
    "        \n",
    "        return {\n",
    "            'basic_stats': stats,\n",
    "            'category_stats': category_stats,\n",
    "            'data_quality': {\n",
    "                'valid_rate': cleaned_data['valid_rate'],\n",
    "                'error_count': len(cleaned_data['errors'])\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def trend_analysis(self, data: List[Dict], stats: Dict) -> Dict:\n",
    "        \"\"\"è¶‹åŠ¿åˆ†æ\"\"\"\n",
    "        print(\"ğŸ“ˆ æ­¥éª¤3ï¼šè¶‹åŠ¿åˆ†æ\")\n",
    "        \n",
    "        if len(data) < 2:\n",
    "            return {'trend': 'insufficient_data'}\n",
    "        \n",
    "        # ç®€å•è¶‹åŠ¿åˆ†æï¼ˆåŸºäºæ—¶é—´åºåˆ—ï¼‰\n",
    "        sorted_data = sorted(data, key=lambda x: x['date'])\n",
    "        \n",
    "        first_half = sorted_data[:len(sorted_data)//2]\n",
    "        second_half = sorted_data[len(sorted_data)//2:]\n",
    "        \n",
    "        first_avg = sum(r['value'] for r in first_half) / len(first_half)\n",
    "        second_avg = sum(r['value'] for r in second_half) / len(second_half)\n",
    "        \n",
    "        change_rate = (second_avg - first_avg) / first_avg * 100 if first_avg != 0 else 0\n",
    "        \n",
    "        if change_rate > 10:\n",
    "            trend = 'strong_upward'\n",
    "        elif change_rate > 5:\n",
    "            trend = 'moderate_upward'\n",
    "        elif change_rate > -5:\n",
    "            trend = 'stable'\n",
    "        elif change_rate > -10:\n",
    "            trend = 'moderate_downward'\n",
    "        else:\n",
    "            trend = 'strong_downward'\n",
    "        \n",
    "        print(f\"   ğŸ¯ è¶‹åŠ¿: {trend}\")\n",
    "        print(f\"   ğŸ“Š å˜åŒ–ç‡: {change_rate:.2f}%\")\n",
    "        \n",
    "        return {\n",
    "            'trend': trend,\n",
    "            'change_rate': change_rate,\n",
    "            'first_period_avg': first_avg,\n",
    "            'second_period_avg': second_avg\n",
    "        }\n",
    "    \n",
    "    def generate_report(self, stats: Dict, trend: Dict, raw_info: Dict) -> str:\n",
    "        \"\"\"ç”Ÿæˆåˆ†ææŠ¥å‘Š\"\"\"\n",
    "        print(\"ğŸ“ æ­¥éª¤4ï¼šç”ŸæˆæŠ¥å‘Š\")\n",
    "        \n",
    "        report_prompt = PromptTemplate(\n",
    "            input_variables=[\"stats\", \"trend\", \"data_quality\"],\n",
    "            template=\"\"\"\n",
    "è¯·åŸºäºä»¥ä¸‹æ•°æ®åˆ†æç»“æœç”Ÿæˆä¸€ä»½ä¸“ä¸šçš„åˆ†ææŠ¥å‘Šï¼š\n",
    "\n",
    "ç»Ÿè®¡æ•°æ®ï¼š{stats}\n",
    "è¶‹åŠ¿åˆ†æï¼š{trend}\n",
    "æ•°æ®è´¨é‡ï¼š{data_quality}\n",
    "\n",
    "è¯·ç”ŸæˆåŒ…å«ä»¥ä¸‹éƒ¨åˆ†çš„æŠ¥å‘Šï¼š\n",
    "1. æ•°æ®æ¦‚å†µ\n",
    "2. å…³é”®å‘ç°\n",
    "3. è¶‹åŠ¿æ´å¯Ÿ\n",
    "4. å»ºè®®å’Œç»“è®º\n",
    "\n",
    "æŠ¥å‘Šï¼š\n",
    "\"\"\"\n",
    "        )\n",
    "        \n",
    "        if hasattr(self.llm, 'invoke'):\n",
    "            try:\n",
    "                chain = report_prompt | self.llm | StrOutputParser()\n",
    "                report = chain.invoke({\n",
    "                    \"stats\": json.dumps(stats, ensure_ascii=False, indent=2),\n",
    "                    \"trend\": json.dumps(trend, ensure_ascii=False, indent=2),\n",
    "                    \"data_quality\": json.dumps(raw_info['data_quality'], ensure_ascii=False, indent=2)\n",
    "                })\n",
    "                return report\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # ç”Ÿæˆç®€å•æŠ¥å‘Š\n",
    "        basic_stats = stats['basic_stats']\n",
    "        trend_desc = {\n",
    "            'strong_upward': 'å¼ºçƒˆä¸Šå‡',\n",
    "            'moderate_upward': 'æ¸©å’Œä¸Šå‡',\n",
    "            'stable': 'ä¿æŒç¨³å®š',\n",
    "            'moderate_downward': 'æ¸©å’Œä¸‹é™',\n",
    "            'strong_downward': 'æ€¥å‰§ä¸‹é™'\n",
    "        }.get(trend['trend'], 'è¶‹åŠ¿ä¸æ˜')\n",
    "        \n",
    "        return f\"\"\"\n",
    "ğŸ“Š æ•°æ®åˆ†ææŠ¥å‘Š\n",
    "\n",
    "1. æ•°æ®æ¦‚å†µ\n",
    "   - æ€»è®°å½•æ•°: {raw_info['total_records']}\n",
    "   - æœ‰æ•ˆè®°å½•: {basic_stats['count']}\n",
    "   - æ•°æ®è´¨é‡: {raw_info['data_quality']['valid_rate']:.1%}\n",
    "\n",
    "2. å…³é”®å‘ç°\n",
    "   - å¹³å‡å€¼: {basic_stats['mean']:.2f}\n",
    "   - æ•°å€¼èŒƒå›´: {basic_stats['min']:.2f} - {basic_stats['max']:.2f}\n",
    "   - æ•°æ®æ€»å’Œ: {basic_stats['sum']:.2f}\n",
    "\n",
    "3. è¶‹åŠ¿æ´å¯Ÿ\n",
    "   - æ•´ä½“è¶‹åŠ¿: {trend_desc}\n",
    "   - å˜åŒ–ç‡: {trend['change_rate']:.2f}%\n",
    "   - å‰æœŸå¹³å‡: {trend['first_period_avg']:.2f}\n",
    "   - åæœŸå¹³å‡: {trend['second_period_avg']:.2f}\n",
    "\n",
    "4. å»ºè®®å’Œç»“è®º\n",
    "   - æ•°æ®è´¨é‡è‰¯å¥½ï¼Œå¯ä¿¡åº¦é«˜\n",
    "   - è¶‹åŠ¿å˜åŒ–æ˜æ˜¾ï¼Œéœ€è¦å…³æ³¨åç»­å‘å±•\n",
    "   - å»ºè®®æŒç»­ç›‘æ§æ•°æ®å˜åŒ–\n",
    "\"\"\"\n",
    "    \n",
    "    def analyze(self, raw_data: List[Dict]) -> Dict:\n",
    "        \"\"\"æ‰§è¡Œå®Œæ•´çš„æ•°æ®åˆ†ææµç¨‹\"\"\"\n",
    "        print(\"ğŸ”„ å¼€å§‹æ•°æ®åˆ†ææµç¨‹...\")\n",
    "        \n",
    "        # æ­¥éª¤1ï¼šæ•°æ®æ¸…ç†\n",
    "        cleaned_result = self.validate_and_clean_data(raw_data)\n",
    "        \n",
    "        # æ­¥éª¤2ï¼šç»Ÿè®¡åˆ†æ\n",
    "        stats_result = self.statistical_analysis(cleaned_result)\n",
    "        \n",
    "        if 'error' in stats_result:\n",
    "            return {'error': stats_result['error']}\n",
    "        \n",
    "        # æ­¥éª¤3ï¼šè¶‹åŠ¿åˆ†æ\n",
    "        trend_result = self.trend_analysis(cleaned_result['valid_data'], stats_result)\n",
    "        \n",
    "        # æ­¥éª¤4ï¼šç”ŸæˆæŠ¥å‘Š\n",
    "        report = self.generate_report(stats_result, trend_result, cleaned_result)\n",
    "        \n",
    "        print(\"âœ… æ•°æ®åˆ†æå®Œæˆ\")\n",
    "        \n",
    "        return {\n",
    "            'statistics': stats_result,\n",
    "            'trend': trend_result,\n",
    "            'report': report,\n",
    "            'data_info': cleaned_result\n",
    "        }\n",
    "\n",
    "# åˆ›å»ºæ•°æ®åˆ†æé“¾\n",
    "data_analyzer = DataAnalysisChain(llm)\n",
    "\n",
    "# å‡†å¤‡æµ‹è¯•æ•°æ®\n",
    "test_data = [\n",
    "    {'date': '2024-01-01', 'value': 100, 'category': 'A'},\n",
    "    {'date': '2024-01-02', 'value': 110, 'category': 'A'},\n",
    "    {'date': '2024-01-03', 'value': 95, 'category': 'B'},\n",
    "    {'date': '2024-01-04', 'value': 120, 'category': 'A'},\n",
    "    {'date': '2024-01-05', 'value': -10, 'category': 'C'},  # æ— æ•ˆæ•°æ®\n",
    "    {'date': '2024-01-06', 'value': 130, 'category': 'B'},\n",
    "    {'date': '2024-01-07', 'value': 'invalid', 'category': 'A'},  # æ— æ•ˆæ•°æ®\n",
    "    {'date': '2024-01-08', 'value': 140, 'category': 'A'},\n",
    "    {'date': '2024-01-09', 'value': 150, 'category': 'B'},\n",
    "    {'value': 160, 'category': 'A'},  # ç¼ºå°‘æ—¥æœŸ\n",
    "]\n",
    "\n",
    "print(\"\\nğŸ§ª æ•°æ®åˆ†æé“¾æµ‹è¯•ï¼š\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "result = data_analyzer.analyze(test_data)\n",
    "\n",
    "if 'error' not in result:\n",
    "    print(\"\\nğŸ“‹ åˆ†ææŠ¥å‘Šï¼š\")\n",
    "    print(result['report'])\nelse:\n",
    "    print(f\"âŒ åˆ†æå¤±è´¥: {result['error']}\")\n",
    "\n",
    "print(\"\\nâœ… ç»ƒä¹ 2å®Œæˆï¼ä½ å·²ç»å­¦ä¼šäº†å¦‚ä½•æ„å»ºå¤šæ­¥éª¤æ•°æ®åˆ†æé“¾ã€‚\")\n",
    "\n",
    "# è®°å½•ç»ƒä¹ å®Œæˆ\n",
    "tracker.complete_exercise(lesson_id, \"exercise_data_analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ğŸ“Š å­¦ä¹ æ€»ç»“ä¸è¿›åº¦å›é¡¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®Œæˆè¯¾ç¨‹å¹¶ç”Ÿæˆå­¦ä¹ æŠ¥å‘Š\n",
    "completion_time = tracker.complete_lesson(lesson_id)\n",
    "\n",
    "print(\"ğŸ“ LangChainé“¾ç³»ç»Ÿå…¥é—¨ - è¯¾ç¨‹å®Œæˆï¼\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nğŸ“š æœ¬è¯¾ç¨‹æ ¸å¿ƒçŸ¥è¯†ç‚¹ï¼š\")\n",
    "key_concepts = [\n",
    "    \"Chainçš„åŸºæœ¬æ¦‚å¿µå’Œç»„æˆ\",\n",
    "    \"LCEL (LangChain Expression Language)è¯­æ³•\",\n",
    "    \"ç®€å•LLMé“¾çš„æ„å»º\",\n",
    "    \"é¡ºåºé“¾çš„è®¾è®¡å’Œå®ç°\",\n",
    "    \"å¹¶è¡Œé“¾çš„åˆ›å»ºå’Œä¼˜åŒ–\",\n",
    "    \"è‡ªå®šä¹‰é“¾çš„å¼€å‘æŠ€å·§\",\n",
    "    \"é”™è¯¯å¤„ç†å’Œè°ƒè¯•æœºåˆ¶\",\n",
    "    \"é“¾çš„æ€§èƒ½ç›‘æ§\",\n",
    "    \"æ™ºèƒ½å®¢æœç³»ç»Ÿè®¾è®¡\",\n",
    "    \"æ•°æ®åˆ†ææµç¨‹è‡ªåŠ¨åŒ–\"\n",
    "]\n",
    "\n",
    "for i, concept in enumerate(key_concepts, 1):\n",
    "    print(f\"   {i:2d}. {concept}\")\n",
    "\n",
    "print(\"\\nğŸ› ï¸ å®è·µæŠ€èƒ½æ”¶è·ï¼š\")\n",
    "skills = [\n",
    "    \"æŒæ¡é“¾çš„åŸºç¡€æ„å»ºæ–¹æ³•\",\n",
    "    \"èƒ½å¤Ÿè®¾è®¡å¤æ‚çš„å¤šæ­¥éª¤å¤„ç†æµç¨‹\",\n",
    "    \"å®ç°å¹¶è¡Œå¤„ç†å’Œç»“æœåˆå¹¶\",\n",
    "    \"æ„å»ºå¼ºå¥çš„é”™è¯¯å¤„ç†æœºåˆ¶\",\n",
    "    \"å¼€å‘å®é™…ä¸šåŠ¡åº”ç”¨ç³»ç»Ÿ\"\n",
    "]\n",
    "\n",
    "for i, skill in enumerate(skills, 1):\n",
    "    print(f\"   âš¡ {skill}\")\n",
    "\n",
    "print(\"\\nğŸ¯ æœ€ä½³å®è·µè¦ç‚¹ï¼š\")\n",
    "best_practices = [\n",
    "    \"åˆç†åˆ†è§£å¤æ‚ä»»åŠ¡ä¸ºç®€å•æ­¥éª¤\",\n",
    "    \"ä¸ºæ¯ä¸ªæ­¥éª¤æ·»åŠ é”™è¯¯å¤„ç†\",\n",
    "    \"ä½¿ç”¨å¹¶è¡Œå¤„ç†æé«˜æ•ˆç‡\",\n",
    "    \"å®æ–½æ—¥å¿—è®°å½•å’Œè°ƒè¯•æœºåˆ¶\",\n",
    "    \"è€ƒè™‘æ€§èƒ½ä¼˜åŒ–å’Œæˆæœ¬æ§åˆ¶\"\n",
    "]\n",
    "\n",
    "for practice in best_practices:\n",
    "    print(f\"   ğŸ’¡ {practice}\")\n",
    "\n",
    "print(\"\\nğŸ¯ ä¸‹ä¸€æ­¥å­¦ä¹ å»ºè®®ï¼š\")\n",
    "next_steps = [\n",
    "    \"å­¦ä¹ Agentæ™ºèƒ½ä»£ç†ç³»ç»Ÿï¼ˆ02_agents_basics.ipynbï¼‰\",\n",
    "    \"æ¢ç´¢è®°å¿†ç³»ç»Ÿå’Œä¸Šä¸‹æ–‡ç®¡ç†\",\n",
    "    \"æ·±å…¥Chainçš„é«˜çº§ç‰¹æ€§\",\n",
    "    \"å®è·µæ›´å¤æ‚çš„ä¸šåŠ¡åœºæ™¯\"\n",
    "]\n",
    "\n",
    "for i, step in enumerate(next_steps, 1):\n",
    "    print(f\"   ğŸ“ˆ {step}\")\n",
    "\n",
    "# æ˜¾ç¤ºæ•´ä½“å­¦ä¹ è¿›åº¦\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "tracker.display_progress_summary()\n",
    "\n",
    "print(\"\\nğŸ‰ æ­å–œå®ŒæˆLangChainé“¾ç³»ç»Ÿå…¥é—¨è¯¾ç¨‹ï¼\")\n",
    "print(\"ğŸ’ª ä½ å·²ç»æŒæ¡äº†æ„å»ºå¤æ‚AIåº”ç”¨çš„æ ¸å¿ƒæŠ€æœ¯ï¼\")\n",
    "print(\"ğŸš€ ç»§ç»­å­¦ä¹ Agentå’ŒMemoryï¼Œæ„å»ºæ›´æ™ºèƒ½çš„åº”ç”¨ï¼\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}