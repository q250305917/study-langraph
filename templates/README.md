# LangChain Learning æ¨¡æ¿ç³»ç»Ÿ

[![Python Version](https://img.shields.io/badge/python-3.8+-blue.svg)](https://python.org)
[![LangChain Version](https://img.shields.io/badge/langchain-0.1+-green.svg)](https://github.com/langchain-ai/langchain)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)

è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„LangChainå­¦ä¹ æ¨¡æ¿ç³»ç»Ÿï¼Œæä¾›äº†å‚æ•°åŒ–çš„ç¤ºä¾‹ä»£ç æ¨¡æ¿ï¼Œè¦†ç›–LangChainçš„æ‰€æœ‰æ ¸å¿ƒåº”ç”¨åœºæ™¯ã€‚é€šè¿‡è¿™äº›æ¨¡æ¿ï¼Œå­¦ä¹ è€…å¯ä»¥å¿«é€Ÿç†è§£å’Œåº”ç”¨ä¸åŒçš„LangChainåŠŸèƒ½ï¼ŒåŒæ—¶è·å¾—å¯å¤ç”¨çš„ä»£ç ç»“æ„ã€‚

## ğŸ“‹ ç›®å½•

- [ç³»ç»Ÿæ¦‚è§ˆ](#ç³»ç»Ÿæ¦‚è§ˆ)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [ç›®å½•ç»“æ„](#ç›®å½•ç»“æ„)
- [æ¨¡æ¿ç±»å‹](#æ¨¡æ¿ç±»å‹)
- [é…ç½®ç³»ç»Ÿ](#é…ç½®ç³»ç»Ÿ)
- [ä½¿ç”¨ç¤ºä¾‹](#ä½¿ç”¨ç¤ºä¾‹)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
- [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)
- [è´¡çŒ®æŒ‡å—](#è´¡çŒ®æŒ‡å—)

## ğŸ¯ ç³»ç»Ÿæ¦‚è§ˆ

### æ ¸å¿ƒç‰¹æ€§

- **ğŸ“ å‚æ•°åŒ–æ¨¡æ¿**: æ‰€æœ‰æ¨¡æ¿éƒ½æ”¯æŒçµæ´»çš„å‚æ•°é…ç½®
- **ğŸ”„ ç»Ÿä¸€æ¥å£**: ä¸€è‡´çš„setupã€executeã€get_exampleæ–¹æ³•
- **âš¡ é«˜æ€§èƒ½**: æ”¯æŒå¼‚æ­¥æ‰§è¡Œå’Œç¼“å­˜æœºåˆ¶
- **ğŸ›¡ï¸ ç±»å‹å®‰å…¨**: å®Œæ•´çš„ç±»å‹æ³¨è§£å’Œå‚æ•°éªŒè¯
- **ğŸ“Š ç›‘æ§æ”¯æŒ**: å†…ç½®æ€§èƒ½æŒ‡æ ‡æ”¶é›†å’Œæ‰§è¡Œå†å²
- **ğŸ”§ æ˜“äºæ‰©å±•**: æ¨¡å—åŒ–è®¾è®¡ï¼Œä¾¿äºæ·»åŠ æ–°æ¨¡æ¿

### æ”¯æŒçš„LangChainåŠŸèƒ½

- **LLMè°ƒç”¨**: OpenAIã€Anthropicã€æœ¬åœ°æ¨¡å‹ç­‰
- **æç¤ºå·¥ç¨‹**: å„ç§åœºæ™¯çš„æç¤ºè¯æ¨¡æ¿
- **é“¾ç»„åˆ**: é¡ºåºé“¾ã€å¹¶è¡Œé“¾ã€æ¡ä»¶é“¾ç­‰
- **æ™ºèƒ½ä»£ç†**: ReActã€å·¥å…·è°ƒç”¨ã€è§„åˆ’ä»£ç†ç­‰
- **æ•°æ®å¤„ç†**: æ–‡æ¡£åŠ è½½ã€æ–‡æœ¬åˆ†å‰²ã€å‘é‡åŒ–
- **è®°å¿†ç³»ç»Ÿ**: å¯¹è¯è®°å¿†ã€æ‘˜è¦è®°å¿†ã€å‘é‡è®°å¿†
- **è¯„ä¼°å·¥å…·**: å‡†ç¡®æ€§è¯„ä¼°ã€æ€§èƒ½åˆ†æã€æˆæœ¬æ§åˆ¶

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

```bash
# Pythonç‰ˆæœ¬è¦æ±‚
python >= 3.8

# æ ¸å¿ƒä¾èµ–
pip install langchain>=0.1.0
pip install langchain-openai>=0.1.0
pip install langchain-anthropic>=0.1.0
```

### åŸºç¡€å®‰è£…

```bash
# å…‹éš†é¡¹ç›®
git clone <repository-url>
cd study_langraph

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# è®¾ç½®ç¯å¢ƒå˜é‡
export OPENAI_API_KEY="your-openai-api-key"
export ANTHROPIC_API_KEY="your-anthropic-api-key"
```

### ç¬¬ä¸€ä¸ªç¤ºä¾‹

```python
from templates.llm.openai_template import OpenAITemplate

# åˆ›å»ºLLMæ¨¡æ¿å®ä¾‹
template = OpenAITemplate()

# é…ç½®å‚æ•°
template.setup(
    api_key="your-openai-api-key",
    model_name="gpt-3.5-turbo",
    temperature=0.7
)

# æ‰§è¡Œè°ƒç”¨
result = template.run("ä»‹ç»ä¸€ä¸‹LangChainçš„ä¸»è¦ç‰¹ç‚¹")
print(result.content)
```

## ğŸ“ ç›®å½•ç»“æ„

```
templates/
â”œâ”€â”€ README.md                    # æœ¬æ–‡æ¡£
â”œâ”€â”€ __init__.py                  # åŒ…åˆå§‹åŒ–
â”œâ”€â”€ base/                        # åŸºç¡€æ¶æ„
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ template_base.py         # æ¨¡æ¿åŸºç±»
â”‚   â”œâ”€â”€ config_loader.py         # é…ç½®åŠ è½½å™¨
â”‚   â””â”€â”€ parameter_validator.py   # å‚æ•°éªŒè¯å™¨
â”œâ”€â”€ configs/                     # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ default.yaml             # é»˜è®¤é…ç½®
â”‚   â”œâ”€â”€ development.yaml         # å¼€å‘ç¯å¢ƒé…ç½®
â”‚   â”œâ”€â”€ production.yaml          # ç”Ÿäº§ç¯å¢ƒé…ç½®
â”‚   â””â”€â”€ template_configs/        # æ¨¡æ¿ä¸“ç”¨é…ç½®
â”‚       â”œâ”€â”€ llm_template.yaml
â”‚       â”œâ”€â”€ data_template.yaml
â”‚       â”œâ”€â”€ chain_template.yaml
â”‚       â””â”€â”€ agent_template.yaml
â”œâ”€â”€ llm/                         # LLMæ¨¡æ¿
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ openai_template.py       # OpenAIæ¨¡æ¿
â”‚   â”œâ”€â”€ anthropic_template.py    # Anthropicæ¨¡æ¿
â”‚   â”œâ”€â”€ local_llm_template.py    # æœ¬åœ°æ¨¡å‹æ¨¡æ¿
â”‚   â””â”€â”€ multi_model_template.py  # å¤šæ¨¡å‹å¯¹æ¯”æ¨¡æ¿
â”œâ”€â”€ prompts/                     # æç¤ºè¯æ¨¡æ¿
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ chat_template.py         # å¯¹è¯æ¨¡æ¿
â”‚   â”œâ”€â”€ completion_template.py   # è¡¥å…¨æ¨¡æ¿
â”‚   â”œâ”€â”€ few_shot_template.py     # å°‘æ ·æœ¬å­¦ä¹ æ¨¡æ¿
â”‚   â””â”€â”€ role_playing_template.py # è§’è‰²æ‰®æ¼”æ¨¡æ¿
â”œâ”€â”€ chains/                      # é“¾ç»„åˆæ¨¡æ¿
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ sequential_chain.py      # é¡ºåºé“¾æ¨¡æ¿
â”‚   â”œâ”€â”€ parallel_chain.py        # å¹¶è¡Œé“¾æ¨¡æ¿
â”‚   â”œâ”€â”€ conditional_chain.py     # æ¡ä»¶é“¾æ¨¡æ¿
â”‚   â””â”€â”€ pipeline_chain.py        # ç®¡é“é“¾æ¨¡æ¿
â”œâ”€â”€ agents/                      # ä»£ç†æ¨¡æ¿
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ react_agent.py          # ReActä»£ç†æ¨¡æ¿
â”‚   â”œâ”€â”€ tool_calling_agent.py   # å·¥å…·è°ƒç”¨ä»£ç†æ¨¡æ¿
â”‚   â”œâ”€â”€ planning_agent.py       # è§„åˆ’ä»£ç†æ¨¡æ¿
â”‚   â””â”€â”€ multi_agent_template.py # å¤šä»£ç†åä½œæ¨¡æ¿
â”œâ”€â”€ data/                        # æ•°æ®å¤„ç†æ¨¡æ¿
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ document_loader.py       # æ–‡æ¡£åŠ è½½æ¨¡æ¿
â”‚   â”œâ”€â”€ text_splitter.py        # æ–‡æœ¬åˆ†å‰²æ¨¡æ¿
â”‚   â”œâ”€â”€ vectorstore_template.py # å‘é‡å­˜å‚¨æ¨¡æ¿
â”‚   â””â”€â”€ retrieval_template.py   # æ£€ç´¢æ¨¡æ¿
â”œâ”€â”€ memory/                      # è®°å¿†ç³»ç»Ÿæ¨¡æ¿
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conversation_memory.py   # å¯¹è¯è®°å¿†æ¨¡æ¿
â”‚   â”œâ”€â”€ summary_memory.py       # æ‘˜è¦è®°å¿†æ¨¡æ¿
â”‚   â”œâ”€â”€ vector_memory.py        # å‘é‡è®°å¿†æ¨¡æ¿
â”‚   â””â”€â”€ knowledge_base.py       # çŸ¥è¯†åº“æ¨¡æ¿
â”œâ”€â”€ evaluation/                  # è¯„ä¼°æ¨¡æ¿
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ accuracy_eval.py        # å‡†ç¡®æ€§è¯„ä¼°æ¨¡æ¿
â”‚   â”œâ”€â”€ performance_eval.py     # æ€§èƒ½è¯„ä¼°æ¨¡æ¿
â”‚   â”œâ”€â”€ cost_analysis.py        # æˆæœ¬åˆ†ææ¨¡æ¿
â”‚   â””â”€â”€ ab_testing.py          # A/Bæµ‹è¯•æ¨¡æ¿
â””â”€â”€ examples/                    # ä½¿ç”¨ç¤ºä¾‹
    â”œâ”€â”€ basic_examples/          # åŸºç¡€ç¤ºä¾‹
    â”œâ”€â”€ advanced_examples/       # é«˜çº§ç¤ºä¾‹
    â”œâ”€â”€ tutorials/               # æ•™ç¨‹
    â””â”€â”€ best_practices/          # æœ€ä½³å®è·µ
```

## ğŸ”§ æ¨¡æ¿ç±»å‹

### 1. LLMæ¨¡æ¿ (`templates/llm/`)

ç”¨äºè°ƒç”¨å„ç§å¤§è¯­è¨€æ¨¡å‹çš„æ¨¡æ¿ã€‚

**æ”¯æŒçš„æ¨¡å‹**:
- OpenAI (GPT-3.5, GPT-4)
- Anthropic (Claudeç³»åˆ—)
- æœ¬åœ°æ¨¡å‹ (Llama, ChatGLMç­‰)

**æ ¸å¿ƒåŠŸèƒ½**:
- ç»Ÿä¸€çš„æ¨¡å‹è°ƒç”¨æ¥å£
- å‚æ•°è‡ªåŠ¨éªŒè¯
- é”™è¯¯é‡è¯•æœºåˆ¶
- æˆæœ¬ç›‘æ§

**ä½¿ç”¨ç¤ºä¾‹**:
```python
from templates.llm.openai_template import OpenAITemplate

template = OpenAITemplate()
template.setup(
    model_name="gpt-4",
    temperature=0.5,
    max_tokens=2000
)
result = template.run("è§£é‡Šé‡å­è®¡ç®—çš„åŸºæœ¬åŸç†")
```

### 2. æç¤ºè¯æ¨¡æ¿ (`templates/prompts/`)

ç®¡ç†å„ç§åº”ç”¨åœºæ™¯çš„æç¤ºè¯æ¨¡æ¿ã€‚

**æ¨¡æ¿ç±»å‹**:
- å¯¹è¯æ¨¡æ¿: å¤šè½®å¯¹è¯åœºæ™¯
- è¡¥å…¨æ¨¡æ¿: æ–‡æœ¬è¡¥å…¨ä»»åŠ¡
- å°‘æ ·æœ¬æ¨¡æ¿: Few-shotå­¦ä¹ 
- è§’è‰²æ‰®æ¼”: ç‰¹å®šè§’è‰²è®¾å®š

**ç‰¹æ€§**:
- åŠ¨æ€å‚æ•°æ›¿æ¢
- æ¨¡æ¿ç»§æ‰¿å’Œç»„åˆ
- ä¸Šä¸‹æ–‡ç®¡ç†
- æ ¼å¼éªŒè¯

### 3. é“¾ç»„åˆæ¨¡æ¿ (`templates/chains/`)

å®ç°å¤æ‚çš„å·¥ä½œæµç»„åˆã€‚

**é“¾ç±»å‹**:
- **é¡ºåºé“¾**: æ­¥éª¤ä¾æ¬¡æ‰§è¡Œ
- **å¹¶è¡Œé“¾**: åŒæ—¶æ‰§è¡Œå¤šä¸ªæ­¥éª¤
- **æ¡ä»¶é“¾**: æ ¹æ®æ¡ä»¶é€‰æ‹©æ‰§è¡Œè·¯å¾„
- **ç®¡é“é“¾**: æ•°æ®æµå¼å¤„ç†

**ä½¿ç”¨åœºæ™¯**:
- æ–‡æ¡£å¤„ç†æµæ°´çº¿
- å¤šæ­¥éª¤æ¨ç†ä»»åŠ¡
- å†…å®¹ç”Ÿæˆå·¥ä½œæµ
- æ•°æ®åˆ†æç®¡é“

### 4. ä»£ç†æ¨¡æ¿ (`templates/agents/`)

æ™ºèƒ½ä»£ç†å®ç°ï¼Œèƒ½å¤Ÿè‡ªä¸»å†³ç­–å’Œä½¿ç”¨å·¥å…·ã€‚

**ä»£ç†ç±»å‹**:
- **ReActä»£ç†**: æ¨ç†-è¡ŒåŠ¨å¾ªç¯
- **å·¥å…·è°ƒç”¨ä»£ç†**: ç›´æ¥å·¥å…·è°ƒç”¨
- **è§„åˆ’ä»£ç†**: å…ˆè§„åˆ’åæ‰§è¡Œ
- **å¤šä»£ç†åä½œ**: ä»£ç†é—´åä½œ

**æ ¸å¿ƒèƒ½åŠ›**:
- è‡ªä¸»æ¨ç†å†³ç­–
- å·¥å…·é€‰æ‹©å’Œä½¿ç”¨
- ä»»åŠ¡åˆ†è§£æ‰§è¡Œ
- é”™è¯¯å¤„ç†æ¢å¤

### 5. æ•°æ®å¤„ç†æ¨¡æ¿ (`templates/data/`)

å¤„ç†å„ç§æ•°æ®æºå’Œæ ¼å¼ã€‚

**åŠŸèƒ½æ¨¡å—**:
- **æ–‡æ¡£åŠ è½½å™¨**: æ”¯æŒPDFã€Wordã€HTMLç­‰
- **æ–‡æœ¬åˆ†å‰²å™¨**: æ™ºèƒ½æ–‡æœ¬åˆ†å—
- **å‘é‡å­˜å‚¨**: å¤šç§å‘é‡æ•°æ®åº“
- **æ£€ç´¢æ¨¡æ¿**: è¯­ä¹‰æœç´¢å’Œè¿‡æ»¤

**æ”¯æŒæ ¼å¼**:
- æ–‡æ¡£: PDF, DOCX, TXT, HTML, MD
- æ•°æ®: CSV, JSON, XML
- ç½‘é¡µ: HTML, XML, RSS

### 6. è®°å¿†ç³»ç»Ÿæ¨¡æ¿ (`templates/memory/`)

ç®¡ç†å¯¹è¯å†å²å’ŒçŸ¥è¯†çŠ¶æ€ã€‚

**è®°å¿†ç±»å‹**:
- **ç¼“å†²è®°å¿†**: å®Œæ•´å¯¹è¯å†å²
- **æ‘˜è¦è®°å¿†**: å‹ç¼©å†å²ä¿¡æ¯
- **å‘é‡è®°å¿†**: è¯­ä¹‰ç›¸ä¼¼æ€§æ£€ç´¢
- **çŸ¥è¯†åº“**: ç»“æ„åŒ–çŸ¥è¯†å­˜å‚¨

### 7. è¯„ä¼°æ¨¡æ¿ (`templates/evaluation/`)

è¯„ä¼°ç³»ç»Ÿæ€§èƒ½å’Œè´¨é‡ã€‚

**è¯„ä¼°ç»´åº¦**:
- **å‡†ç¡®æ€§è¯„ä¼°**: ç­”æ¡ˆæ­£ç¡®æ€§
- **æ€§èƒ½è¯„ä¼°**: æ‰§è¡Œé€Ÿåº¦å’Œèµ„æºä½¿ç”¨
- **æˆæœ¬åˆ†æ**: APIè°ƒç”¨æˆæœ¬
- **A/Bæµ‹è¯•**: å¯¹æ¯”å®éªŒ

## âš™ï¸ é…ç½®ç³»ç»Ÿ

### é…ç½®æ–‡ä»¶å±‚çº§

1. **å…¨å±€é»˜è®¤é…ç½®** (`configs/default.yaml`)
2. **ç¯å¢ƒç‰¹å®šé…ç½®** (`configs/development.yaml`, `configs/production.yaml`)
3. **æ¨¡æ¿ä¸“ç”¨é…ç½®** (`configs/template_configs/`)

### é…ç½®ä¼˜å…ˆçº§

```
æ¨¡æ¿ä¸“ç”¨é…ç½® > ç¯å¢ƒé…ç½® > å…¨å±€é»˜è®¤é…ç½®
```

### ç¯å¢ƒå˜é‡æ”¯æŒ

é…ç½®æ–‡ä»¶æ”¯æŒç¯å¢ƒå˜é‡å¼•ç”¨ï¼š

```yaml
openai:
  api_key: "${OPENAI_API_KEY}"
  organization: "${OPENAI_ORG_ID}"
```

### å¼€å‘ç¯å¢ƒé…ç½®ç¤ºä¾‹

```yaml
# development.yaml ç‰¹ç‚¹
global:
  debug_mode: true
  log_level: "DEBUG"
  cache:
    enabled: false  # å¼€å‘æ—¶ç¦ç”¨ç¼“å­˜
llm:
  openai:
    model: "gpt-3.5-turbo"  # ä½¿ç”¨ä¾¿å®œçš„æ¨¡å‹
data:
  chunk_size: 500  # è¾ƒå°çš„å—ä¾¿äºè°ƒè¯•
```

### ç”Ÿäº§ç¯å¢ƒé…ç½®ç¤ºä¾‹

```yaml
# production.yaml ç‰¹ç‚¹
global:
  debug_mode: false
  log_level: "INFO"
  cache:
    enabled: true
    backend: "redis"  # ä½¿ç”¨Redisç¼“å­˜
llm:
  openai:
    model: "gpt-4"  # ä½¿ç”¨æ›´å¼ºçš„æ¨¡å‹
security:
  api_key_required: true
  rate_limiting: true
```

## ğŸ“š ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹1: æ–‡æ¡£é—®ç­”ç³»ç»Ÿ

```python
from templates.data.document_loader import DocumentLoaderTemplate
from templates.data.vectorstore_template import VectorStoreTemplate
from templates.chains.sequential_chain import SequentialChainTemplate
from templates.llm.openai_template import OpenAITemplate

# 1. åŠ è½½æ–‡æ¡£
doc_loader = DocumentLoaderTemplate()
doc_loader.setup(
    file_path="./documents/company_handbook.pdf",
    chunk_size=1000,
    chunk_overlap=100
)
documents = doc_loader.run()

# 2. åˆ›å»ºå‘é‡å­˜å‚¨
vectorstore = VectorStoreTemplate()
vectorstore.setup(
    documents=documents,
    embedding_model="text-embedding-ada-002",
    vectorstore_type="chroma",
    collection_name="company_docs"
)
vectorstore.run()

# 3. åˆ›å»ºé—®ç­”é“¾
llm = OpenAITemplate()
llm.setup(model_name="gpt-4", temperature=0.1)

qa_chain = SequentialChainTemplate()
qa_chain.setup(
    llm=llm.llm,
    steps=[
        {
            "name": "retrieval",
            "prompt": "åŸºäºæ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜: {question}",
            "output_key": "answer"
        }
    ],
    input_variables=["question"]
)

# 4. ä½¿ç”¨é—®ç­”ç³»ç»Ÿ
answer = qa_chain.run("å…¬å¸çš„ä¼‘å‡æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿ")
print(answer)
```

### ç¤ºä¾‹2: å¤šæ¨¡å‹å¯¹æ¯”åˆ†æ

```python
from templates.llm.multi_model_template import MultiModelTemplate

# åˆ›å»ºå¤šæ¨¡å‹å¯¹æ¯”æ¨¡æ¿
multi_model = MultiModelTemplate()
multi_model.setup(
    models=[
        {
            "name": "gpt-4",
            "provider": "openai",
            "config": {"temperature": 0.7}
        },
        {
            "name": "claude-3-sonnet",
            "provider": "anthropic",
            "config": {"temperature": 0.7}
        }
    ]
)

# å¯¹æ¯”åˆ†æ
prompt = "åˆ†æäººå·¥æ™ºèƒ½å¯¹æ•™è‚²è¡Œä¸šçš„å½±å“"
results = multi_model.run(prompt)

for model_name, result in results.items():
    print(f"=== {model_name} ===")
    print(result.content)
    print(f"æ‰§è¡Œæ—¶é—´: {result.execution_time}ç§’")
    print(f"Tokenä½¿ç”¨: {result.token_usage}")
    print()
```

### ç¤ºä¾‹3: æ™ºèƒ½ä»£ç†å·¥ä½œæµ

```python
from templates.agents.react_agent import ReactAgentTemplate
from langchain.tools import Calculator, Wikipedia

# åˆ›å»ºå·¥å…·åˆ—è¡¨
tools = [
    Calculator(),
    Wikipedia()
]

# åˆ›å»ºReActä»£ç†
agent = ReactAgentTemplate()
agent.setup(
    tools=tools,
    llm_config={
        "model_name": "gpt-4",
        "temperature": 0.1
    },
    max_iterations=10,
    verbose=True
)

# æ‰§è¡Œå¤æ‚ä»»åŠ¡
task = """
å¸®æˆ‘åˆ†æä¸€ä¸‹ï¼š
1. æŸ¥è¯¢çˆ±å› æ–¯å¦çš„å‡ºç”Ÿå¹´ä»½
2. è®¡ç®—ä»–å¦‚æœè¿˜æ´»ç€ç°åœ¨å¤šå°‘å²
3. åˆ†æä»–å¯¹ç°ä»£ç‰©ç†å­¦çš„ä¸»è¦è´¡çŒ®
"""

result = agent.run(task)
print(result)
```

### ç¤ºä¾‹4: æ‰¹é‡æ•°æ®å¤„ç†

```python
from templates.data.text_splitter import TextSplitterTemplate
from templates.evaluation.performance_eval import PerformanceEvalTemplate
import asyncio

async def batch_process_documents():
    # åˆ›å»ºæ–‡æœ¬åˆ†å‰²æ¨¡æ¿
    splitter = TextSplitterTemplate()
    splitter.setup(
        splitter_type="recursive",
        chunk_size=1000,
        chunk_overlap=100,
        parallel_processing=True,
        max_workers=4
    )
    
    # æ‰¹é‡å¤„ç†æ–‡æ¡£
    document_paths = [
        "./docs/doc1.pdf",
        "./docs/doc2.pdf", 
        "./docs/doc3.pdf"
    ]
    
    # æ€§èƒ½è¯„ä¼°
    perf_eval = PerformanceEvalTemplate()
    perf_eval.setup(benchmark_enabled=True)
    
    with perf_eval.measure("batch_processing"):
        results = await splitter.run_async(document_paths)
    
    # è¾“å‡ºæ€§èƒ½æŠ¥å‘Š
    report = perf_eval.generate_report()
    print(report)
    
    return results

# è¿è¡Œå¼‚æ­¥å¤„ç†
results = asyncio.run(batch_process_documents())
```

## ğŸ¯ æœ€ä½³å®è·µ

### 1. æ¨¡æ¿é€‰æ‹©æŒ‡å—

**é€‰æ‹©LLMæ¨¡æ¿æ—¶**:
- å¼€å‘é˜¶æ®µ: ä½¿ç”¨`gpt-3.5-turbo`é™ä½æˆæœ¬
- ç”Ÿäº§ç¯å¢ƒ: ä½¿ç”¨`gpt-4`ä¿è¯è´¨é‡  
- æœ¬åœ°éƒ¨ç½²: ä½¿ç”¨`local_llm_template`

**é€‰æ‹©é“¾ç±»å‹æ—¶**:
- ç®€å•ä»»åŠ¡: ä½¿ç”¨å•ä¸ªLLMæ¨¡æ¿
- é¡ºåºä¾èµ–: ä½¿ç”¨`sequential_chain`
- ç‹¬ç«‹å¹¶è¡Œ: ä½¿ç”¨`parallel_chain`
- æ¡ä»¶åˆ†æ”¯: ä½¿ç”¨`conditional_chain`

**é€‰æ‹©ä»£ç†ç±»å‹æ—¶**:
- éœ€è¦æ¨ç†: ä½¿ç”¨`react_agent`
- æ•ˆç‡ä¼˜å…ˆ: ä½¿ç”¨`tool_calling_agent`
- å¤æ‚ä»»åŠ¡: ä½¿ç”¨`planning_agent`

### 2. æ€§èƒ½ä¼˜åŒ–

**ç¼“å­˜ç­–ç•¥**:
```python
# å¯ç”¨æ¨¡æ¿çº§åˆ«ç¼“å­˜
template.setup(
    cache_enabled=True,
    cache_ttl=1800  # 30åˆ†é’Ÿ
)

# å…¨å±€ç¼“å­˜é…ç½®
from templates.base.config_loader import ConfigLoader
config = ConfigLoader()
config.enable_global_cache("redis")
```

**å¼‚æ­¥æ‰§è¡Œ**:
```python
# ä½¿ç”¨å¼‚æ­¥æ‰§è¡Œæé«˜å¹¶å‘æ€§èƒ½
results = await template.run_async(input_data)

# æ‰¹é‡å¼‚æ­¥å¤„ç†
tasks = [template.run_async(data) for data in batch_data]
results = await asyncio.gather(*tasks)
```

**èµ„æºç®¡ç†**:
```python
# è®¾ç½®èµ„æºé™åˆ¶
template.setup(
    max_memory_usage=1024,  # 1GBå†…å­˜é™åˆ¶
    timeout=60.0,           # 60ç§’è¶…æ—¶
    max_workers=4           # æœ€å¤§å¹¶å‘æ•°
)
```

### 3. é”™è¯¯å¤„ç†

**é‡è¯•æœºåˆ¶**:
```python
template.setup(
    retry_count=3,
    retry_delay=1.0,
    error_handling="graceful"
)
```

**ç›‘æ§å’Œæ—¥å¿—**:
```python
# å¯ç”¨è¯¦ç»†æ—¥å¿—
import logging
logging.basicConfig(level=logging.DEBUG)

# æ€§èƒ½ç›‘æ§
template.setup(enable_metrics=True)
metrics = template.get_metrics()
print(f"æˆåŠŸç‡: {metrics['success_rate']:.2%}")
print(f"å¹³å‡æ‰§è¡Œæ—¶é—´: {metrics['avg_execution_time']:.2f}ç§’")
```

### 4. å®‰å…¨æœ€ä½³å®è·µ

**APIå¯†é’¥ç®¡ç†**:
```bash
# ä½¿ç”¨ç¯å¢ƒå˜é‡
export OPENAI_API_KEY="sk-..."
export ANTHROPIC_API_KEY="sk-ant-..."

# æˆ–ä½¿ç”¨.envæ–‡ä»¶
echo "OPENAI_API_KEY=sk-..." > .env
```

**è¾“å…¥éªŒè¯**:
```python
# å¯ç”¨ä¸¥æ ¼å‚æ•°éªŒè¯
template.setup(
    parameter_validation="strict",
    sanitize_input=True,
    max_input_length=10000
)
```

**æƒé™æ§åˆ¶**:
```python
# ä»£ç†æƒé™é™åˆ¶
agent.setup(
    allowed_tools=["calculator", "search"],  # ç™½åå•
    forbidden_tools=["file_delete"],         # é»‘åå•
    sandbox_mode=True                        # æ²™ç®±æ¨¡å¼
)
```

### 5. æµ‹è¯•ç­–ç•¥

**å•å…ƒæµ‹è¯•**:
```python
import unittest
from templates.llm.openai_template import OpenAITemplate

class TestOpenAITemplate(unittest.TestCase):
    def setUp(self):
        self.template = OpenAITemplate()
        
    def test_basic_setup(self):
        self.template.setup(
            model_name="gpt-3.5-turbo",
            temperature=0.5
        )
        self.assertEqual(self.template.config.model_name, "gpt-3.5-turbo")
        
    def test_example_execution(self):
        example = self.template.get_example()
        # ä½¿ç”¨ç¤ºä¾‹å‚æ•°è¿›è¡Œæµ‹è¯•
        self.template.setup(**example["setup_parameters"])
        result = self.template.run(example["execute_parameters"]["input"])
        self.assertIsNotNone(result)
```

**é›†æˆæµ‹è¯•**:
```python
# ç«¯åˆ°ç«¯æµ‹è¯•
def test_document_qa_pipeline():
    # æµ‹è¯•å®Œæ•´çš„æ–‡æ¡£é—®ç­”æµç¨‹
    doc_loader = DocumentLoaderTemplate()
    vectorstore = VectorStoreTemplate()
    qa_chain = SequentialChainTemplate()
    
    # æ‰§è¡Œå®Œæ•´æµç¨‹
    documents = doc_loader.run("test_document.pdf")
    vectorstore.run(documents)
    answer = qa_chain.run("æµ‹è¯•é—®é¢˜")
    
    assert len(answer) > 0
```

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ

#### 1. APIå¯†é’¥é”™è¯¯

**é”™è¯¯ä¿¡æ¯**: `AuthenticationError: Invalid API key`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥ç¯å¢ƒå˜é‡
echo $OPENAI_API_KEY

# é‡æ–°è®¾ç½®APIå¯†é’¥
export OPENAI_API_KEY="your-correct-api-key"

# éªŒè¯å¯†é’¥æ ¼å¼
python -c "import os; print(len(os.getenv('OPENAI_API_KEY', '')))"
```

#### 2. å†…å­˜ä¸è¶³é”™è¯¯

**é”™è¯¯ä¿¡æ¯**: `MemoryError: Unable to allocate memory`

**è§£å†³æ–¹æ¡ˆ**:
```python
# å‡å°‘æ‰¹å¤„ç†å¤§å°
template.setup(
    batch_size=10,          # å‡å°‘æ‰¹æ¬¡å¤§å°
    chunk_size=500,         # å‡å°‘å—å¤§å°
    max_memory_usage=512    # é™åˆ¶å†…å­˜ä½¿ç”¨
)

# å¯ç”¨æµå¼å¤„ç†
template.setup(streaming=True)
```

#### 3. è¶…æ—¶é”™è¯¯

**é”™è¯¯ä¿¡æ¯**: `TimeoutError: Request timed out`

**è§£å†³æ–¹æ¡ˆ**:
```python
# å¢åŠ è¶…æ—¶æ—¶é—´
template.setup(timeout=120.0)

# å¯ç”¨é‡è¯•æœºåˆ¶
template.setup(
    retry_count=3,
    retry_delay=2.0
)

# æ£€æŸ¥ç½‘ç»œè¿æ¥
import requests
response = requests.get("https://api.openai.com/v1/models", timeout=10)
print(response.status_code)
```

#### 4. ä¾èµ–åŒ…å†²çª

**é”™è¯¯ä¿¡æ¯**: `ImportError: No module named 'xxx'`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# é‡æ–°å®‰è£…ä¾èµ–
pip install --upgrade langchain langchain-openai langchain-anthropic

# æ£€æŸ¥ç‰ˆæœ¬å…¼å®¹æ€§
pip list | grep langchain

# ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows
pip install -r requirements.txt
```

#### 5. é…ç½®æ–‡ä»¶é”™è¯¯

**é”™è¯¯ä¿¡æ¯**: `ConfigurationError: Invalid configuration`

**è§£å†³æ–¹æ¡ˆ**:
```python
# éªŒè¯é…ç½®æ–‡ä»¶è¯­æ³•
import yaml
with open("configs/development.yaml", "r") as f:
    config = yaml.safe_load(f)
    print("é…ç½®æ–‡ä»¶è¯­æ³•æ­£ç¡®")

# ä½¿ç”¨é…ç½®éªŒè¯å·¥å…·
from templates.base.config_loader import ConfigLoader
loader = ConfigLoader()
is_valid = loader.validate_config("configs/development.yaml")
print(f"é…ç½®æœ‰æ•ˆæ€§: {is_valid}")
```

### è°ƒè¯•æŠ€å·§

#### 1. å¯ç”¨è¯¦ç»†æ—¥å¿—

```python
import logging
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# å¯ç”¨æ¨¡æ¿è°ƒè¯•æ¨¡å¼
template.setup(
    debug_mode=True,
    verbose=True,
    log_intermediate_steps=True
)
```

#### 2. ä½¿ç”¨æ€§èƒ½åˆ†æå™¨

```python
from templates.evaluation.performance_eval import PerformanceEvalTemplate

perf_eval = PerformanceEvalTemplate()
perf_eval.setup(
    profiling_enabled=True,
    memory_profiling=True
)

with perf_eval.profile("template_execution"):
    result = template.run(input_data)

# æŸ¥çœ‹æ€§èƒ½æŠ¥å‘Š
print(perf_eval.get_profile_report())
```

#### 3. æ‰§è¡Œå†å²åˆ†æ

```python
# æŸ¥çœ‹æ‰§è¡Œå†å²
history = template.execution_history
for record in history[-5:]:  # æœ€è¿‘5æ¬¡æ‰§è¡Œ
    print(f"æ‰§è¡ŒID: {record['execution_id']}")
    print(f"æ‰§è¡Œæ—¶é—´: {record['execution_time']:.2f}ç§’")
    print(f"æˆåŠŸçŠ¶æ€: {record['success']}")
    if not record['success']:
        print(f"é”™è¯¯ä¿¡æ¯: {record.get('error', 'Unknown')}")
```

### æ€§èƒ½è°ƒä¼˜æŒ‡å—

#### 1. ç›‘æ§å…³é”®æŒ‡æ ‡

```python
# è·å–æ€§èƒ½æŒ‡æ ‡
metrics = template.get_metrics()
print(f"æ€»æ‰§è¡Œæ¬¡æ•°: {metrics['total_executions']}")
print(f"æˆåŠŸç‡: {metrics['success_rate']:.2%}")
print(f"å¹³å‡æ‰§è¡Œæ—¶é—´: {metrics['avg_execution_time']:.2f}ç§’")
print(f"æœ€å¤§æ‰§è¡Œæ—¶é—´: {metrics['max_execution_time']:.2f}ç§’")
```

#### 2. èµ„æºä½¿ç”¨ä¼˜åŒ–

```python
# CPUå¯†é›†å‹ä»»åŠ¡ä¼˜åŒ–
template.setup(
    max_workers=4,              # åŸºäºCPUæ ¸å¿ƒæ•°
    parallel_processing=True,
    batch_size=50
)

# å†…å­˜å¯†é›†å‹ä»»åŠ¡ä¼˜åŒ–
template.setup(
    max_memory_usage=1024,      # 1GBé™åˆ¶
    enable_streaming=True,
    chunk_processing=True
)

# I/Oå¯†é›†å‹ä»»åŠ¡ä¼˜åŒ–
template.setup(
    async_enabled=True,
    connection_pool_size=10,
    request_timeout=30.0
)
```

#### 3. ç¼“å­˜ç­–ç•¥ä¼˜åŒ–

```python
# æ™ºèƒ½ç¼“å­˜é…ç½®
template.setup(
    cache_enabled=True,
    cache_strategy="lru",       # LRUç­–ç•¥
    cache_size=1000,           # ç¼“å­˜é¡¹æ•°é‡
    cache_ttl=3600,            # 1å°æ—¶è¿‡æœŸ
    cache_compression=True     # å¯ç”¨å‹ç¼©
)

# ç¼“å­˜å‘½ä¸­ç‡ç›‘æ§
cache_stats = template.get_cache_stats()
print(f"ç¼“å­˜å‘½ä¸­ç‡: {cache_stats['hit_rate']:.2%}")
```

## ğŸ¤ è´¡çŒ®æŒ‡å—

### å¼€å‘ç¯å¢ƒè®¾ç½®

```bash
# 1. Forkå¹¶å…‹éš†ä»“åº“
git clone https://github.com/your-username/study-langraph.git
cd study-langraph

# 2. åˆ›å»ºå¼€å‘åˆ†æ”¯
git checkout -b feature/new-template

# 3. å®‰è£…å¼€å‘ä¾èµ–
pip install -r requirements-dev.txt

# 4. å®‰è£…pre-commit hooks
pre-commit install
```

### æ·»åŠ æ–°æ¨¡æ¿

1. **åˆ›å»ºæ¨¡æ¿æ–‡ä»¶**:
```python
# templates/custom/my_template.py
from templates.base.template_base import TemplateBase
from typing import Dict, Any

class MyTemplate(TemplateBase):
    def setup(self, **parameters) -> None:
        # å®ç°å‚æ•°è®¾ç½®é€»è¾‘
        pass
    
    def execute(self, input_data, **kwargs):
        # å®ç°æ ¸å¿ƒæ‰§è¡Œé€»è¾‘
        pass
    
    def get_example(self) -> Dict[str, Any]:
        # è¿”å›ä½¿ç”¨ç¤ºä¾‹
        return {
            "setup_parameters": {},
            "execute_parameters": {},
            "expected_output": {}
        }
```

2. **åˆ›å»ºé…ç½®æ–‡ä»¶**:
```yaml
# templates/configs/template_configs/my_template.yaml
name: "MyTemplate"
version: "1.0.0"
description: "æˆ‘çš„è‡ªå®šä¹‰æ¨¡æ¿"
template_type: "custom"

parameters:
  my_param:
    type: "str"
    required: true
    description: "ç¤ºä¾‹å‚æ•°"
```

3. **ç¼–å†™æµ‹è¯•**:
```python
# tests/templates/custom/test_my_template.py
import unittest
from templates.custom.my_template import MyTemplate

class TestMyTemplate(unittest.TestCase):
    def test_basic_functionality(self):
        template = MyTemplate()
        template.setup(my_param="test_value")
        result = template.run("test_input")
        self.assertIsNotNone(result)
```

4. **æ›´æ–°æ–‡æ¡£**:
```markdown
# åœ¨README.mdä¸­æ·»åŠ æ–°æ¨¡æ¿è¯´æ˜
### è‡ªå®šä¹‰æ¨¡æ¿ (`templates/custom/`)
...
```

### ä»£ç è§„èŒƒ

- **ç±»å‹æ³¨è§£**: æ‰€æœ‰å‡½æ•°éƒ½åº”æœ‰å®Œæ•´çš„ç±»å‹æ³¨è§£
- **æ–‡æ¡£å­—ç¬¦ä¸²**: ä½¿ç”¨Googleé£æ ¼çš„docstring
- **é”™è¯¯å¤„ç†**: ä½¿ç”¨é¡¹ç›®å®šä¹‰çš„å¼‚å¸¸ç±»å‹
- **æ—¥å¿—è®°å½•**: ä½¿ç”¨é¡¹ç›®çš„æ—¥å¿—ç³»ç»Ÿ
- **æµ‹è¯•è¦†ç›–**: æ–°ä»£ç åº”æœ‰>=90%çš„æµ‹è¯•è¦†ç›–ç‡

### æäº¤è§„èŒƒ

```bash
# æäº¤ä¿¡æ¯æ ¼å¼
git commit -m "feat(templates): æ·»åŠ æ–°çš„è‡ªå®šä¹‰æ¨¡æ¿

- å®ç°MyTemplateç±»
- æ·»åŠ é…ç½®æ–‡ä»¶å’Œæµ‹è¯•
- æ›´æ–°æ–‡æ¡£

closes #123"
```

**æäº¤ç±»å‹**:
- `feat`: æ–°åŠŸèƒ½
- `fix`: Bugä¿®å¤
- `docs`: æ–‡æ¡£æ›´æ–°
- `style`: ä»£ç æ ¼å¼è°ƒæ•´
- `refactor`: ä»£ç é‡æ„
- `test`: æµ‹è¯•ç›¸å…³
- `chore`: æ„å»ºè¿‡ç¨‹æˆ–è¾…åŠ©å·¥å…·çš„å˜åŠ¨

### Pull Requestæµç¨‹

1. ç¡®ä¿æ‰€æœ‰æµ‹è¯•é€šè¿‡
2. æ›´æ–°ç›¸å…³æ–‡æ¡£
3. éµå¾ªä»£ç è§„èŒƒ
4. æä¾›æ¸…æ™°çš„PRæè¿°
5. å“åº”ä»£ç å®¡æŸ¥åé¦ˆ

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨MITè®¸å¯è¯ - æŸ¥çœ‹ [LICENSE](LICENSE) æ–‡ä»¶äº†è§£è¯¦æƒ…ã€‚

## ğŸ™ è‡´è°¢

- [LangChain](https://github.com/langchain-ai/langchain) - å¼ºå¤§çš„LLMåº”ç”¨å¼€å‘æ¡†æ¶
- [OpenAI](https://openai.com/) - GPTæ¨¡å‹æä¾›å•†
- [Anthropic](https://www.anthropic.com/) - Claudeæ¨¡å‹æä¾›å•†
- æ‰€æœ‰è´¡çŒ®è€…å’Œç¤¾åŒºæˆå‘˜

## ğŸ“ æ”¯æŒä¸åé¦ˆ

- **Issueè·Ÿè¸ª**: [GitHub Issues](https://github.com/your-username/study-langraph/issues)
- **è®¨è®ºè®ºå›**: [GitHub Discussions](https://github.com/your-username/study-langraph/discussions)
- **é‚®ç®±è”ç³»**: langchain-learning@example.com

---

**å¿«é€Ÿé“¾æ¥**:
- [å®‰è£…æŒ‡å—](#å¿«é€Ÿå¼€å§‹)
- [APIæ–‡æ¡£](docs/api_reference.md)
- [æ•™ç¨‹ç¤ºä¾‹](examples/tutorials/)
- [æœ€ä½³å®è·µ](examples/best_practices/)
- [FAQ](docs/faq.md)