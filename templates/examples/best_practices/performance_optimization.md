# æ€§èƒ½ä¼˜åŒ–æœ€ä½³å®è·µæŒ‡å—

æœ¬æŒ‡å—æä¾›äº†ä¼˜åŒ–LangChain Learningæ¨¡æ¿ç³»ç»Ÿæ€§èƒ½çš„å®ç”¨å»ºè®®å’ŒæŠ€å·§ã€‚

## ğŸ¯ ä¼˜åŒ–ç›®æ ‡

- **å“åº”é€Ÿåº¦**: å‡å°‘ç”¨æˆ·ç­‰å¾…æ—¶é—´
- **èµ„æºæ•ˆç‡**: ä¼˜åŒ–å†…å­˜å’ŒCPUä½¿ç”¨
- **æˆæœ¬æ§åˆ¶**: é™ä½APIè°ƒç”¨è´¹ç”¨
- **ç³»ç»Ÿç¨³å®šæ€§**: æé«˜ç³»ç»Ÿå¯é æ€§

## ğŸš€ LLMæ€§èƒ½ä¼˜åŒ–

### 1. æ¨¡å‹é€‰æ‹©ç­–ç•¥

```python
# æ ¹æ®ä»»åŠ¡å¤æ‚åº¦é€‰æ‹©æ¨¡å‹
def choose_model_by_task(task_type: str) -> str:
    """æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©æœ€é€‚åˆçš„æ¨¡å‹"""
    
    model_mapping = {
        # ç®€å•ä»»åŠ¡ - å¿«é€Ÿä¸”ä¾¿å®œ
        "translation": "gpt-3.5-turbo",
        "summarization": "gpt-3.5-turbo", 
        "classification": "gpt-3.5-turbo",
        
        # å¤æ‚ä»»åŠ¡ - è´¨é‡ä¼˜å…ˆ
        "reasoning": "gpt-4",
        "creative_writing": "gpt-4",
        "code_generation": "gpt-4",
        
        # å¯¹è¯ä»»åŠ¡ - å¹³è¡¡æ€§èƒ½å’Œæˆæœ¬
        "chat": "gpt-3.5-turbo",
        "customer_service": "gpt-3.5-turbo"
    }
    
    return model_mapping.get(task_type, "gpt-3.5-turbo")

# ç¤ºä¾‹ä½¿ç”¨
llm_template = OpenAITemplate()
llm_template.setup(
    model_name=choose_model_by_task("translation"),
    temperature=0.3  # ç¿»è¯‘ä»»åŠ¡ä½¿ç”¨è¾ƒä½æ¸©åº¦
)
```

### 2. å‚æ•°ä¼˜åŒ–

```python
# é’ˆå¯¹ä¸åŒåœºæ™¯çš„å‚æ•°é…ç½®
class OptimizedConfigs:
    """ä¼˜åŒ–çš„é…ç½®é›†åˆ"""
    
    @staticmethod
    def fast_response_config():
        """å¿«é€Ÿå“åº”é…ç½® - ç‰ºç‰²ä¸€äº›è´¨é‡æ¢å–é€Ÿåº¦"""
        return {
            "model_name": "gpt-3.5-turbo",
            "temperature": 0.3,
            "max_tokens": 500,
            "top_p": 0.8,
            "frequency_penalty": 0.1
        }
    
    @staticmethod
    def high_quality_config():
        """é«˜è´¨é‡é…ç½® - é‡è§†è¾“å‡ºè´¨é‡"""
        return {
            "model_name": "gpt-4",
            "temperature": 0.7,
            "max_tokens": 2000,
            "top_p": 0.95,
            "frequency_penalty": 0.0
        }
    
    @staticmethod
    def cost_effective_config():
        """æˆæœ¬æ•ˆç›Šé…ç½® - å¹³è¡¡æˆæœ¬å’Œè´¨é‡"""
        return {
            "model_name": "gpt-3.5-turbo",
            "temperature": 0.5,
            "max_tokens": 1000,
            "top_p": 0.9,
            "frequency_penalty": 0.2
        }

# æ ¹æ®åœºæ™¯é€‰æ‹©é…ç½®
def setup_llm_for_scenario(scenario: str):
    llm = OpenAITemplate()
    
    if scenario == "real_time_chat":
        config = OptimizedConfigs.fast_response_config()
    elif scenario == "content_creation":
        config = OptimizedConfigs.high_quality_config()
    else:
        config = OptimizedConfigs.cost_effective_config()
    
    llm.setup(**config)
    return llm
```

### 3. ç¼“å­˜ç­–ç•¥

```python
from functools import lru_cache
import hashlib
import pickle
import time

class SmartCache:
    """æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿ"""
    
    def __init__(self, ttl=3600, max_size=1000):
        self.cache = {}
        self.ttl = ttl
        self.max_size = max_size
        self.access_times = {}
    
    def _generate_key(self, prompt: str, config: dict) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        content = f"{prompt}_{str(sorted(config.items()))}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def get(self, prompt: str, config: dict):
        """è·å–ç¼“å­˜ç»“æœ"""
        key = self._generate_key(prompt, config)
        
        if key in self.cache:
            cached_time, result = self.cache[key]
            
            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            if time.time() - cached_time < self.ttl:
                self.access_times[key] = time.time()
                return result
            else:
                # æ¸…ç†è¿‡æœŸç¼“å­˜
                del self.cache[key]
                if key in self.access_times:
                    del self.access_times[key]
        
        return None
    
    def set(self, prompt: str, config: dict, result):
        """è®¾ç½®ç¼“å­˜"""
        key = self._generate_key(prompt, config)
        
        # å¦‚æœç¼“å­˜å·²æ»¡ï¼Œåˆ é™¤æœ€ä¹…æœªè®¿é—®çš„é¡¹
        if len(self.cache) >= self.max_size:
            oldest_key = min(self.access_times, key=self.access_times.get)
            del self.cache[oldest_key]
            del self.access_times[oldest_key]
        
        self.cache[key] = (time.time(), result)
        self.access_times[key] = time.time()

# ä½¿ç”¨ç¼“å­˜çš„LLMæ¨¡æ¿
class CachedLLMTemplate(OpenAITemplate):
    """å¸¦ç¼“å­˜çš„LLMæ¨¡æ¿"""
    
    def __init__(self, cache_ttl=3600):
        super().__init__()
        self.cache = SmartCache(ttl=cache_ttl)
    
    def run(self, input_data: str, **kwargs):
        """å¸¦ç¼“å­˜çš„æ‰§è¡Œæ–¹æ³•"""
        
        # ç”Ÿæˆé…ç½®é”®
        config_key = {
            "model": self.model_name,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens
        }
        
        # å°è¯•ä»ç¼“å­˜è·å–
        cached_result = self.cache.get(input_data, config_key)
        if cached_result:
            return cached_result
        
        # æ‰§è¡ŒLLMè°ƒç”¨
        result = super().run(input_data, **kwargs)
        
        # ä¿å­˜åˆ°ç¼“å­˜
        self.cache.set(input_data, config_key, result)
        
        return result
```

### 4. æ‰¹å¤„ç†ä¼˜åŒ–

```python
import asyncio
from typing import List, Dict, Any

class BatchProcessor:
    """æ‰¹å¤„ç†å™¨ - ä¼˜åŒ–å¤šä¸ªè¯·æ±‚çš„å¤„ç†"""
    
    def __init__(self, llm_template, batch_size=5, delay=0.1):
        self.llm_template = llm_template
        self.batch_size = batch_size
        self.delay = delay  # è¯·æ±‚é—´å»¶è¿Ÿï¼Œé¿å…é€Ÿç‡é™åˆ¶
    
    async def process_batch(self, prompts: List[str]) -> List[Dict[str, Any]]:
        """å¼‚æ­¥æ‰¹å¤„ç†"""
        results = []
        
        for i in range(0, len(prompts), self.batch_size):
            batch = prompts[i:i + self.batch_size]
            
            # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
            tasks = []
            for prompt in batch:
                task = self._process_single(prompt)
                tasks.append(task)
            
            # ç­‰å¾…æ‰¹æ¬¡å®Œæˆ
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # å¤„ç†ç»“æœ
            for j, result in enumerate(batch_results):
                if isinstance(result, Exception):
                    results.append({
                        "input": batch[j],
                        "success": False,
                        "error": str(result)
                    })
                else:
                    results.append({
                        "input": batch[j],
                        "success": True,
                        "output": result
                    })
            
            # æ·»åŠ å»¶è¿Ÿé¿å…é€Ÿç‡é™åˆ¶
            if i + self.batch_size < len(prompts):
                await asyncio.sleep(self.delay)
        
        return results
    
    async def _process_single(self, prompt: str):
        """å¤„ç†å•ä¸ªè¯·æ±‚"""
        return await self.llm_template.run_async(prompt)

# ä½¿ç”¨ç¤ºä¾‹
async def batch_processing_example():
    llm = OpenAITemplate()
    llm.setup(model_name="gpt-3.5-turbo", async_enabled=True)
    
    processor = BatchProcessor(llm, batch_size=3)
    
    prompts = [
        "ä»‹ç»Pythonçš„ç‰¹ç‚¹",
        "è§£é‡Šæœºå™¨å­¦ä¹ æ¦‚å¿µ", 
        "æè¿°äº‘è®¡ç®—ä¼˜åŠ¿",
        "åˆ†æäººå·¥æ™ºèƒ½å‘å±•",
        "è®¨è®ºåŒºå—é“¾æŠ€æœ¯"
    ]
    
    results = await processor.process_batch(prompts)
    
    for result in results:
        if result["success"]:
            print(f"âœ… {result['input'][:20]}... -> æˆåŠŸ")
        else:
            print(f"âŒ {result['input'][:20]}... -> {result['error']}")
```

## ğŸ“Š æ•°æ®å¤„ç†ä¼˜åŒ–

### 1. æ–‡æ¡£åŠ è½½ä¼˜åŒ–

```python
class OptimizedDocumentLoader:
    """ä¼˜åŒ–çš„æ–‡æ¡£åŠ è½½å™¨"""
    
    def __init__(self):
        self.file_size_limits = {
            "txt": 50 * 1024 * 1024,   # 50MB
            "pdf": 20 * 1024 * 1024,   # 20MB
            "docx": 10 * 1024 * 1024   # 10MB
        }
    
    def should_process_file(self, file_path: str) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åº”è¯¥å¤„ç†"""
        import os
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(file_path)
        file_ext = os.path.splitext(file_path)[1][1:].lower()
        
        max_size = self.file_size_limits.get(file_ext, 5 * 1024 * 1024)
        
        if file_size > max_size:
            print(f"âš ï¸ è·³è¿‡å¤§æ–‡ä»¶: {file_path} ({file_size / 1024 / 1024:.1f}MB)")
            return False
        
        return True
    
    def load_with_streaming(self, file_path: str, chunk_size=8192):
        """æµå¼åŠ è½½å¤§æ–‡ä»¶"""
        content = ""
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                while True:
                    chunk = f.read(chunk_size)
                    if not chunk:
                        break
                    content += chunk
                    
                    # å®šæœŸè®©å‡ºæ§åˆ¶æƒ
                    if len(content) % (chunk_size * 10) == 0:
                        time.sleep(0.001)  # 1mså»¶è¿Ÿ
                        
        except Exception as e:
            print(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥: {file_path} - {str(e)}")
            return None
        
        return content

# å¹¶è¡Œæ–‡æ¡£å¤„ç†
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor

def process_document_parallel(file_paths: List[str], max_workers=None):
    """å¹¶è¡Œå¤„ç†å¤šä¸ªæ–‡æ¡£"""
    
    if max_workers is None:
        max_workers = min(mp.cpu_count(), len(file_paths))
    
    results = []
    
    # ä½¿ç”¨è¿›ç¨‹æ± å¤„ç†CPUå¯†é›†å‹ä»»åŠ¡
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        future_to_file = {
            executor.submit(load_and_process_file, file_path): file_path 
            for file_path in file_paths
        }
        
        for future in future_to_file:
            file_path = future_to_file[future]
            try:
                result = future.result(timeout=60)  # 60ç§’è¶…æ—¶
                results.append(result)
            except Exception as e:
                print(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥: {file_path} - {str(e)}")
    
    return results

def load_and_process_file(file_path: str):
    """å•ä¸ªæ–‡ä»¶å¤„ç†å‡½æ•°ï¼ˆåœ¨å­è¿›ç¨‹ä¸­è¿è¡Œï¼‰"""
    loader = OptimizedDocumentLoader()
    
    if not loader.should_process_file(file_path):
        return None
    
    content = loader.load_with_streaming(file_path)
    
    if content:
        # åŸºç¡€é¢„å¤„ç†
        content = content.strip()
        content = ' '.join(content.split())  # æ ‡å‡†åŒ–ç©ºç™½å­—ç¬¦
        
        return {
            "file_path": file_path,
            "content": content,
            "size": len(content)
        }
    
    return None
```

### 2. å‘é‡åŒ–ä¼˜åŒ–

```python
class OptimizedVectorStore:
    """ä¼˜åŒ–çš„å‘é‡å­˜å‚¨"""
    
    def __init__(self):
        self.batch_size = 100
        self.embedding_cache = {}
    
    def embed_texts_efficiently(self, texts: List[str], embedding_model):
        """é«˜æ•ˆçš„æ–‡æœ¬åµŒå…¥"""
        embeddings = []
        
        # æ£€æŸ¥ç¼“å­˜
        cached_embeddings = {}
        texts_to_embed = []
        
        for i, text in enumerate(texts):
            text_hash = hashlib.md5(text.encode()).hexdigest()
            if text_hash in self.embedding_cache:
                cached_embeddings[i] = self.embedding_cache[text_hash]
            else:
                texts_to_embed.append((i, text, text_hash))
        
        # æ‰¹é‡å¤„ç†æœªç¼“å­˜çš„æ–‡æœ¬
        if texts_to_embed:
            batch_texts = [item[1] for item in texts_to_embed]
            
            # åˆ†æ‰¹å¤„ç†
            for i in range(0, len(batch_texts), self.batch_size):
                batch = batch_texts[i:i + self.batch_size]
                
                try:
                    batch_embeddings = embedding_model.embed_documents(batch)
                    
                    # æ›´æ–°ç¼“å­˜å’Œç»“æœ
                    for j, embedding in enumerate(batch_embeddings):
                        original_index = texts_to_embed[i + j][0]
                        text_hash = texts_to_embed[i + j][2]
                        
                        embeddings.append((original_index, embedding))
                        self.embedding_cache[text_hash] = embedding
                        
                except Exception as e:
                    print(f"âŒ åµŒå…¥æ‰¹æ¬¡å¤±è´¥: {str(e)}")
        
        # åˆå¹¶ç¼“å­˜å’Œæ–°è®¡ç®—çš„åµŒå…¥
        final_embeddings = [None] * len(texts)
        
        for index, embedding in cached_embeddings.items():
            final_embeddings[index] = embedding
        
        for index, embedding in embeddings:
            final_embeddings[index] = embedding
        
        return final_embeddings
    
    def optimize_index(self, vectorstore, force_rebuild=False):
        """ä¼˜åŒ–å‘é‡ç´¢å¼•"""
        
        # æ£€æŸ¥ç´¢å¼•å¥åº·çŠ¶å†µ
        stats = vectorstore.get_statistics()
        
        if force_rebuild or self._should_rebuild_index(stats):
            print("ğŸ”„ é‡å»ºå‘é‡ç´¢å¼•ä»¥ä¼˜åŒ–æ€§èƒ½...")
            
            # å¤‡ä»½ç°æœ‰æ•°æ®
            backup_data = vectorstore.export_data()
            
            # é‡å»ºç´¢å¼•
            vectorstore.rebuild_index(optimize=True)
            
            print("âœ… ç´¢å¼•é‡å»ºå®Œæˆ")
        
        return vectorstore
    
    def _should_rebuild_index(self, stats: dict) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦é‡å»ºç´¢å¼•"""
        
        # åˆ é™¤æ¯”ä¾‹è¿‡é«˜
        if stats.get("deleted_ratio", 0) > 0.3:
            return True
        
        # ç´¢å¼•ç¢ç‰‡åŒ–ä¸¥é‡
        if stats.get("fragmentation", 0) > 0.5:
            return True
        
        # æŸ¥è¯¢æ€§èƒ½ä¸‹é™
        if stats.get("avg_query_time", 0) > 1.0:  # è¶…è¿‡1ç§’
            return True
        
        return False
```

### 3. å†…å­˜ç®¡ç†ä¼˜åŒ–

```python
import gc
import psutil
import threading
from typing import Optional

class MemoryManager:
    """å†…å­˜ç®¡ç†å™¨"""
    
    def __init__(self, max_memory_mb=2048):
        self.max_memory_mb = max_memory_mb
        self.monitoring = False
        self.monitor_thread: Optional[threading.Thread] = None
    
    def get_memory_usage(self) -> float:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡(MB)"""
        process = psutil.Process()
        return process.memory_info().rss / 1024 / 1024
    
    def start_monitoring(self, check_interval=10):
        """å¼€å§‹å†…å­˜ç›‘æ§"""
        self.monitoring = True
        self.monitor_thread = threading.Thread(
            target=self._monitor_memory,
            args=(check_interval,),
            daemon=True
        )
        self.monitor_thread.start()
    
    def stop_monitoring(self):
        """åœæ­¢å†…å­˜ç›‘æ§"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join()
    
    def _monitor_memory(self, check_interval: int):
        """å†…å­˜ç›‘æ§å¾ªç¯"""
        while self.monitoring:
            usage = self.get_memory_usage()
            
            if usage > self.max_memory_mb * 0.8:  # 80%è­¦å‘Š
                print(f"âš ï¸ å†…å­˜ä½¿ç”¨ç‡é«˜: {usage:.1f}MB / {self.max_memory_mb}MB")
                self._cleanup_memory()
            
            if usage > self.max_memory_mb:  # è¶…å‡ºé™åˆ¶
                print(f"ğŸš¨ å†…å­˜è¶…å‡ºé™åˆ¶: {usage:.1f}MB / {self.max_memory_mb}MB")
                self._force_cleanup()
            
            time.sleep(check_interval)
    
    def _cleanup_memory(self):
        """æ¸…ç†å†…å­˜"""
        print("ğŸ§¹ æ‰§è¡Œå†…å­˜æ¸…ç†...")
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        collected = gc.collect()
        print(f"ğŸ—‘ï¸ å›æ”¶äº† {collected} ä¸ªå¯¹è±¡")
        
        # æ¸…ç†å…¨å±€ç¼“å­˜
        if hasattr(self, 'global_cache'):
            self.global_cache.clear()
    
    def _force_cleanup(self):
        """å¼ºåˆ¶å†…å­˜æ¸…ç†"""
        self._cleanup_memory()
        
        # æ›´æ¿€è¿›çš„æ¸…ç†ç­–ç•¥
        import ctypes
        ctypes.CDLL("libc.so.6").malloc_trim(0)  # Linux only

# ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨è‡ªåŠ¨ç®¡ç†å†…å­˜
class MemoryAwareTemplate:
    """å†…å­˜æ„ŸçŸ¥çš„æ¨¡æ¿åŸºç±»"""
    
    def __init__(self, max_memory_mb=1024):
        self.memory_manager = MemoryManager(max_memory_mb)
    
    def __enter__(self):
        self.memory_manager.start_monitoring()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.memory_manager.stop_monitoring()
        self.memory_manager._cleanup_memory()

# ä½¿ç”¨ç¤ºä¾‹
with MemoryAwareTemplate(max_memory_mb=512) as template:
    # åœ¨å†…å­˜ç›‘æ§ä¸‹æ‰§è¡Œæ“ä½œ
    result = template.process_large_dataset()
```

## ğŸ”„ ç³»ç»Ÿçº§ä¼˜åŒ–

### 1. è¿æ¥æ± ç®¡ç†

```python
import aiohttp
from urllib3.util.retry import Retry
from requests.adapters import HTTPAdapter
import requests

class OptimizedHTTPClient:
    """ä¼˜åŒ–çš„HTTPå®¢æˆ·ç«¯"""
    
    def __init__(self, max_connections=100, max_connections_per_host=20):
        # é…ç½®é‡è¯•ç­–ç•¥
        retry_strategy = Retry(
            total=3,
            backoff_factor=0.5,
            status_forcelist=[429, 500, 502, 503, 504]
        )
        
        # é…ç½®è¿æ¥é€‚é…å™¨
        adapter = HTTPAdapter(
            max_retries=retry_strategy,
            pool_connections=max_connections_per_host,
            pool_maxsize=max_connections
        )
        
        # åˆ›å»ºä¼šè¯
        self.session = requests.Session()
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
        # è®¾ç½®é»˜è®¤è¶…æ—¶
        self.session.timeout = (5, 30)  # è¿æ¥è¶…æ—¶5ç§’ï¼Œè¯»å–è¶…æ—¶30ç§’
    
    async def create_async_session(self):
        """åˆ›å»ºå¼‚æ­¥ä¼šè¯"""
        connector = aiohttp.TCPConnector(
            limit=100,          # æ€»è¿æ¥æ•°é™åˆ¶
            limit_per_host=20,  # æ¯ä¸ªä¸»æœºè¿æ¥æ•°é™åˆ¶
            ttl_dns_cache=300,  # DNSç¼“å­˜5åˆ†é’Ÿ
            use_dns_cache=True,
            keepalive_timeout=30,
            enable_cleanup_closed=True
        )
        
        timeout = aiohttp.ClientTimeout(
            total=30,           # æ€»è¶…æ—¶
            connect=5           # è¿æ¥è¶…æ—¶
        )
        
        return aiohttp.ClientSession(
            connector=connector,
            timeout=timeout
        )

# LLMæ¨¡æ¿çš„HTTPä¼˜åŒ–
class OptimizedOpenAITemplate(OpenAITemplate):
    """ä¼˜åŒ–çš„OpenAIæ¨¡æ¿"""
    
    def __init__(self):
        super().__init__()
        self.http_client = OptimizedHTTPClient()
    
    def setup(self, **parameters):
        super().setup(**parameters)
        
        # ä½¿ç”¨ä¼˜åŒ–çš„HTTPå®¢æˆ·ç«¯
        if hasattr(self.llm, '_client'):
            self.llm._client.session = self.http_client.session
```

### 2. å¼‚æ­¥ä¼˜åŒ–

```python
import asyncio
from typing import List, Coroutine, Any
import aiofiles

class AsyncOptimizer:
    """å¼‚æ­¥æ“ä½œä¼˜åŒ–å™¨"""
    
    def __init__(self, max_concurrent=10):
        self.max_concurrent = max_concurrent
        self.semaphore = asyncio.Semaphore(max_concurrent)
    
    async def run_with_semaphore(self, coro: Coroutine) -> Any:
        """ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘"""
        async with self.semaphore:
            return await coro
    
    async def batch_execute(self, coroutines: List[Coroutine]) -> List[Any]:
        """æ‰¹é‡æ‰§è¡Œåç¨‹"""
        
        # åˆ›å»ºå—é™åˆ¶çš„ä»»åŠ¡
        limited_coroutines = [
            self.run_with_semaphore(coro) 
            for coro in coroutines
        ]
        
        # ä½¿ç”¨gatheræ‰§è¡Œï¼Œä½†å¤„ç†å¼‚å¸¸
        results = await asyncio.gather(*limited_coroutines, return_exceptions=True)
        
        # åˆ†ç¦»æˆåŠŸå’Œå¤±è´¥çš„ç»“æœ
        successes = []
        failures = []
        
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                failures.append((i, result))
            else:
                successes.append((i, result))
        
        if failures:
            print(f"âš ï¸ {len(failures)} ä¸ªä»»åŠ¡å¤±è´¥")
            for i, error in failures:
                print(f"   ä»»åŠ¡ {i}: {str(error)}")
        
        return results
    
    async def progressive_execution(self, coroutines: List[Coroutine], 
                                  batch_size=5, delay=1.0) -> List[Any]:
        """æ¸è¿›å¼æ‰§è¡Œ - é¿å…çªå‘è´Ÿè½½"""
        
        all_results = []
        
        for i in range(0, len(coroutines), batch_size):
            batch = coroutines[i:i + batch_size]
            
            print(f"ğŸš€ æ‰§è¡Œæ‰¹æ¬¡ {i//batch_size + 1}/{(len(coroutines)-1)//batch_size + 1}")
            
            batch_results = await self.batch_execute(batch)
            all_results.extend(batch_results)
            
            # æ‰¹æ¬¡é—´å»¶è¿Ÿ
            if i + batch_size < len(coroutines):
                await asyncio.sleep(delay)
        
        return all_results

# å¼‚æ­¥æ–‡ä»¶I/Oä¼˜åŒ–
class AsyncFileHandler:
    """å¼‚æ­¥æ–‡ä»¶å¤„ç†å™¨"""
    
    @staticmethod
    async def read_files_async(file_paths: List[str]) -> List[str]:
        """å¼‚æ­¥è¯»å–å¤šä¸ªæ–‡ä»¶"""
        
        async def read_single_file(file_path: str) -> str:
            try:
                async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                    return await f.read()
            except Exception as e:
                print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {file_path} - {str(e)}")
                return ""
        
        tasks = [read_single_file(path) for path in file_paths]
        contents = await asyncio.gather(*tasks)
        
        return contents
    
    @staticmethod
    async def write_files_async(file_data: List[tuple]) -> None:
        """å¼‚æ­¥å†™å…¥å¤šä¸ªæ–‡ä»¶"""
        
        async def write_single_file(file_path: str, content: str) -> None:
            try:
                async with aiofiles.open(file_path, 'w', encoding='utf-8') as f:
                    await f.write(content)
            except Exception as e:
                print(f"âŒ å†™å…¥æ–‡ä»¶å¤±è´¥: {file_path} - {str(e)}")
        
        tasks = [write_single_file(path, content) for path, content in file_data]
        await asyncio.gather(*tasks)
```

## ğŸ“ˆ ç›‘æ§å’Œåˆ†æ

### 1. æ€§èƒ½æŒ‡æ ‡æ”¶é›†

```python
import time
import threading
from collections import defaultdict, deque
from dataclasses import dataclass
from typing import Dict, List, Optional

@dataclass
class PerformanceMetric:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""
    timestamp: float
    operation: str
    duration: float
    success: bool
    memory_usage: float
    error: Optional[str] = None

class PerformanceCollector:
    """æ€§èƒ½æŒ‡æ ‡æ”¶é›†å™¨"""
    
    def __init__(self, max_history=1000):
        self.metrics: deque = deque(maxlen=max_history)
        self.operation_stats: Dict[str, List[float]] = defaultdict(list)
        self.lock = threading.Lock()
    
    def record_metric(self, operation: str, duration: float, 
                     success: bool, memory_usage: float, error: str = None):
        """è®°å½•æ€§èƒ½æŒ‡æ ‡"""
        
        metric = PerformanceMetric(
            timestamp=time.time(),
            operation=operation,
            duration=duration,
            success=success,
            memory_usage=memory_usage,
            error=error
        )
        
        with self.lock:
            self.metrics.append(metric)
            if success:
                self.operation_stats[operation].append(duration)
    
    def get_statistics(self, operation: str = None) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        
        with self.lock:
            if operation:
                durations = self.operation_stats[operation]
            else:
                durations = [m.duration for m in self.metrics if m.success]
        
        if not durations:
            return {"count": 0}
        
        durations.sort()
        n = len(durations)
        
        return {
            "count": n,
            "avg_duration": sum(durations) / n,
            "min_duration": durations[0],
            "max_duration": durations[-1],
            "p50_duration": durations[n // 2],
            "p95_duration": durations[int(n * 0.95)],
            "p99_duration": durations[int(n * 0.99)]
        }
    
    def get_error_analysis(self) -> Dict[str, int]:
        """è·å–é”™è¯¯åˆ†æ"""
        
        error_counts = defaultdict(int)
        
        with self.lock:
            for metric in self.metrics:
                if not metric.success and metric.error:
                    error_counts[metric.error] += 1
        
        return dict(error_counts)

# æ€§èƒ½ç›‘æ§è£…é¥°å™¨
def monitor_performance(collector: PerformanceCollector, operation_name: str):
    """æ€§èƒ½ç›‘æ§è£…é¥°å™¨"""
    
    def decorator(func):
        def wrapper(*args, **kwargs):
            start_time = time.time()
            start_memory = psutil.Process().memory_info().rss / 1024 / 1024
            
            try:
                result = func(*args, **kwargs)
                success = True
                error = None
            except Exception as e:
                result = None
                success = False
                error = str(e)
                raise
            finally:
                end_time = time.time()
                end_memory = psutil.Process().memory_info().rss / 1024 / 1024
                
                duration = end_time - start_time
                memory_usage = end_memory - start_memory
                
                collector.record_metric(
                    operation=operation_name,
                    duration=duration,
                    success=success,
                    memory_usage=memory_usage,
                    error=error
                )
            
            return result
        return wrapper
    return decorator

# ä½¿ç”¨ç¤ºä¾‹
performance_collector = PerformanceCollector()

@monitor_performance(performance_collector, "llm_call")
def monitored_llm_call(template, prompt):
    """è¢«ç›‘æ§çš„LLMè°ƒç”¨"""
    return template.run(prompt)
```

### 2. å®æ—¶ç›‘æ§é¢æ¿

```python
import json
import datetime
from typing import Dict, Any

class MonitoringDashboard:
    """ç›‘æ§é¢æ¿"""
    
    def __init__(self, collector: PerformanceCollector):
        self.collector = collector
    
    def generate_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆç›‘æ§æŠ¥å‘Š"""
        
        # æ€»ä½“ç»Ÿè®¡
        overall_stats = self.collector.get_statistics()
        
        # æ“ä½œçº§åˆ«ç»Ÿè®¡
        operation_stats = {}
        for operation in self.collector.operation_stats.keys():
            operation_stats[operation] = self.collector.get_statistics(operation)
        
        # é”™è¯¯åˆ†æ
        error_analysis = self.collector.get_error_analysis()
        
        # æœ€è¿‘è¶‹åŠ¿ï¼ˆæœ€è¿‘100ä¸ªæŒ‡æ ‡ï¼‰
        recent_metrics = list(self.collector.metrics)[-100:]
        recent_durations = [m.duration for m in recent_metrics if m.success]
        
        # æˆåŠŸç‡
        total_count = len(recent_metrics)
        success_count = sum(1 for m in recent_metrics if m.success)
        success_rate = success_count / total_count if total_count > 0 else 0
        
        return {
            "timestamp": datetime.datetime.now().isoformat(),
            "overall_statistics": overall_stats,
            "operation_statistics": operation_stats,
            "error_analysis": error_analysis,
            "recent_trend": {
                "success_rate": success_rate,
                "avg_duration_recent": sum(recent_durations) / len(recent_durations) if recent_durations else 0,
                "total_requests": total_count
            }
        }
    
    def export_metrics(self, file_path: str):
        """å¯¼å‡ºæŒ‡æ ‡åˆ°æ–‡ä»¶"""
        
        report = self.generate_report()
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š ç›‘æ§æŠ¥å‘Šå·²å¯¼å‡ºåˆ°: {file_path}")
    
    def print_summary(self):
        """æ‰“å°ç›‘æ§æ‘˜è¦"""
        
        report = self.generate_report()
        
        print("\n" + "="*50)
        print("ğŸ“Š æ€§èƒ½ç›‘æ§æ‘˜è¦")
        print("="*50)
        
        overall = report["overall_statistics"]
        if overall["count"] > 0:
            print(f"æ€»è¯·æ±‚æ•°: {overall['count']}")
            print(f"å¹³å‡è€—æ—¶: {overall['avg_duration']:.3f}ç§’")
            print(f"P95å»¶è¿Ÿ: {overall['p95_duration']:.3f}ç§’")
            print(f"P99å»¶è¿Ÿ: {overall['p99_duration']:.3f}ç§’")
        
        recent = report["recent_trend"]
        print(f"æˆåŠŸç‡: {recent['success_rate']:.2%}")
        print(f"æœ€è¿‘å¹³å‡è€—æ—¶: {recent['avg_duration_recent']:.3f}ç§’")
        
        if report["error_analysis"]:
            print("\nâŒ é”™è¯¯ç»Ÿè®¡:")
            for error, count in report["error_analysis"].items():
                print(f"   {error}: {count}æ¬¡")
        
        print("\nğŸ“ˆ å„æ“ä½œç»Ÿè®¡:")
        for operation, stats in report["operation_statistics"].items():
            if stats["count"] > 0:
                print(f"   {operation}: {stats['count']}æ¬¡, "
                      f"å¹³å‡{stats['avg_duration']:.3f}ç§’")
```

## ğŸ”§ é…ç½®ä¼˜åŒ–

### 1. ç¯å¢ƒç‰¹å®šä¼˜åŒ–

```python
# config_optimizer.py
import os
from typing import Dict, Any

class ConfigOptimizer:
    """é…ç½®ä¼˜åŒ–å™¨"""
    
    @staticmethod
    def get_optimized_config(environment: str = "development") -> Dict[str, Any]:
        """è·å–ä¼˜åŒ–çš„é…ç½®"""
        
        base_config = {
            "llm": {
                "temperature": 0.7,
                "max_tokens": 1000,
                "timeout": 30.0
            },
            "data": {
                "chunk_size": 1000,
                "batch_size": 10
            },
            "cache": {
                "enabled": True,
                "ttl": 3600
            }
        }
        
        if environment == "development":
            return ConfigOptimizer._optimize_for_development(base_config)
        elif environment == "production":
            return ConfigOptimizer._optimize_for_production(base_config)
        elif environment == "testing":
            return ConfigOptimizer._optimize_for_testing(base_config)
        else:
            return base_config
    
    @staticmethod
    def _optimize_for_development(config: Dict[str, Any]) -> Dict[str, Any]:
        """å¼€å‘ç¯å¢ƒä¼˜åŒ–"""
        
        # ä¼˜å…ˆè°ƒè¯•å’Œå¿«é€Ÿè¿­ä»£
        config["llm"]["model_name"] = "gpt-3.5-turbo"  # ä¾¿å®œçš„æ¨¡å‹
        config["llm"]["max_tokens"] = 500              # å‡å°‘tokenä½¿ç”¨
        config["data"]["chunk_size"] = 500             # å°å—ä¾¿äºè°ƒè¯•
        config["data"]["batch_size"] = 5               # å°æ‰¹æ¬¡å‡å°‘ç­‰å¾…
        config["cache"]["enabled"] = False             # ç¦ç”¨ç¼“å­˜ç¡®ä¿æ•°æ®æ–°é²œ
        
        # æ·»åŠ è°ƒè¯•é€‰é¡¹
        config["debug"] = {
            "verbose": True,
            "log_level": "DEBUG",
            "save_intermediate_results": True
        }
        
        return config
    
    @staticmethod
    def _optimize_for_production(config: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿäº§ç¯å¢ƒä¼˜åŒ–"""
        
        # ä¼˜å…ˆæ€§èƒ½å’Œç¨³å®šæ€§
        config["llm"]["model_name"] = "gpt-4"          # é«˜è´¨é‡æ¨¡å‹
        config["llm"]["max_tokens"] = 2000             # æ›´å¤šè¾“å‡º
        config["llm"]["timeout"] = 60.0                # æ›´é•¿è¶…æ—¶
        config["data"]["chunk_size"] = 1500            # å¤§å—æé«˜æ•ˆç‡
        config["data"]["batch_size"] = 20              # å¤§æ‰¹æ¬¡æé«˜åå
        config["cache"]["enabled"] = True              # å¯ç”¨ç¼“å­˜
        config["cache"]["ttl"] = 7200                  # æ›´é•¿ç¼“å­˜æ—¶é—´
        
        # æ·»åŠ ç”Ÿäº§é€‰é¡¹
        config["production"] = {
            "monitoring_enabled": True,
            "error_reporting": True,
            "performance_logging": True,
            "rate_limiting": True
        }
        
        return config
    
    @staticmethod
    def _optimize_for_testing(config: Dict[str, Any]) -> Dict[str, Any]:
        """æµ‹è¯•ç¯å¢ƒä¼˜åŒ–"""
        
        # ä¼˜å…ˆé€Ÿåº¦å’Œå¯é¢„æµ‹æ€§
        config["llm"]["model_name"] = "gpt-3.5-turbo"
        config["llm"]["temperature"] = 0.1             # ä½éšæœºæ€§
        config["llm"]["max_tokens"] = 200              # å¿«é€Ÿå“åº”
        config["data"]["chunk_size"] = 300             # å°å—å¿«é€Ÿå¤„ç†
        config["data"]["batch_size"] = 3               # å°æ‰¹æ¬¡
        config["cache"]["enabled"] = False             # ç¦ç”¨ç¼“å­˜ç¡®ä¿ä¸€è‡´æ€§
        
        # æ·»åŠ æµ‹è¯•é€‰é¡¹
        config["testing"] = {
            "mock_api_calls": True,
            "deterministic_results": True,
            "fast_mode": True
        }
        
        return config

# è‡ªåŠ¨é…ç½®è°ƒä¼˜
class AutoTuner:
    """è‡ªåŠ¨é…ç½®è°ƒä¼˜å™¨"""
    
    def __init__(self, collector: PerformanceCollector):
        self.collector = collector
    
    def suggest_optimizations(self) -> Dict[str, str]:
        """åŸºäºæ€§èƒ½æ•°æ®å»ºè®®ä¼˜åŒ–"""
        
        suggestions = []
        stats = self.collector.get_statistics()
        
        if not stats or stats["count"] < 10:
            return {"message": "æ•°æ®ä¸è¶³ï¼Œéœ€è¦æ›´å¤šæ€§èƒ½æ•°æ®"}
        
        # åˆ†æå“åº”æ—¶é—´
        if stats["avg_duration"] > 5.0:
            suggestions.append("ğŸŒ å¹³å‡å“åº”æ—¶é—´è¿‡é•¿ï¼Œå»ºè®®ï¼š")
            suggestions.append("   - ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹(gpt-3.5-turbo)")
            suggestions.append("   - å‡å°‘max_tokenså‚æ•°")
            suggestions.append("   - å¯ç”¨ç¼“å­˜")
        
        if stats["p95_duration"] > 10.0:
            suggestions.append("â° P95å»¶è¿Ÿè¿‡é«˜ï¼Œå»ºè®®ï¼š")
            suggestions.append("   - ä¼˜åŒ–ç½‘ç»œè¿æ¥")
            suggestions.append("   - å¢åŠ è¶…æ—¶é‡è¯•")
            suggestions.append("   - è€ƒè™‘ä½¿ç”¨CDN")
        
        # åˆ†æé”™è¯¯ç‡
        error_analysis = self.collector.get_error_analysis()
        total_errors = sum(error_analysis.values())
        error_rate = total_errors / stats["count"]
        
        if error_rate > 0.05:  # 5%é”™è¯¯ç‡
            suggestions.append("âŒ é”™è¯¯ç‡è¿‡é«˜ï¼Œå»ºè®®ï¼š")
            suggestions.append("   - å¢åŠ é‡è¯•æœºåˆ¶")
            suggestions.append("   - æ”¹å–„é”™è¯¯å¤„ç†")
            suggestions.append("   - æ£€æŸ¥APIé…é¢")
        
        return {
            "suggestions": suggestions,
            "current_stats": stats,
            "error_analysis": error_analysis
        }
```

## ğŸ“‹ ä¼˜åŒ–æ£€æŸ¥æ¸…å•

### ğŸ” æ€§èƒ½å®¡æŸ¥æ¸…å•

ä½¿ç”¨ä»¥ä¸‹æ¸…å•å®šæœŸå®¡æŸ¥ä½ çš„ç³»ç»Ÿæ€§èƒ½ï¼š

```markdown
## LLMä¼˜åŒ–æ£€æŸ¥

- [ ] æ ¹æ®ä»»åŠ¡é€‰æ‹©åˆé€‚çš„æ¨¡å‹
- [ ] ä¼˜åŒ–temperatureå’Œmax_tokenså‚æ•°
- [ ] å®ç°æ™ºèƒ½ç¼“å­˜ç­–ç•¥
- [ ] ä½¿ç”¨æ‰¹å¤„ç†å‡å°‘APIè°ƒç”¨
- [ ] å¯ç”¨å¼‚æ­¥å¤„ç†ï¼ˆå¦‚é€‚ç”¨ï¼‰
- [ ] ç›‘æ§APIä½¿ç”¨é‡å’Œæˆæœ¬
- [ ] å®ç°é‡è¯•å’Œé”™è¯¯å¤„ç†

## æ•°æ®å¤„ç†ä¼˜åŒ–æ£€æŸ¥

- [ ] è®¾ç½®åˆç†çš„æ–‡ä»¶å¤§å°é™åˆ¶
- [ ] ä¼˜åŒ–æ–‡æœ¬åˆ†å‰²å‚æ•°
- [ ] ä½¿ç”¨å¹¶è¡Œå¤„ç†å¤§æ‰¹é‡æ•°æ®
- [ ] å®šæœŸä¼˜åŒ–å‘é‡ç´¢å¼•
- [ ] ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ
- [ ] å®ç°æµå¼å¤„ç†ï¼ˆå¦‚éœ€è¦ï¼‰

## ç³»ç»Ÿçº§ä¼˜åŒ–æ£€æŸ¥

- [ ] é…ç½®è¿æ¥æ± å’Œé‡è¯•ç­–ç•¥
- [ ] ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢
- [ ] ä½¿ç”¨CDNåŠ é€Ÿï¼ˆå¦‚é€‚ç”¨ï¼‰
- [ ] å®ç°è´Ÿè½½å‡è¡¡ï¼ˆå¦‚éœ€è¦ï¼‰
- [ ] ç›‘æ§ç³»ç»Ÿèµ„æºä½¿ç”¨
- [ ] è®¾ç½®æ€§èƒ½å‘Šè­¦

## ç›‘æ§å’Œåˆ†ææ£€æŸ¥

- [ ] æ”¶é›†å…³é”®æ€§èƒ½æŒ‡æ ‡
- [ ] è®¾ç½®ç›‘æ§é¢æ¿
- [ ] å®šæœŸåˆ†ææ€§èƒ½è¶‹åŠ¿
- [ ] å®ç°é”™è¯¯è¿½è¸ª
- [ ] å»ºç«‹æ€§èƒ½åŸºçº¿
- [ ] å®šæœŸæ€§èƒ½å›é¡¾
```

é€šè¿‡éµå¾ªè¿™äº›æœ€ä½³å®è·µå’Œä½¿ç”¨æä¾›çš„ä¼˜åŒ–å·¥å…·ï¼Œä½ å¯ä»¥æ˜¾è‘—æå‡LangChain Learningæ¨¡æ¿ç³»ç»Ÿçš„æ€§èƒ½å’Œæ•ˆç‡ã€‚è®°ä½ï¼Œä¼˜åŒ–æ˜¯ä¸€ä¸ªæŒç»­çš„è¿‡ç¨‹ï¼Œéœ€è¦æ ¹æ®å®é™…ä½¿ç”¨æƒ…å†µä¸æ–­è°ƒæ•´å’Œæ”¹è¿›ã€‚