#!/usr/bin/env python3
"""
LLMæ¨¡æ¿ä½¿ç”¨æ¼”ç¤ºè„šæœ¬

æœ¬è„šæœ¬æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨å„ç§LLMæ¨¡æ¿ï¼ŒåŒ…æ‹¬ï¼š
1. OpenAIæ¨¡æ¿ - GPTç³»åˆ—æ¨¡å‹
2. Anthropicæ¨¡æ¿ - Claudeç³»åˆ—æ¨¡å‹
3. æœ¬åœ°LLMæ¨¡æ¿ - Ollama/LlamaCppç­‰
4. å¤šæ¨¡å‹æ¨¡æ¿ - æ¨¡å‹å¯¹æ¯”å’Œåˆ‡æ¢

è¿è¡Œè¦æ±‚ï¼š
- è®¾ç½®ç›¸åº”çš„APIå¯†é’¥ç¯å¢ƒå˜é‡
- å®‰è£…å¿…è¦çš„ä¾èµ–åŒ…
- å¯¹äºæœ¬åœ°æ¨¡å‹ï¼Œç¡®ä¿OllamaæœåŠ¡è¿è¡Œ

ä½¿ç”¨æ–¹æ³•ï¼š
    python llm_templates_demo.py [--demo {openai,anthropic,local,multi,all}]
"""

import os
import sys
import asyncio
import argparse
import time
from typing import Dict, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

# å¯¼å…¥LLMæ¨¡æ¿
from templates.llm import OpenAITemplate, AnthropicTemplate, LocalLLMTemplate, MultiModelTemplate


class LLMTemplateDemo:
    """LLMæ¨¡æ¿æ¼”ç¤ºç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ¼”ç¤ºç±»"""
        self.test_prompt = "è¯·ç”¨ä¸­æ–‡ç®€è¦ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„ä¸‰ä¸ªä¸»è¦åº”ç”¨é¢†åŸŸï¼Œæ¯ä¸ªé¢†åŸŸç”¨ä¸€å¥è¯æ¦‚æ‹¬ã€‚"
        self.results: Dict[str, Any] = {}
    
    def print_header(self, title: str) -> None:
        """æ‰“å°æ ‡é¢˜"""
        print("\n" + "="*80)
        print(f" {title}")
        print("="*80)
    
    def print_result(self, model_name: str, result: Any, response_time: float = None) -> None:
        """æ‰“å°ç»“æœ"""
        print(f"\nã€{model_name}ã€‘")
        print("-" * 40)
        
        if hasattr(result, 'content'):
            print(f"å“åº”å†…å®¹: {result.content}")
            
            if hasattr(result, 'response_time'):
                print(f"å“åº”æ—¶é—´: {result.response_time:.2f}ç§’")
            elif response_time:
                print(f"å“åº”æ—¶é—´: {response_time:.2f}ç§’")
                
            if hasattr(result, 'total_tokens'):
                print(f"Tokenä½¿ç”¨: {result.total_tokens}")
            elif hasattr(result, 'tokens_used'):
                print(f"Tokenä½¿ç”¨: {result.tokens_used}")
                
            if hasattr(result, 'estimated_cost'):
                if result.estimated_cost > 0:
                    print(f"é¢„ä¼°æˆæœ¬: ${result.estimated_cost:.6f}")
                else:
                    print("æˆæœ¬: å…è´¹ï¼ˆæœ¬åœ°æ¨¡å‹ï¼‰")
        else:
            print(f"å“åº”: {result}")
        
        print("-" * 40)
    
    def demo_openai_template(self) -> None:
        """æ¼”ç¤ºOpenAIæ¨¡æ¿"""
        self.print_header("OpenAIæ¨¡æ¿æ¼”ç¤º")
        
        # æ£€æŸ¥APIå¯†é’¥
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            print("âš ï¸  æœªè®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡ï¼Œè·³è¿‡OpenAIæ¼”ç¤º")
            return
        
        try:
            # åˆ›å»ºæ¨¡æ¿å®ä¾‹
            template = OpenAITemplate()
            
            print("ğŸ”§ é…ç½®OpenAIæ¨¡æ¿...")
            template.setup(
                api_key=api_key,
                model_name="gpt-3.5-turbo",
                temperature=0.7,
                max_tokens=200
            )
            
            print("ğŸ“ æ‰§è¡ŒåŒæ­¥è°ƒç”¨...")
            start_time = time.time()
            result = template.run(self.test_prompt)
            sync_time = time.time() - start_time
            
            self.print_result("GPT-3.5-turbo (åŒæ­¥)", result)
            self.results["openai_sync"] = result
            
            # æ¼”ç¤ºæµå¼è¾“å‡º
            print("\nğŸ“ æ¼”ç¤ºæµå¼è¾“å‡º...")
            print("æµå¼å“åº”: ", end="", flush=True)
            
            for chunk in template.stream("ç”¨ä¸€å¥è¯ä»‹ç»æœºå™¨å­¦ä¹ "):
                print(chunk, end="", flush=True)
                time.sleep(0.05)  # æ¨¡æ‹Ÿå®æ—¶æ˜¾ç¤º
            print("\n")
            
            # æ¼”ç¤ºå¼‚æ­¥è°ƒç”¨
            print("ğŸ“ æ‰§è¡Œå¼‚æ­¥è°ƒç”¨...")
            
            async def async_demo():
                start_time = time.time()
                result = await template.run_async("ç®€è¿°æ·±åº¦å­¦ä¹ çš„ä¼˜åŠ¿")
                async_time = time.time() - start_time
                self.print_result("GPT-3.5-turbo (å¼‚æ­¥)", result, async_time)
                return result
            
            # è¿è¡Œå¼‚æ­¥æ¼”ç¤º
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                async_result = loop.run_until_complete(async_demo())
                self.results["openai_async"] = async_result
            finally:
                loop.close()
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            stats = template.get_statistics()
            print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            print(f"   æ€»è¯·æ±‚æ•°: {stats['total_requests']}")
            print(f"   æ€»Tokenä½¿ç”¨: {stats['total_tokens_used']}")
            print(f"   æ€»æˆæœ¬: ${stats['total_cost']:.6f}")
            print(f"   å¹³å‡å“åº”æ—¶é—´: {stats.get('average_response_time', 0):.2f}ç§’")
            
        except Exception as e:
            print(f"âŒ OpenAIæ¼”ç¤ºå¤±è´¥: {str(e)}")
    
    def demo_anthropic_template(self) -> None:
        """æ¼”ç¤ºAnthropicæ¨¡æ¿"""
        self.print_header("Anthropicæ¨¡æ¿æ¼”ç¤º")
        
        # æ£€æŸ¥APIå¯†é’¥
        api_key = os.getenv("ANTHROPIC_API_KEY")
        if not api_key:
            print("âš ï¸  æœªè®¾ç½®ANTHROPIC_API_KEYç¯å¢ƒå˜é‡ï¼Œè·³è¿‡Anthropicæ¼”ç¤º")
            return
        
        try:
            # åˆ›å»ºæ¨¡æ¿å®ä¾‹
            template = AnthropicTemplate()
            
            print("ğŸ”§ é…ç½®Anthropicæ¨¡æ¿...")
            template.setup(
                api_key=api_key,
                model_name="claude-3-haiku-20240307",  # ä½¿ç”¨è¾ƒä¾¿å®œçš„æ¨¡å‹
                max_tokens=200,
                temperature=0.7,
                system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œè¯·æä¾›å‡†ç¡®å’Œæœ‰ç”¨çš„ä¿¡æ¯ã€‚"
            )
            
            print("ğŸ“ æ‰§è¡Œè°ƒç”¨...")
            start_time = time.time()
            result = template.run(self.test_prompt)
            response_time = time.time() - start_time
            
            self.print_result("Claude-3-Haiku", result)
            self.results["anthropic"] = result
            
            # æ¼”ç¤ºæµå¼è¾“å‡º
            print("\nğŸ“ æ¼”ç¤ºæµå¼è¾“å‡º...")
            print("æµå¼å“åº”: ", end="", flush=True)
            
            for chunk in template.stream("è§£é‡Šä»€ä¹ˆæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†"):
                print(chunk, end="", flush=True)
                time.sleep(0.05)
            print("\n")
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            stats = template.get_statistics()
            print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            print(f"   æ€»è¯·æ±‚æ•°: {stats['total_requests']}")
            print(f"   æ€»Tokenä½¿ç”¨: {stats['total_tokens_used']}")
            print(f"   æ€»æˆæœ¬: ${stats['total_cost']:.6f}")
            
        except Exception as e:
            print(f"âŒ Anthropicæ¼”ç¤ºå¤±è´¥: {str(e)}")
    
    def demo_local_llm_template(self) -> None:
        """æ¼”ç¤ºæœ¬åœ°LLMæ¨¡æ¿"""
        self.print_header("æœ¬åœ°LLMæ¨¡æ¿æ¼”ç¤º")
        
        try:
            # åˆ›å»ºæ¨¡æ¿å®ä¾‹
            template = LocalLLMTemplate()
            
            print("ğŸ”§ é…ç½®æœ¬åœ°LLMæ¨¡æ¿ï¼ˆOllamaåç«¯ï¼‰...")
            template.setup(
                backend="ollama",
                model_name="llama2",  # å¦‚æœæ²¡æœ‰æ­¤æ¨¡å‹ä¼šå°è¯•ä¸‹è½½
                base_url="http://localhost:11434",
                temperature=0.7,
                max_tokens=200,
                system_prompt="You are a helpful AI assistant. Please respond in Chinese."
            )
            
            # æ£€æŸ¥å¥åº·çŠ¶æ€
            health = template.check_health()
            print(f"å¥åº·æ£€æŸ¥: {health}")
            
            if health.get("status") != "healthy":
                print("âš ï¸  æœ¬åœ°LLMæœåŠ¡ä¸å¯ç”¨ï¼Œè¯·ç¡®ä¿Ollamaæ­£åœ¨è¿è¡Œä¸”æ¨¡å‹å·²ä¸‹è½½")
                print("   å¯åŠ¨Ollama: ollama serve")
                print("   ä¸‹è½½æ¨¡å‹: ollama pull llama2")
                return
            
            print("ğŸ“ æ‰§è¡Œè°ƒç”¨...")
            start_time = time.time()
            result = template.run(self.test_prompt)
            response_time = time.time() - start_time
            
            self.print_result("Llama2 (æœ¬åœ°)", result)
            self.results["local"] = result
            
            # æ¼”ç¤ºæµå¼è¾“å‡º
            print("\nğŸ“ æ¼”ç¤ºæµå¼è¾“å‡º...")
            print("æµå¼å“åº”: ", end="", flush=True)
            
            try:
                for chunk in template.stream("What is machine learning?"):
                    print(chunk, end="", flush=True)
                    time.sleep(0.1)
                print("\n")
            except Exception as e:
                print(f"æµå¼è¾“å‡ºå¤±è´¥: {str(e)}")
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            stats = template.get_statistics()
            print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            print(f"   æ€»è¯·æ±‚æ•°: {stats['total_requests']}")
            print(f"   æ€»ç”Ÿæˆæ—¶é—´: {stats['total_generation_time']:.2f}ç§’")
            print(f"   å¹³å‡ç”Ÿæˆé€Ÿåº¦: {stats.get('average_tokens_per_second', 0):.1f} tokens/ç§’")
            
        except Exception as e:
            print(f"âŒ æœ¬åœ°LLMæ¼”ç¤ºå¤±è´¥: {str(e)}")
            print("   è¯·ç¡®ä¿OllamaæœåŠ¡æ­£åœ¨è¿è¡Œï¼šollama serve")
    
    def demo_multi_model_template(self) -> None:
        """æ¼”ç¤ºå¤šæ¨¡å‹æ¨¡æ¿"""
        self.print_header("å¤šæ¨¡å‹æ¨¡æ¿æ¼”ç¤º")
        
        try:
            # åˆ›å»ºå¤šæ¨¡å‹æ¨¡æ¿
            template = MultiModelTemplate()
            
            print("ğŸ”§ é…ç½®å¤šæ¨¡å‹ç³»ç»Ÿ...")
            template.setup(
                routing_strategy="smart",
                fallback_models=[],
                max_parallel_requests=3
            )
            
            # æ·»åŠ å¯ç”¨çš„æ¨¡å‹
            models_added = 0
            
            # å°è¯•æ·»åŠ OpenAIæ¨¡å‹
            if os.getenv("OPENAI_API_KEY"):
                try:
                    openai_template = OpenAITemplate()
                    template.add_model(
                        name="gpt-3.5",
                        template=openai_template,
                        setup_params={
                            "api_key": os.getenv("OPENAI_API_KEY"),
                            "model_name": "gpt-3.5-turbo",
                            "temperature": 0.7,
                            "max_tokens": 150
                        },
                        priority=2,
                        cost_per_1k_tokens=0.002,
                        tags=["fast", "commercial"]
                    )
                    models_added += 1
                    print("âœ… å·²æ·»åŠ OpenAIæ¨¡å‹")
                except Exception as e:
                    print(f"âš ï¸  æ·»åŠ OpenAIæ¨¡å‹å¤±è´¥: {str(e)}")
            
            # å°è¯•æ·»åŠ Anthropicæ¨¡å‹
            if os.getenv("ANTHROPIC_API_KEY"):
                try:
                    anthropic_template = AnthropicTemplate()
                    template.add_model(
                        name="claude",
                        template=anthropic_template,
                        setup_params={
                            "api_key": os.getenv("ANTHROPIC_API_KEY"),
                            "model_name": "claude-3-haiku-20240307",
                            "max_tokens": 150,
                            "temperature": 0.7
                        },
                        priority=3,
                        cost_per_1k_tokens=0.00125,
                        tags=["quality", "commercial"]
                    )
                    models_added += 1
                    print("âœ… å·²æ·»åŠ Anthropicæ¨¡å‹")
                except Exception as e:
                    print(f"âš ï¸  æ·»åŠ Anthropicæ¨¡å‹å¤±è´¥: {str(e)}")
            
            # å°è¯•æ·»åŠ æœ¬åœ°æ¨¡å‹
            try:
                local_template = LocalLLMTemplate()
                # å…ˆæ£€æŸ¥å¥åº·çŠ¶æ€
                local_template.setup(
                    backend="ollama",
                    model_name="llama2",
                    base_url="http://localhost:11434"
                )
                health = local_template.check_health()
                
                if health.get("status") == "healthy":
                    template.add_model(
                        name="local-llama",
                        template=local_template,
                        setup_params={
                            "backend": "ollama",
                            "model_name": "llama2",
                            "temperature": 0.7,
                            "max_tokens": 150
                        },
                        priority=1,
                        cost_per_1k_tokens=0.0,  # å…è´¹
                        tags=["free", "local", "private"]
                    )
                    models_added += 1
                    print("âœ… å·²æ·»åŠ æœ¬åœ°æ¨¡å‹")
                else:
                    print("âš ï¸  æœ¬åœ°æ¨¡å‹ä¸å¯ç”¨")
            except Exception as e:
                print(f"âš ï¸  æ·»åŠ æœ¬åœ°æ¨¡å‹å¤±è´¥: {str(e)}")
            
            if models_added == 0:
                print("âŒ æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹ï¼Œè¯·è®¾ç½®APIå¯†é’¥æˆ–å¯åŠ¨æœ¬åœ°æœåŠ¡")
                return
            
            print(f"\nğŸ“‹ å·²é…ç½® {models_added} ä¸ªæ¨¡å‹")
            
            # æ¼”ç¤ºæ™ºèƒ½è·¯ç”±
            print("\nğŸ“ æ¼”ç¤ºæ™ºèƒ½è·¯ç”±...")
            result = template.run(self.test_prompt)
            self.print_result(f"æ™ºèƒ½è·¯ç”±é€‰æ‹©: {result.model_name}", result)
            
            # æ¼”ç¤ºæ¨¡å‹å¯¹æ¯”ï¼ˆå¦‚æœæœ‰å¤šä¸ªæ¨¡å‹ï¼‰
            if models_added > 1:
                print("\nğŸ“ æ¼”ç¤ºæ¨¡å‹å¯¹æ¯”...")
                comparison = template.compare_models("ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ")
                
                print(f"\nğŸ† å¯¹æ¯”ç»“æœ:")
                print(f"   æœ€å¿«æ¨¡å‹: {comparison.fastest_model}")
                print(f"   æœ€ä¾¿å®œæ¨¡å‹: {comparison.cheapest_model}")
                print(f"   æœ€ä¼˜è´¨é‡æ¨¡å‹: {comparison.best_quality_model}")
                print(f"   æ€»æˆæœ¬: ${comparison.total_cost:.6f}")
                print(f"   å¯¹æ¯”è€—æ—¶: {comparison.comparison_time:.2f}ç§’")
                
                print(f"\nğŸ“Š å„æ¨¡å‹å“åº”:")
                for response in comparison.responses:
                    if response.success:
                        print(f"   {response.model_name}: {response.response_time:.2f}s, "
                              f"{response.tokens_used} tokens, ${response.estimated_cost:.6f}")
                    else:
                        print(f"   {response.model_name}: å¤±è´¥ - {response.error_message}")
            
            # æ¼”ç¤ºä¸åŒåå¥½çš„è·¯ç”±
            if models_added > 1:
                print("\nğŸ“ æ¼”ç¤ºåå¥½è·¯ç”±...")
                
                # æˆæœ¬ä¼˜å…ˆ
                result_cost = template.run("ç®€è¿°AI", prefer_cost=True)
                print(f"æˆæœ¬ä¼˜å…ˆé€‰æ‹©: {result_cost.model_name}")
                
                # è´¨é‡ä¼˜å…ˆ
                result_quality = template.run("ç®€è¿°AI", prefer_quality=True)
                print(f"è´¨é‡ä¼˜å…ˆé€‰æ‹©: {result_quality.model_name}")
                
                # é€Ÿåº¦ä¼˜å…ˆ
                result_speed = template.run("ç®€è¿°AI", prefer_speed=True)
                print(f"é€Ÿåº¦ä¼˜å…ˆé€‰æ‹©: {result_speed.model_name}")
            
            # æ˜¾ç¤ºå…¨å±€ç»Ÿè®¡
            global_stats = template.get_global_statistics()
            print(f"\nğŸ“Š å…¨å±€ç»Ÿè®¡:")
            print(f"   æ€»è¯·æ±‚æ•°: {global_stats['total_requests']}")
            print(f"   æˆåŠŸç‡: {global_stats['success_rate']:.1%}")
            print(f"   æ€»æˆæœ¬: ${global_stats['total_cost']:.6f}")
            print(f"   å¹³å‡å“åº”æ—¶é—´: {global_stats['average_response_time']:.2f}ç§’")
            
            # æ˜¾ç¤ºå„æ¨¡å‹ç»Ÿè®¡
            model_stats = template.get_model_statistics()
            print(f"\nğŸ“Š å„æ¨¡å‹ç»Ÿè®¡:")
            for model_name, stats in model_stats.items():
                print(f"   {model_name}: {stats['requests']}æ¬¡è¯·æ±‚, "
                      f"æˆåŠŸç‡{stats['success_rate']:.1%}, "
                      f"å¹³å‡{stats.get('average_response_time', 0):.2f}ç§’")
                      
        except Exception as e:
            print(f"âŒ å¤šæ¨¡å‹æ¼”ç¤ºå¤±è´¥: {str(e)}")
    
    def run_all_demos(self) -> None:
        """è¿è¡Œæ‰€æœ‰æ¼”ç¤º"""
        print("ğŸš€ å¼€å§‹LLMæ¨¡æ¿æ¼”ç¤º")
        print("æœ¬æ¼”ç¤ºå°†å±•ç¤ºOpenAIã€Anthropicã€æœ¬åœ°LLMå’Œå¤šæ¨¡å‹æ¨¡æ¿çš„åŠŸèƒ½")
        
        # è¿è¡Œå„ä¸ªæ¼”ç¤º
        self.demo_openai_template()
        self.demo_anthropic_template()
        self.demo_local_llm_template()
        self.demo_multi_model_template()
        
        # æ€»ç»“
        self.print_header("æ¼”ç¤ºæ€»ç»“")
        print("âœ¨ æ¼”ç¤ºå®Œæˆï¼")
        
        if self.results:
            print(f"\nğŸ“‹ å…±å®Œæˆ {len(self.results)} ä¸ªæ¨¡æ¿æ¼”ç¤º")
            for model_name, result in self.results.items():
                print(f"   âœ… {model_name}: {len(result.content) if hasattr(result, 'content') else 'N/A'} å­—ç¬¦")
        
        print("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
        print("   - å¯¹äºäº‘ç«¯APIï¼šOpenAIé€‚åˆå¿«é€ŸåŸå‹ï¼ŒAnthropicé€‚åˆå¤æ‚æ¨ç†")
        print("   - å¯¹äºæœ¬åœ°éƒ¨ç½²ï¼šä½¿ç”¨Ollamaå¯ä»¥å®Œå…¨æ§åˆ¶æ•°æ®éšç§")
        print("   - å¯¹äºç”Ÿäº§ç¯å¢ƒï¼šä½¿ç”¨å¤šæ¨¡å‹æ¨¡æ¿å®ç°è´Ÿè½½å‡è¡¡å’Œæ•…éšœè½¬ç§»")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="LLMæ¨¡æ¿æ¼”ç¤ºè„šæœ¬")
    parser.add_argument(
        "--demo",
        choices=["openai", "anthropic", "local", "multi", "all"],
        default="all",
        help="é€‰æ‹©è¦è¿è¡Œçš„æ¼”ç¤º"
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ¼”ç¤ºå®ä¾‹
    demo = LLMTemplateDemo()
    
    # æ ¹æ®å‚æ•°è¿è¡Œç›¸åº”çš„æ¼”ç¤º
    if args.demo == "openai":
        demo.demo_openai_template()
    elif args.demo == "anthropic":
        demo.demo_anthropic_template()
    elif args.demo == "local":
        demo.demo_local_llm_template()
    elif args.demo == "multi":
        demo.demo_multi_model_template()
    else:
        demo.run_all_demos()


if __name__ == "__main__":
    main()