# LangChain Learning å…¥é—¨æ•™ç¨‹

æœ¬æ•™ç¨‹å°†å¸®åŠ©ä½ å¿«é€Ÿä¸Šæ‰‹LangChain Learningæ¨¡æ¿ç³»ç»Ÿï¼Œä»é›¶å¼€å§‹æ„å»ºä½ çš„ç¬¬ä¸€ä¸ªLLMåº”ç”¨ã€‚

## ğŸ“š å­¦ä¹ ç›®æ ‡

å®Œæˆæœ¬æ•™ç¨‹åï¼Œä½ å°†èƒ½å¤Ÿï¼š
- ç†è§£æ¨¡æ¿ç³»ç»Ÿçš„åŸºæœ¬æ¦‚å¿µå’Œæ¶æ„
- é…ç½®å’Œä½¿ç”¨LLMæ¨¡æ¿
- å¤„ç†æ–‡æ¡£å’Œæ„å»ºçŸ¥è¯†åº“
- åˆ›å»ºæ™ºèƒ½å¯¹è¯ç³»ç»Ÿ
- ç›‘æ§å’Œä¼˜åŒ–åº”ç”¨æ€§èƒ½

## ğŸ”§ ç¯å¢ƒå‡†å¤‡

### 1. å®‰è£…ä¾èµ–

```bash
# åŸºç¡€ä¾èµ–
pip install langchain>=0.1.0
pip install langchain-openai>=0.1.0
pip install langchain-anthropic>=0.1.0

# æ•°æ®å¤„ç†ä¾èµ–
pip install chromadb>=0.4.0
pip install sentence-transformers>=2.2.0
pip install pypdf>=3.0.0

# å¯é€‰ä¾èµ–
pip install faiss-cpu>=1.7.4  # é«˜æ€§èƒ½å‘é‡æœç´¢
pip install tiktoken>=0.5.0   # OpenAI tokenè®¡ç®—
```

### 2. è®¾ç½®APIå¯†é’¥

```bash
# è®¾ç½®OpenAI APIå¯†é’¥
export OPENAI_API_KEY="sk-your-openai-api-key"

# è®¾ç½®Anthropic APIå¯†é’¥ï¼ˆå¯é€‰ï¼‰
export ANTHROPIC_API_KEY="sk-ant-your-anthropic-api-key"

# æˆ–è€…åˆ›å»º.envæ–‡ä»¶
echo "OPENAI_API_KEY=sk-your-openai-api-key" > .env
echo "ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key" >> .env
```

### 3. éªŒè¯å®‰è£…

```python
# test_installation.py
import os
from templates.llm.openai_template import OpenAITemplate

# æ£€æŸ¥APIå¯†é’¥
assert os.getenv("OPENAI_API_KEY"), "è¯·è®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡"

# æµ‹è¯•æ¨¡æ¿å¯¼å…¥
template = OpenAITemplate()
print("âœ… å®‰è£…æˆåŠŸï¼æ¨¡æ¿ç³»ç»Ÿå·²å°±ç»ª")
```

## ğŸš€ ç¬¬ä¸€æ­¥ï¼šä½ çš„ç¬¬ä¸€ä¸ªLLMè°ƒç”¨

è®©æˆ‘ä»¬ä»æœ€ç®€å•çš„LLMè°ƒç”¨å¼€å§‹ï¼š

```python
# step1_first_llm_call.py
from templates.llm.openai_template import OpenAITemplate

# 1. åˆ›å»ºLLMæ¨¡æ¿å®ä¾‹
llm = OpenAITemplate()

# 2. é…ç½®å‚æ•°
llm.setup(
    api_key=os.getenv("OPENAI_API_KEY"),
    model_name="gpt-3.5-turbo",
    temperature=0.7,
    max_tokens=1000
)

# 3. æ‰§è¡Œè°ƒç”¨
response = llm.run("è¯·ä»‹ç»ä¸€ä¸‹Pythonç¼–ç¨‹è¯­è¨€çš„ç‰¹ç‚¹")

# 4. æŸ¥çœ‹ç»“æœ
print("AIå›å¤:", response.content)
print("æ‰§è¡Œæ—¶é—´:", llm.get_status()["execution_time"], "ç§’")
```

### ğŸ’¡ ç†è§£å…³é”®æ¦‚å¿µ

**æ¨¡æ¿(Template)**: å°è£…äº†ç‰¹å®šåŠŸèƒ½çš„å¯å¤ç”¨ä»£ç å•å…ƒ
- `setup()`: é…ç½®æ¨¡æ¿å‚æ•°
- `run()`: æ‰§è¡Œæ ¸å¿ƒåŠŸèƒ½
- `get_status()`: è·å–æ‰§è¡ŒçŠ¶æ€

**é…ç½®å‚æ•°**:
- `model_name`: ä½¿ç”¨çš„æ¨¡å‹åç§°
- `temperature`: æ§åˆ¶è¾“å‡ºéšæœºæ€§ (0.0-2.0)
- `max_tokens`: æœ€å¤§è¾“å‡ºé•¿åº¦

## ğŸ“ ç¬¬äºŒæ­¥ï¼šæç¤ºå·¥ç¨‹å’Œå‚æ•°è°ƒä¼˜

å­¦ä¹ å¦‚ä½•ä¼˜åŒ–æç¤ºè¯å’Œè°ƒæ•´å‚æ•°ï¼š

```python
# step2_prompt_engineering.py
from templates.llm.openai_template import OpenAITemplate

def compare_temperatures():
    """æ¯”è¾ƒä¸åŒæ¸©åº¦å‚æ•°çš„æ•ˆæœ"""
    
    prompt = "è¯·ä¸ºä¸€å®¶ç§‘æŠ€å…¬å¸å†™ä¸€ä¸ªåˆ›æ„å¹¿å‘Šè¯­"
    temperatures = [0.1, 0.7, 1.3]
    
    for temp in temperatures:
        print(f"\n=== æ¸©åº¦å‚æ•°: {temp} ===")
        
        llm = OpenAITemplate()
        llm.setup(
            model_name="gpt-3.5-turbo",
            temperature=temp,
            max_tokens=100
        )
        
        response = llm.run(prompt)
        print("è¾“å‡º:", response.content)

def system_prompt_example():
    """ä½¿ç”¨ç³»ç»Ÿæç¤ºè¯å®šä¹‰AIè§’è‰²"""
    
    llm = OpenAITemplate()
    llm.setup(
        model_name="gpt-3.5-turbo",
        temperature=0.7,
        system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Pythonç¼–ç¨‹å¯¼å¸ˆï¼Œæ“…é•¿ç”¨ç®€å•æ˜“æ‡‚çš„æ–¹å¼è§£é‡Šå¤æ‚æ¦‚å¿µã€‚"
    )
    
    response = llm.run("ä»€ä¹ˆæ˜¯è£…é¥°å™¨ï¼Ÿè¯·ä¸¾ä¸ªä¾‹å­")
    print("ä¸“ä¸šå¯¼å¸ˆå›å¤:", response.content)

# è¿è¡Œç¤ºä¾‹
compare_temperatures()
system_prompt_example()
```

### ğŸ¯ æç¤ºå·¥ç¨‹æœ€ä½³å®è·µ

1. **æ˜ç¡®æŒ‡ä»¤**: ä½¿ç”¨æ¸…æ™°ã€å…·ä½“çš„æŒ‡ä»¤
2. **æä¾›ä¸Šä¸‹æ–‡**: ç»™AIè¶³å¤Ÿçš„èƒŒæ™¯ä¿¡æ¯
3. **æŒ‡å®šæ ¼å¼**: æ˜ç¡®æœŸæœ›çš„è¾“å‡ºæ ¼å¼
4. **ä½¿ç”¨ç¤ºä¾‹**: é€šè¿‡ç¤ºä¾‹å±•ç¤ºæœŸæœ›çš„è¡Œä¸º

```python
# å¥½çš„æç¤ºè¯ç¤ºä¾‹
good_prompt = """
ä½œä¸ºä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆï¼Œè¯·åˆ†æä»¥ä¸‹é”€å”®æ•°æ®ï¼š

æ•°æ®: [1æœˆ: 100ä¸‡, 2æœˆ: 120ä¸‡, 3æœˆ: 95ä¸‡]

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºåˆ†æç»“æœï¼š
1. è¶‹åŠ¿åˆ†æ
2. å…³é”®å‘ç°
3. æ”¹è¿›å»ºè®®

ä½¿ç”¨ä¸“ä¸šä½†æ˜“æ‡‚çš„è¯­è¨€ã€‚
"""
```

## ğŸ“„ ç¬¬ä¸‰æ­¥ï¼šæ–‡æ¡£å¤„ç†å’ŒçŸ¥è¯†åº“æ„å»º

å­¦ä¹ å¦‚ä½•å¤„ç†æ–‡æ¡£å¹¶æ„å»ºå¯æœç´¢çš„çŸ¥è¯†åº“ï¼š

```python
# step3_document_processing.py
from templates.data.document_loader import DocumentLoaderTemplate
from templates.data.text_splitter import TextSplitterTemplate
from templates.data.vectorstore_template import VectorStoreTemplate

def build_knowledge_base():
    """æ„å»ºçŸ¥è¯†åº“çš„å®Œæ•´æµç¨‹"""
    
    # 1. åŠ è½½æ–‡æ¡£
    print("ğŸ“š åŠ è½½æ–‡æ¡£...")
    loader = DocumentLoaderTemplate()
    loader.setup(
        file_paths=["./docs/company_handbook.pdf", "./docs/policies.txt"],
        file_type="auto",  # è‡ªåŠ¨æ£€æµ‹æ–‡ä»¶ç±»å‹
        encoding="utf-8"
    )
    documents = loader.run()
    print(f"âœ… åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")
    
    # 2. åˆ†å‰²æ–‡æœ¬
    print("âœ‚ï¸ åˆ†å‰²æ–‡æœ¬...")
    splitter = TextSplitterTemplate()
    splitter.setup(
        splitter_type="recursive",
        chunk_size=1000,      # æ¯ä¸ªå—1000å­—ç¬¦
        chunk_overlap=100     # å—ä¹‹é—´é‡å 100å­—ç¬¦
    )
    chunks = splitter.run(documents)
    print(f"âœ… ç”Ÿæˆäº† {len(chunks)} ä¸ªæ–‡æœ¬å—")
    
    # 3. åˆ›å»ºå‘é‡å­˜å‚¨
    print("ğŸ” åˆ›å»ºå‘é‡å­˜å‚¨...")
    vectorstore = VectorStoreTemplate()
    vectorstore.setup(
        vectorstore_type="chroma",
        collection_name="company_knowledge",
        embedding_model="text-embedding-ada-002",
        persist_directory="./data/knowledge_base"
    )
    result = vectorstore.run(chunks)
    print(f"âœ… å‘é‡å­˜å‚¨åˆ›å»ºå®Œæˆï¼Œå­˜å‚¨äº† {result['count']} ä¸ªå‘é‡")
    
    return vectorstore

def search_knowledge_base(vectorstore):
    """æœç´¢çŸ¥è¯†åº“"""
    from templates.data.retrieval_template import RetrievalTemplate
    
    retriever = RetrievalTemplate()
    retriever.setup(
        vectorstore=vectorstore,
        k=3,  # è¿”å›æœ€ç›¸å…³çš„3ä¸ªç»“æœ
        similarity_threshold=0.7
    )
    
    # æµ‹è¯•æœç´¢
    queries = [
        "å…¬å¸çš„ä¼‘å‡æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿ",
        "å¦‚ä½•ç”³è¯·å‡ºå·®ï¼Ÿ",
        "å‘˜å·¥ç¦åˆ©æœ‰å“ªäº›ï¼Ÿ"
    ]
    
    for query in queries:
        print(f"\nâ“ æŸ¥è¯¢: {query}")
        results = retriever.run(query)
        
        for i, result in enumerate(results, 1):
            print(f"ğŸ“„ ç»“æœ {i}: {result['content'][:100]}...")

# è¿è¡Œç¤ºä¾‹
vectorstore = build_knowledge_base()
search_knowledge_base(vectorstore)
```

### ğŸ“‹ æ–‡æ¡£å¤„ç†è¦ç‚¹

**æ”¯æŒçš„æ–‡ä»¶æ ¼å¼**:
- æ–‡æœ¬æ–‡ä»¶: `.txt`, `.md`
- PDFæ–‡æ¡£: `.pdf`
- Wordæ–‡æ¡£: `.docx`
- ç½‘é¡µæ–‡ä»¶: `.html`

**æ–‡æœ¬åˆ†å‰²ç­–ç•¥**:
- `recursive`: é€’å½’åˆ†å‰²ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§
- `character`: æŒ‰å­—ç¬¦æ•°åˆ†å‰²
- `token`: æŒ‰tokenæ•°åˆ†å‰²

**å‘é‡åŒ–é€‰æ‹©**:
- OpenAI Embeddings: é«˜è´¨é‡ï¼Œéœ€è¦APIè°ƒç”¨
- SentenceTransformers: æœ¬åœ°è¿è¡Œï¼Œå…è´¹ä½¿ç”¨

## ğŸ¤– ç¬¬å››æ­¥ï¼šæ„å»ºæ™ºèƒ½å¯¹è¯ç³»ç»Ÿ

ç»“åˆLLMå’ŒçŸ¥è¯†åº“åˆ›å»ºæ™ºèƒ½é—®ç­”ç³»ç»Ÿï¼š

```python
# step4_qa_system.py
from templates.llm.openai_template import OpenAITemplate
from templates.chains.sequential_chain import SequentialChainTemplate
from templates.data.retrieval_template import RetrievalTemplate

class IntelligentQASystem:
    """æ™ºèƒ½é—®ç­”ç³»ç»Ÿ"""
    
    def __init__(self, vectorstore):
        self.vectorstore = vectorstore
        self._setup_components()
    
    def _setup_components(self):
        """è®¾ç½®ç³»ç»Ÿç»„ä»¶"""
        
        # 1. è®¾ç½®æ£€ç´¢å™¨
        self.retriever = RetrievalTemplate()
        self.retriever.setup(
            vectorstore=self.vectorstore,
            k=3,
            similarity_threshold=0.6
        )
        
        # 2. è®¾ç½®LLM
        self.llm = OpenAITemplate()
        self.llm.setup(
            model_name="gpt-3.5-turbo",
            temperature=0.1,  # ä½æ¸©åº¦ç¡®ä¿å‡†ç¡®æ€§
            system_prompt="""
            ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¼ä¸šåŠ©æ‰‹ã€‚åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
            å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯šå®åœ°è¯´"æˆ‘åœ¨ç°æœ‰æ–‡æ¡£ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯"ã€‚
            å›ç­”è¦å‡†ç¡®ã€ç®€æ´ã€æœ‰ç”¨ã€‚
            """
        )
        
        # 3. è®¾ç½®é—®ç­”é“¾
        self.qa_chain = SequentialChainTemplate()
        self.qa_chain.setup(
            chain_type="sequential",
            llm=self.llm.llm,
            steps=[
                {
                    "name": "context_retrieval",
                    "prompt": "åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ï¼š\n\næ–‡æ¡£å†…å®¹ï¼š\n{context}\n\né—®é¢˜ï¼š{question}\n\nç­”æ¡ˆï¼š",
                    "output_key": "answer"
                }
            ],
            input_variables=["context", "question"]
        )
    
    def ask(self, question: str) -> dict:
        """å¤„ç†ç”¨æˆ·é—®é¢˜"""
        
        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        relevant_docs = self.retriever.run(question)
        
        if not relevant_docs:
            return {
                "answer": "æŠ±æ­‰ï¼Œæˆ‘åœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚",
                "sources": [],
                "confidence": 0.0
            }
        
        # 2. å‡†å¤‡ä¸Šä¸‹æ–‡
        context = "\n".join([doc["content"] for doc in relevant_docs])
        
        # 3. ç”Ÿæˆå›ç­”
        result = self.qa_chain.run({
            "context": context,
            "question": question
        })
        
        return {
            "answer": result["answer"],
            "sources": [doc["metadata"]["source"] for doc in relevant_docs],
            "confidence": sum(doc.get("score", 0) for doc in relevant_docs) / len(relevant_docs)
        }

def interactive_chat(qa_system):
    """äº¤äº’å¼èŠå¤©ç•Œé¢"""
    print("\nğŸ¤– æ™ºèƒ½é—®ç­”ç³»ç»Ÿå·²å¯åŠ¨ï¼è¾“å…¥'quit'é€€å‡º")
    print("-" * 50)
    
    while True:
        question = input("\nâ“ ä½ çš„é—®é¢˜: ").strip()
        
        if question.lower() in ['quit', 'exit', 'q']:
            print("ğŸ‘‹ å†è§ï¼")
            break
        
        if not question:
            continue
        
        # è·å–å›ç­”
        result = qa_system.ask(question)
        
        print(f"\nğŸ¤– å›ç­”: {result['answer']}")
        print(f"ğŸ“š ä¿¡æ¯æ¥æº: {', '.join(result['sources'])}")
        print(f"ğŸ¯ ç½®ä¿¡åº¦: {result['confidence']:.2f}")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # å‡è®¾ä½ å·²ç»æœ‰äº†vectorstoreï¼ˆä»ç¬¬ä¸‰æ­¥è·å¾—ï¼‰
    qa_system = IntelligentQASystem(vectorstore)
    
    # æµ‹è¯•å‡ ä¸ªé—®é¢˜
    test_questions = [
        "å…¬å¸çš„å·¥ä½œæ—¶é—´æ˜¯ä»€ä¹ˆï¼Ÿ",
        "å¦‚ä½•ç”³è¯·å¹´å‡ï¼Ÿ",
        "å…¬å¸æœ‰å“ªäº›åŸ¹è®­æœºä¼šï¼Ÿ"
    ]
    
    for question in test_questions:
        result = qa_system.ask(question)
        print(f"\né—®é¢˜: {question}")
        print(f"å›ç­”: {result['answer']}")
    
    # å¯åŠ¨äº¤äº’å¼èŠå¤©
    interactive_chat(qa_system)
```

## ğŸ”„ ç¬¬äº”æ­¥ï¼šä½¿ç”¨é“¾ç»„åˆå¤æ‚å·¥ä½œæµ

å­¦ä¹ å¦‚ä½•ç»„åˆå¤šä¸ªæ­¥éª¤åˆ›å»ºå¤æ‚çš„å·¥ä½œæµï¼š

```python
# step5_advanced_chains.py
from templates.chains.sequential_chain import SequentialChainTemplate
from templates.chains.parallel_chain import ParallelChainTemplate
from templates.llm.openai_template import OpenAITemplate

def content_creation_pipeline():
    """å†…å®¹åˆ›ä½œæµæ°´çº¿"""
    
    # åˆ›å»ºLLMå®ä¾‹
    llm = OpenAITemplate()
    llm.setup(model_name="gpt-3.5-turbo", temperature=0.8)
    
    # åˆ›å»ºé¡ºåºé“¾ï¼šå¤§çº² -> å†…å®¹ -> ä¿®æ”¹
    chain = SequentialChainTemplate()
    chain.setup(
        chain_type="sequential",
        llm=llm.llm,
        steps=[
            {
                "name": "outline_generation",
                "prompt": "ä¸ºä¸»é¢˜'{topic}'åˆ›å»ºè¯¦ç»†çš„æ–‡ç« å¤§çº²ï¼ŒåŒ…å«3-5ä¸ªä¸»è¦éƒ¨åˆ†",
                "output_key": "outline"
            },
            {
                "name": "content_writing",
                "prompt": "åŸºäºä»¥ä¸‹å¤§çº²å†™ä¸€ç¯‡ä¸“ä¸šçš„æ–‡ç« ï¼š\n{outline}",
                "output_key": "draft"
            },
            {
                "name": "content_polish",
                "prompt": "æ”¹è¿›ä»¥ä¸‹æ–‡ç« ï¼Œä½¿å…¶æ›´åŠ ç”ŸåŠ¨æœ‰è¶£ï¼š\n{draft}",
                "output_key": "final_article"
            }
        ],
        input_variables=["topic"]
    )
    
    # æµ‹è¯•æµæ°´çº¿
    topic = "äººå·¥æ™ºèƒ½åœ¨æ•™è‚²ä¸­çš„åº”ç”¨"
    result = chain.run({"topic": topic})
    
    print("ğŸ“ å¤§çº²:")
    print(result["outline"])
    print("\nğŸ“„ æœ€ç»ˆæ–‡ç« :")
    print(result["final_article"][:500] + "...")

def multi_perspective_analysis():
    """å¤šè§’åº¦åˆ†æï¼ˆå¹¶è¡Œå¤„ç†ï¼‰"""
    
    llm = OpenAITemplate()
    llm.setup(model_name="gpt-3.5-turbo", temperature=0.6)
    
    # åˆ›å»ºå¹¶è¡Œé“¾ï¼šåŒæ—¶è¿›è¡ŒæŠ€æœ¯ã€å•†ä¸šã€ç¤¾ä¼šå½±å“åˆ†æ
    chain = ParallelChainTemplate()
    chain.setup(
        chain_type="parallel",
        llm=llm.llm,
        steps=[
            {
                "name": "technical_analysis",
                "prompt": "ä»æŠ€æœ¯è§’åº¦åˆ†æ'{topic}'ï¼Œé‡ç‚¹å…³æ³¨æŠ€æœ¯åŸç†å’Œå®ç°æ–¹å¼",
                "output_key": "tech_analysis"
            },
            {
                "name": "business_analysis",
                "prompt": "ä»å•†ä¸šè§’åº¦åˆ†æ'{topic}'ï¼Œé‡ç‚¹å…³æ³¨å¸‚åœºæœºä¼šå’Œå•†ä¸šæ¨¡å¼",
                "output_key": "business_analysis"
            },
            {
                "name": "social_analysis",
                "prompt": "ä»ç¤¾ä¼šå½±å“è§’åº¦åˆ†æ'{topic}'ï¼Œé‡ç‚¹å…³æ³¨å¯¹ç¤¾ä¼šçš„ç§¯æå’Œæ¶ˆæå½±å“",
                "output_key": "social_analysis"
            }
        ],
        input_variables=["topic"],
        max_workers=3,  # å¹¶è¡Œæ‰§è¡Œ
        timeout=60.0
    )
    
    # æµ‹è¯•å¹¶è¡Œåˆ†æ
    topic = "åŒºå—é“¾æŠ€æœ¯"
    result = chain.run({"topic": topic})
    
    print("ğŸ”§ æŠ€æœ¯åˆ†æ:")
    print(result["tech_analysis"][:200] + "...")
    print("\nğŸ’¼ å•†ä¸šåˆ†æ:")
    print(result["business_analysis"][:200] + "...")
    print("\nğŸŒ ç¤¾ä¼šå½±å“åˆ†æ:")
    print(result["social_analysis"][:200] + "...")

# è¿è¡Œç¤ºä¾‹
content_creation_pipeline()
print("\n" + "="*50 + "\n")
multi_perspective_analysis()
```

## ğŸ“Š ç¬¬å…­æ­¥ï¼šæ€§èƒ½ç›‘æ§å’Œä¼˜åŒ–

å­¦ä¹ å¦‚ä½•ç›‘æ§å’Œä¼˜åŒ–ä½ çš„åº”ç”¨ï¼š

```python
# step6_monitoring.py
from templates.evaluation.performance_eval import PerformanceEvalTemplate
from templates.evaluation.cost_analysis import CostAnalysisTemplate
import time

def performance_monitoring_example():
    """æ€§èƒ½ç›‘æ§ç¤ºä¾‹"""
    
    # åˆ›å»ºæ€§èƒ½è¯„ä¼°å™¨
    perf_eval = PerformanceEvalTemplate()
    perf_eval.setup(
        benchmark_enabled=True,
        profiling_enabled=True,
        memory_profiling=True
    )
    
    # åˆ›å»ºæˆæœ¬åˆ†æå™¨
    cost_analyzer = CostAnalysisTemplate()
    cost_analyzer.setup(
        track_api_calls=True,
        cost_per_token=0.0015,  # GPT-3.5-turboä»·æ ¼
        daily_budget=10.0
    )
    
    # æµ‹è¯•ä¸åŒé…ç½®çš„æ€§èƒ½
    llm = OpenAITemplate()
    
    test_configs = [
        {"model": "gpt-3.5-turbo", "temp": 0.3, "max_tokens": 500},
        {"model": "gpt-3.5-turbo", "temp": 0.7, "max_tokens": 1000},
    ]
    
    for i, config in enumerate(test_configs, 1):
        print(f"\nğŸ“ˆ æµ‹è¯•é…ç½® {i}: {config}")
        
        llm.setup(
            model_name=config["model"],
            temperature=config["temp"],
            max_tokens=config["max_tokens"]
        )
        
        # æ€§èƒ½æµ‹è¯•
        with perf_eval.measure(f"config_{i}"):
            with cost_analyzer.track_cost():
                start_time = time.time()
                
                # æ‰§è¡Œå¤šæ¬¡è°ƒç”¨
                for j in range(3):
                    result = llm.run(f"è§£é‡Šä»€ä¹ˆæ˜¯äº‘è®¡ç®— - æµ‹è¯• {j+1}")
                
                end_time = time.time()
        
        # æ˜¾ç¤ºç»“æœ
        print(f"â±ï¸ æ€»æ‰§è¡Œæ—¶é—´: {end_time - start_time:.2f}ç§’")
        print(f"ğŸ’° é¢„ä¼°æˆæœ¬: ${cost_analyzer.get_total_cost():.4f}")
        
        # è·å–æ€§èƒ½æŠ¥å‘Š
        perf_report = perf_eval.get_report(f"config_{i}")
        print(f"ğŸ§  å†…å­˜å³°å€¼: {perf_report.get('peak_memory', 'N/A')} MB")

def optimization_tips():
    """ä¼˜åŒ–å»ºè®®"""
    print("\nğŸš€ æ€§èƒ½ä¼˜åŒ–å»ºè®®:")
    print("1. æ¨¡å‹é€‰æ‹©: æ ¹æ®ä»»åŠ¡å¤æ‚åº¦é€‰æ‹©åˆé€‚çš„æ¨¡å‹")
    print("2. å‚æ•°è°ƒä¼˜: è°ƒæ•´temperatureå’Œmax_tokenså¹³è¡¡è´¨é‡å’Œæˆæœ¬")
    print("3. ç¼“å­˜ç­–ç•¥: å¯¹ç›¸ä¼¼æŸ¥è¯¢å¯ç”¨ç¼“å­˜")
    print("4. æ‰¹å¤„ç†: ä½¿ç”¨æ‰¹å¤„ç†å‡å°‘APIè°ƒç”¨å¼€é”€")
    print("5. å¼‚æ­¥å¤„ç†: å¯¹ç‹¬ç«‹ä»»åŠ¡ä½¿ç”¨å¼‚æ­¥æ‰§è¡Œ")

# è¿è¡Œç›‘æ§ç¤ºä¾‹
performance_monitoring_example()
optimization_tips()
```

## ğŸ“ æ€»ç»“å’Œä¸‹ä¸€æ­¥

æ­å–œï¼ä½ å·²ç»å®Œæˆäº†LangChain Learningçš„å…¥é—¨æ•™ç¨‹ã€‚ç°åœ¨ä½ åº”è¯¥èƒ½å¤Ÿï¼š

### âœ… ä½ å­¦ä¼šäº†ä»€ä¹ˆ

1. **åŸºç¡€æ“ä½œ**:
   - é…ç½®å’Œä½¿ç”¨LLMæ¨¡æ¿
   - å¤„ç†ä¸åŒç±»å‹çš„æ–‡æ¡£
   - æ„å»ºå’Œæœç´¢å‘é‡æ•°æ®åº“

2. **é«˜çº§åŠŸèƒ½**:
   - åˆ›å»ºå¤æ‚çš„å¤„ç†é“¾
   - æ„å»ºæ™ºèƒ½é—®ç­”ç³»ç»Ÿ
   - ç›‘æ§åº”ç”¨æ€§èƒ½

3. **æœ€ä½³å®è·µ**:
   - æç¤ºå·¥ç¨‹æŠ€å·§
   - é”™è¯¯å¤„ç†ç­–ç•¥
   - æ€§èƒ½ä¼˜åŒ–æ–¹æ³•

### ğŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ 

1. **æ·±å…¥å­¦ä¹ **:
   - [æ™ºèƒ½ä»£ç†æ•™ç¨‹](02_agents_deep_dive.md)
   - [é«˜çº§é“¾ç»„åˆ](03_advanced_chains.md)
   - [è‡ªå®šä¹‰æ¨¡æ¿å¼€å‘](04_custom_templates.md)

2. **å®æˆ˜é¡¹ç›®**:
   - æ„å»ºå®¢æœæœºå™¨äºº
   - åˆ›å»ºä»£ç åŠ©æ‰‹
   - å¼€å‘å†…å®¹ç”Ÿæˆå·¥å…·

3. **ç¤¾åŒºèµ„æº**:
   - [GitHubè®¨è®ºåŒº](https://github.com/your-repo/discussions)
   - [ç¤ºä¾‹é¡¹ç›®åº“](../examples/)
   - [æœ€ä½³å®è·µæŒ‡å—](../best_practices/)

### ğŸ”§ å¸¸ç”¨å‘½ä»¤é€ŸæŸ¥

```bash
# å¿«é€Ÿæµ‹è¯•å®‰è£…
python -c "from templates.llm.openai_template import OpenAITemplate; print('âœ… OK')"

# æŸ¥çœ‹æ¨¡æ¿åˆ—è¡¨
ls templates/*/

# è¿è¡ŒåŸºç¡€ç¤ºä¾‹
python templates/examples/basic_examples/01_llm_basic_usage.py

# æ£€æŸ¥é…ç½®
python -c "from templates.base.config_loader import ConfigLoader; print(ConfigLoader().get_config())"
```

### ğŸ’¡ æç¤ºå’ŒæŠ€å·§

- **å¼€å‘æ—¶**: ä½¿ç”¨`development.yaml`é…ç½®ï¼Œå¯ç”¨è°ƒè¯•æ¨¡å¼
- **ç”Ÿäº§æ—¶**: ä½¿ç”¨`production.yaml`é…ç½®ï¼Œå…³æ³¨æ€§èƒ½å’Œå®‰å…¨
- **æµ‹è¯•æ—¶**: ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹å’Œæ•°æ®é›†å¿«é€Ÿè¿­ä»£
- **è°ƒè¯•æ—¶**: å¯ç”¨`verbose=True`æŸ¥çœ‹è¯¦ç»†æ‰§è¡Œè¿‡ç¨‹

---

**éœ€è¦å¸®åŠ©ï¼Ÿ**
- æŸ¥çœ‹ [FAQæ–‡æ¡£](../docs/faq.md)
- è®¿é—® [æ•…éšœæ’é™¤æŒ‡å—](../docs/troubleshooting.md)
- åœ¨ [GitHub Issues](https://github.com/your-repo/issues) æé—®

ç¥ä½ åœ¨LangChain Learningçš„å­¦ä¹ æ—…ç¨‹ä¸­æ”¶è·æ»¡æ»¡ï¼ğŸ‰