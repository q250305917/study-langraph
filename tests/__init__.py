"""
æµ‹è¯•æ¨¡å—

æœ¬æ¨¡å—åŒ…å« LangChain å­¦ä¹ é¡¹ç›®çš„æ‰€æœ‰æµ‹è¯•ä»£ç ã€‚
æµ‹è¯•é‡‡ç”¨ pytest æ¡†æ¶ï¼Œéµå¾ªæµ‹è¯•æœ€ä½³å®è·µï¼Œç¡®ä¿ä»£ç è´¨é‡å’Œå¯é æ€§ã€‚

æµ‹è¯•ç»“æ„ï¼š
- unit/: å•å…ƒæµ‹è¯•ï¼Œæµ‹è¯•å•ä¸ªå‡½æ•°æˆ–ç±»çš„åŠŸèƒ½
- integration/: é›†æˆæµ‹è¯•ï¼Œæµ‹è¯•æ¨¡å—é—´çš„äº¤äº’
- e2e/: ç«¯åˆ°ç«¯æµ‹è¯•ï¼Œæµ‹è¯•å®Œæ•´çš„ç”¨æˆ·åœºæ™¯
- fixtures/: æµ‹è¯•å¤¹å…·å’Œå…±äº«æµ‹è¯•æ•°æ®
- conftest.py: pytest é…ç½®å’Œå…±äº«å¤¹å…·

æµ‹è¯•ç±»å‹ï¼š
1. å•å…ƒæµ‹è¯•ï¼šæµ‹è¯•å•ä¸ªå‡½æ•°ã€ç±»æˆ–æ–¹æ³•
2. é›†æˆæµ‹è¯•ï¼šæµ‹è¯•æ¨¡å—ã€ç»„ä»¶é—´çš„é›†æˆ
3. åŠŸèƒ½æµ‹è¯•ï¼šæµ‹è¯•ä¸šåŠ¡åŠŸèƒ½çš„æ­£ç¡®æ€§
4. æ€§èƒ½æµ‹è¯•ï¼šæµ‹è¯•ä»£ç çš„æ€§èƒ½å’Œæ•ˆç‡
5. å®‰å…¨æµ‹è¯•ï¼šæµ‹è¯•å®‰å…¨ç›¸å…³çš„åŠŸèƒ½
6. APIæµ‹è¯•ï¼šæµ‹è¯•å¤–éƒ¨APIçš„é›†æˆï¼ˆå¯è·³è¿‡ï¼‰

æµ‹è¯•çº¦å®šï¼š
- æµ‹è¯•æ–‡ä»¶ä»¥ test_ å¼€å¤´
- æµ‹è¯•ç±»ä»¥ Test å¼€å¤´
- æµ‹è¯•æ–¹æ³•ä»¥ test_ å¼€å¤´
- ä½¿ç”¨æè¿°æ€§çš„æµ‹è¯•æ–¹æ³•å
- æ¯ä¸ªæµ‹è¯•åªæµ‹è¯•ä¸€ä¸ªåŠŸèƒ½ç‚¹
- æµ‹è¯•åº”è¯¥ç‹¬ç«‹ä¸”å¯é‡å¤æ‰§è¡Œ

æ ‡è®°(Markers)ï¼š
- @pytest.mark.unit: å•å…ƒæµ‹è¯•
- @pytest.mark.integration: é›†æˆæµ‹è¯•
- @pytest.mark.slow: æ…¢é€Ÿæµ‹è¯•
- @pytest.mark.api: éœ€è¦APIå¯†é’¥çš„æµ‹è¯•
- @pytest.mark.optional: å¯é€‰æµ‹è¯•ï¼ˆä¾èµ–å¤–éƒ¨æœåŠ¡ï¼‰
"""

# æµ‹è¯•ç‰ˆæœ¬ä¿¡æ¯
__test_version__ = "0.1.0"

# æµ‹è¯•é…ç½®
TEST_CONFIG = {
    "timeout": 30,  # é»˜è®¤æµ‹è¯•è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    "retry_count": 3,  # å¤±è´¥é‡è¯•æ¬¡æ•°
    "parallel_workers": 4,  # å¹¶è¡Œæµ‹è¯•å·¥ä½œè¿›ç¨‹æ•°
    "coverage_threshold": 80,  # è¦†ç›–ç‡é˜ˆå€¼ï¼ˆç™¾åˆ†æ¯”ï¼‰
}

# æµ‹è¯•æ•°æ®ç›®å½•
TEST_DATA_DIRS = {
    "fixtures": "tests/fixtures",
    "sample_data": "tests/data",
    "expected_outputs": "tests/expected",
    "temp": "tests/temp",
}

# æµ‹è¯•åˆ†ç±»
TEST_CATEGORIES = {
    "core": "æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•",
    "chains": "é“¾åŠŸèƒ½æµ‹è¯•", 
    "agents": "ä»£ç†åŠŸèƒ½æµ‹è¯•",
    "tools": "å·¥å…·åŠŸèƒ½æµ‹è¯•",
    "utils": "å·¥å…·å‡½æ•°æµ‹è¯•",
    "integration": "é›†æˆæµ‹è¯•",
    "performance": "æ€§èƒ½æµ‹è¯•",
    "security": "å®‰å…¨æµ‹è¯•",
}

def get_test_config():
    """
    è·å–æµ‹è¯•é…ç½®
    
    Returns:
        dict: æµ‹è¯•é…ç½®å­—å…¸
    """
    import os
    
    config = TEST_CONFIG.copy()
    
    # ä»ç¯å¢ƒå˜é‡è¦†ç›–é…ç½®
    config["timeout"] = int(os.getenv("TEST_TIMEOUT", config["timeout"]))
    config["retry_count"] = int(os.getenv("TEST_RETRY_COUNT", config["retry_count"]))
    config["parallel_workers"] = int(os.getenv("TEST_WORKERS", config["parallel_workers"]))
    config["coverage_threshold"] = int(os.getenv("COVERAGE_THRESHOLD", config["coverage_threshold"]))
    
    return config

def setup_test_environment():
    """
    è®¾ç½®æµ‹è¯•ç¯å¢ƒ
    
    åˆ›å»ºå¿…è¦çš„æµ‹è¯•ç›®å½•ï¼Œè®¾ç½®æµ‹è¯•æ•°æ®ï¼Œé…ç½®æ—¥å¿—ç­‰
    """
    import os
    import tempfile
    from pathlib import Path
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®ç›®å½•
    for dir_name, dir_path in TEST_DATA_DIRS.items():
        Path(dir_path).mkdir(parents=True, exist_ok=True)
    
    # è®¾ç½®ä¸´æ—¶ç›®å½•
    temp_dir = tempfile.mkdtemp(prefix="langchain_test_")
    os.environ["TEST_TEMP_DIR"] = temp_dir
    
    # è®¾ç½®æµ‹è¯•ç¯å¢ƒå˜é‡
    os.environ["ENVIRONMENT"] = "testing"
    os.environ["DEBUG"] = "true"
    os.environ["LOG_LEVEL"] = "DEBUG"
    
    # è·³è¿‡éœ€è¦APIå¯†é’¥çš„æµ‹è¯•ï¼ˆå¦‚æœæ²¡æœ‰è®¾ç½®ï¼‰
    api_keys = [
        "OPENAI_API_KEY",
        "ANTHROPIC_API_KEY", 
        "GOOGLE_API_KEY",
        "COHERE_API_KEY",
    ]
    
    missing_keys = [key for key in api_keys if not os.getenv(key)]
    if missing_keys:
        os.environ["SKIP_API_TESTS"] = "true"
        print(f"âš ï¸  è·³è¿‡APIæµ‹è¯•ï¼Œç¼ºå°‘å¯†é’¥: {', '.join(missing_keys)}")

def cleanup_test_environment():
    """
    æ¸…ç†æµ‹è¯•ç¯å¢ƒ
    
    åˆ é™¤ä¸´æ—¶æ–‡ä»¶ï¼Œé‡ç½®ç¯å¢ƒå˜é‡ç­‰
    """
    import os
    import shutil
    
    # æ¸…ç†ä¸´æ—¶ç›®å½•
    temp_dir = os.getenv("TEST_TEMP_DIR")
    if temp_dir and os.path.exists(temp_dir):
        shutil.rmtree(temp_dir, ignore_errors=True)
    
    # æ¸…ç†ç¯å¢ƒå˜é‡
    test_env_vars = [
        "TEST_TEMP_DIR",
        "ENVIRONMENT",
        "SKIP_API_TESTS",
    ]
    
    for var in test_env_vars:
        os.environ.pop(var, None)

def get_test_data_path(filename):
    """
    è·å–æµ‹è¯•æ•°æ®æ–‡ä»¶çš„å®Œæ•´è·¯å¾„
    
    Args:
        filename (str): æµ‹è¯•æ•°æ®æ–‡ä»¶å
        
    Returns:
        str: æµ‹è¯•æ•°æ®æ–‡ä»¶çš„å®Œæ•´è·¯å¾„
    """
    from pathlib import Path
    
    data_dir = Path(TEST_DATA_DIRS["sample_data"])
    return str(data_dir / filename)

def load_test_fixture(fixture_name):
    """
    åŠ è½½æµ‹è¯•å¤¹å…·æ•°æ®
    
    Args:
        fixture_name (str): å¤¹å…·åç§°
        
    Returns:
        dict: å¤¹å…·æ•°æ®
    """
    import json
    from pathlib import Path
    
    fixture_path = Path(TEST_DATA_DIRS["fixtures"]) / f"{fixture_name}.json"
    
    if not fixture_path.exists():
        raise FileNotFoundError(f"æµ‹è¯•å¤¹å…·ä¸å­˜åœ¨: {fixture_path}")
    
    with open(fixture_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def create_mock_llm_response(text, usage=None):
    """
    åˆ›å»ºæ¨¡æ‹Ÿçš„LLMå“åº”
    
    Args:
        text (str): å“åº”æ–‡æœ¬
        usage (dict, optional): ä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯
        
    Returns:
        dict: æ¨¡æ‹Ÿçš„LLMå“åº”æ•°æ®
    """
    response = {
        "text": text,
        "model": "mock-model",
        "created": 1234567890,
        "usage": usage or {
            "prompt_tokens": len(text.split()) * 2,
            "completion_tokens": len(text.split()),
            "total_tokens": len(text.split()) * 3,
        }
    }
    
    return response

def assert_response_format(response, expected_keys=None):
    """
    æ–­è¨€å“åº”æ ¼å¼æ­£ç¡®
    
    Args:
        response: è¦éªŒè¯çš„å“åº”å¯¹è±¡
        expected_keys (list, optional): æœŸæœ›çš„é”®åˆ—è¡¨
    """
    assert response is not None, "å“åº”ä¸èƒ½ä¸ºç©º"
    
    if expected_keys:
        if isinstance(response, dict):
            for key in expected_keys:
                assert key in response, f"å“åº”ä¸­ç¼ºå°‘é”®: {key}"
        else:
            for key in expected_keys:
                assert hasattr(response, key), f"å“åº”å¯¹è±¡ç¼ºå°‘å±æ€§: {key}"

def run_test_suite(test_type="all", verbose=False):
    """
    è¿è¡Œæµ‹è¯•å¥—ä»¶
    
    Args:
        test_type (str): æµ‹è¯•ç±»å‹ ("unit", "integration", "all")
        verbose (bool): æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è¾“å‡º
    """
    import subprocess
    import sys
    
    cmd = ["python", "-m", "pytest"]
    
    if verbose:
        cmd.append("-v")
    
    if test_type == "unit":
        cmd.extend(["-m", "unit"])
    elif test_type == "integration":
        cmd.extend(["-m", "integration"])
    elif test_type == "api":
        cmd.extend(["-m", "api"])
    
    # æ·»åŠ è¦†ç›–ç‡æŠ¥å‘Š
    cmd.extend([
        "--cov=src/langchain_learning",
        "--cov-report=term-missing",
        "--cov-report=html",
    ])
    
    print(f"ğŸ§ª è¿è¡Œæµ‹è¯•: {' '.join(cmd)}")
    result = subprocess.run(cmd, cwd=".", capture_output=False)
    
    return result.returncode == 0

# å®šä¹‰å…¬å…±æ¥å£
__all__ = [
    # ç‰ˆæœ¬å’Œé…ç½®
    "__test_version__",
    "TEST_CONFIG",
    "TEST_DATA_DIRS",
    "TEST_CATEGORIES",
    
    # é…ç½®å‡½æ•°
    "get_test_config",
    
    # ç¯å¢ƒç®¡ç†
    "setup_test_environment",
    "cleanup_test_environment",
    
    # æµ‹è¯•æ•°æ®
    "get_test_data_path",
    "load_test_fixture",
    
    # æ¨¡æ‹Ÿå·¥å…·
    "create_mock_llm_response",
    
    # æ–­è¨€å·¥å…·
    "assert_response_format",
    
    # æµ‹è¯•è¿è¡Œ
    "run_test_suite",
]

# å¦‚æœç›´æ¥è¿è¡Œæ­¤æ¨¡å—ï¼Œè®¾ç½®æµ‹è¯•ç¯å¢ƒå¹¶è¿è¡Œæµ‹è¯•
if __name__ == "__main__":
    import sys
    
    print("ğŸ§ª LangChain å­¦ä¹ é¡¹ç›®æµ‹è¯•å¥—ä»¶")
    print("=" * 40)
    print(f"æµ‹è¯•ç‰ˆæœ¬: {__test_version__}")
    print(f"æµ‹è¯•åˆ†ç±»: {list(TEST_CATEGORIES.keys())}")
    
    # è®¾ç½®æµ‹è¯•ç¯å¢ƒ
    setup_test_environment()
    
    try:
        # è¿è¡Œæµ‹è¯•
        test_type = sys.argv[1] if len(sys.argv) > 1 else "all"
        success = run_test_suite(test_type=test_type, verbose=True)
        
        if success:
            print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
        else:
            print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥!")
            sys.exit(1)
            
    finally:
        # æ¸…ç†æµ‹è¯•ç¯å¢ƒ
        cleanup_test_environment()